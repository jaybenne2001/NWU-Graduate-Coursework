{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michael Haugan <br>\n",
    "MSDS 422 - Winter 2019 <br>\n",
    "Assignment 8: Recurrent Neural Networks <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview\n",
    "This assignment involves working with language models developed with pretrained word vectors. We use sentences (sequences of words) to train language models for predicting movie review sentiment (thumbs up versus thumbs down). We study effects of word vector size, vocabulary size, and neural network structure (hyperparameters) on classification performance. We build on resources for recurrent neural networks (RNNs) as implemented in TensorFlow. RNNs are well suited to the analysis of sequences, as needed for natural language processing (NLP) <br>\n",
    "\n",
    "This assignment requires the use of two pretrained word embeddings selected from a list of supported vectors. That is, we replace each word in a sentence or sequence with a vector of numbers. Methods for downloading word embeddings are provided in the Python package chakin (Links to an external site.)Links to an external site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# objective\n",
    "Suppose management is thinking about using a language model to classify written customer reviews and call and complaint logs. If the most critical customer messages can be identified, then customer support personnel can be assigned to contact those customers.\n",
    "\n",
    "How would you advise senior management? What kinds of systems and methods would be most relevant to the customer services function? Considering the results of this assignment in particular, what is needed to make an automated customer support system that is capable of identifying negative customer feelings? What can data scientists do to make language models more useful in a customer service function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and set parameters\n",
    "SET_FIT_INTERCEPT = True\n",
    "RAND = 23\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.linear_model \n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer  \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import tflearn\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from tflearn.metrics import Accuracy\n",
    "\n",
    "#suppress tf.logging\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first get word embeddings (chakin starter code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Python chakin package previously installed by \n",
    "#    pip install chakin\n",
    "import chakin  \n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chakin.search(lang='English')  # lists available indices in English\n",
    "\n",
    "# Specify English embeddings file to download and install\n",
    "# by index number, number of dimensions, and subfoder name\n",
    "# Note that GloVe 50-, 100-, 200-, and 300-dimensional folders\n",
    "# are downloaded with a single zip download\n",
    "CHAKIN_INDEX = 11\n",
    "NUMBER_OF_DIMENSIONS = 50\n",
    "SUBFOLDER_NAME = \"gloVe.6B\"\n",
    "\n",
    "DATA_FOLDER = \"embeddings\"\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
    "if SUBFOLDER_NAME[-1] == \"d\":\n",
    "    GLOVE_FILENAME = os.path.join(\n",
    "        UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
    "else:\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(\n",
    "        SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
    "\n",
    "\n",
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    # GloVe by Stanford is licensed Apache 2.0:\n",
    "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
    "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n",
    "\n",
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "    import zipfile\n",
    "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
    "        ZIP_FILE = ZIP_FILE_ALT\n",
    "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")\n",
    "\n",
    "print('\\nRun complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn sentiment starter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "\n",
    "import re  # regular expressions\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "RANDOM_SEED = 9999\n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B/glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n",
      "Embedding is of shape: (400001, 50)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n",
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n",
    "\n",
    "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
    "\n",
    "# Additional background code from\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "\n",
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 10000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding\n",
    "\n",
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for working with movie reviews data \n",
    "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
    "#    Upper Saddle River, N.J.: Pearson Education.\n",
    "#    ISBN-13: 978-0-13-388644-3\n",
    "# This original study used a simple bag-of-words approach\n",
    "# to sentiment analysis, along with pre-defined lists of\n",
    "# negative and positive words.        \n",
    "# Code available at:  https://github.com/mtpa/wnds       \n",
    "# ------------------------------------------------------------\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    }
   ],
   "source": [
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under movie-reviews-positive\n"
     ]
    }
   ],
   "source": [
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "\n",
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: while\n",
      "Embedding for this word:\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "First word in first document: officially\n",
      "Embedding for this word:\n",
      " [ 0.13682  -0.10324  -0.10126  -0.13996   0.080166 -0.18858  -0.96708\n",
      " -0.066722 -0.254    -0.61085   0.88298  -0.23186  -0.09482  -0.22099\n",
      "  0.85226   0.47223  -0.73086   0.054607 -0.22859   0.6526    0.05519\n",
      " -0.47021   0.35769   0.18049  -0.23699  -1.3029    0.14341   0.044548\n",
      " -0.70229   0.022042  2.3984   -0.46118  -0.88351  -0.5511   -0.25662\n",
      " -0.56969   1.1733   -0.077844 -0.96175  -0.30038  -0.58143  -0.8909\n",
      " -0.34433  -0.53421  -0.84671   0.03971  -1.0485   -0.12547  -0.072426\n",
      " -0.19364 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.13682  -0.10324  -0.10126  -0.13996   0.080166 -0.18858  -0.96708\n",
      " -0.066722 -0.254    -0.61085   0.88298  -0.23186  -0.09482  -0.22099\n",
      "  0.85226   0.47223  -0.73086   0.054607 -0.22859   0.6526    0.05519\n",
      " -0.47021   0.35769   0.18049  -0.23699  -1.3029    0.14341   0.044548\n",
      " -0.70229   0.022042  2.3984   -0.46118  -0.88351  -0.5511   -0.25662\n",
      " -0.56969   1.1733   -0.077844 -0.96175  -0.30038  -0.58143  -0.8909\n",
      " -0.34433  -0.53421  -0.84671   0.03971  -1.0485   -0.12547  -0.072426\n",
      " -0.19364 ]\n",
      "First word in first document: super\n",
      "Embedding for this word:\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])\n",
    "\n",
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])\n",
    "\n",
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn1 (10k vocab, glove.6B.50d.txt embedding)\n",
    "## train: 0.860, test: 0.675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.445\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.5\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.7 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------      \n",
    "# We use a very simple Recurrent Neural Network for this assignment\n",
    "# Géron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
    "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
    "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
    "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
    "#    Source code available at https://github.com/ageron/handson-ml\n",
    "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
    "#    See section on Training an sequence Classifier, # In [34]:\n",
    "#    which uses the MNIST case data...  we revise to accommodate\n",
    "#    the movie review data in this assignment    \n",
    "# --------------------------------------------------------------------------  \n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn2 (100k vocab, glove.6B.50d.txt embedding)\n",
    "## train: 0.860, test: 0.670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B/glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n",
      "Embedding is of shape: (400001, 50)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n",
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n",
      "\n",
      "Test sentence embeddings from vocabulary of 100000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n",
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-positive\n",
      "max_review_length: 1052\n",
      "min_review_length: 22\n",
      "First word in first document: while\n",
      "Embedding for this word:\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "First word in first document: officially\n",
      "Embedding for this word:\n",
      " [ 0.13682  -0.10324  -0.10126  -0.13996   0.080166 -0.18858  -0.96708\n",
      " -0.066722 -0.254    -0.61085   0.88298  -0.23186  -0.09482  -0.22099\n",
      "  0.85226   0.47223  -0.73086   0.054607 -0.22859   0.6526    0.05519\n",
      " -0.47021   0.35769   0.18049  -0.23699  -1.3029    0.14341   0.044548\n",
      " -0.70229   0.022042  2.3984   -0.46118  -0.88351  -0.5511   -0.25662\n",
      " -0.56969   1.1733   -0.077844 -0.96175  -0.30038  -0.58143  -0.8909\n",
      " -0.34433  -0.53421  -0.84671   0.03971  -1.0485   -0.12547  -0.072426\n",
      " -0.19364 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.13682  -0.10324  -0.10126  -0.13996   0.080166 -0.18858  -0.96708\n",
      " -0.066722 -0.254    -0.61085   0.88298  -0.23186  -0.09482  -0.22099\n",
      "  0.85226   0.47223  -0.73086   0.054607 -0.22859   0.6526    0.05519\n",
      " -0.47021   0.35769   0.18049  -0.23699  -1.3029    0.14341   0.044548\n",
      " -0.70229   0.022042  2.3984   -0.46118  -0.88351  -0.5511   -0.25662\n",
      " -0.56969   1.1733   -0.077844 -0.96175  -0.30038  -0.58143  -0.8909\n",
      " -0.34433  -0.53421  -0.84671   0.03971  -1.0485   -0.12547  -0.072426\n",
      " -0.19364 ]\n",
      "First word in first document: super\n",
      "Embedding for this word:\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n",
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.505\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.54 Test accuracy: 0.45\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.49\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.57\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.71\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.705\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 100000  # specify desired size of pre-defined embedding vocabulary \n",
    "\n",
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "# ------------------------------------------------------------- \n",
    "\n",
    "# Utility function for loading embeddings follows methods described in\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
    "# for the requested pre-trained word embeddings\n",
    "# \n",
    "# Note the use of defaultdict data structure from the Python Standard Library\n",
    "# collections_defaultdict.py lets the caller specify a default value up front\n",
    "# The default value will be retuned if the key is not a known dictionary key\n",
    "# That is, unknown words are represented by a vector of zeros\n",
    "# For word embeddings, this default value is a vector of zeros\n",
    "# Documentation for the Python standard library:\n",
    "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
    "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n",
    "\n",
    "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
    "\n",
    "# Additional background code from\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "\n",
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n",
    "\n",
    "# ------------------------------------------------------------- \n",
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding\n",
    "\n",
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# code for working with movie reviews data \n",
    "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
    "#    Upper Saddle River, N.J.: Pearson Education.\n",
    "#    ISBN-13: 978-0-13-388644-3\n",
    "# This original study used a simple bag-of-words approach\n",
    "# to sentiment analysis, along with pre-defined lists of\n",
    "# negative and positive words.        \n",
    "# Code available at:  https://github.com/mtpa/wnds       \n",
    "# ------------------------------------------------------------\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove\n",
    "\n",
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    \n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "\n",
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])\n",
    "\n",
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])\n",
    "\n",
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])        \n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "\n",
    "# --------------------------------------------------------------------------      \n",
    "# We use a very simple Recurrent Neural Network for this assignment\n",
    "# Géron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
    "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
    "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
    "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
    "#    Source code available at https://github.com/ageron/handson-ml\n",
    "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
    "#    See section on Training an sequence Classifier, # In [34]:\n",
    "#    which uses the MNIST case data...  we revise to accommodate\n",
    "#    the movie review data in this assignment    \n",
    "# --------------------------------------------------------------------------  \n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn3 (vocab size 10k, glove.6B.300d.txt embedding)\n",
    "## train: 0.980, test: 0.625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B/glove.6B.300d.txt\n",
      "Embedding loaded from disks.\n",
      "Embedding is of shape: (400001, 300)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [0.04656, 0.21318, -0.0074364, -0.45854, -0.035639, 0.23643, -0.28836, 0.21521, -0.13486, -1.6413, -0.26091, 0.032434, 0.056621, -0.043296, -0.021672, 0.22476, -0.075129, -0.067018, -0.14247, 0.038825, -0.18951, 0.29977, 0.39305, 0.17887, -0.17343, -0.21178, 0.23617, -0.063681, -0.42318, -0.11661, 0.093754, 0.17296, -0.33073, 0.49112, -0.68995, -0.092462, 0.24742, -0.17991, 0.097908, 0.083118, 0.15299, -0.27276, -0.038934, 0.54453, 0.53737, 0.29105, -0.0073514, 0.04788, -0.4076, -0.026759, 0.17919, 0.010977, -0.10963, -0.26395, 0.07399, 0.26236, -0.1508, 0.34623, 0.25758, 0.11971, -0.037135, -0.071593, 0.43898, -0.040764, 0.016425, -0.4464, 0.17197, 0.046246, 0.058639, 0.041499, 0.53948, 0.52495, 0.11361, -0.048315, -0.36385, 0.18704, 0.092761, -0.11129, -0.42085, 0.13992, -0.39338, -0.067945, 0.12188, 0.16707, 0.075169, -0.015529, -0.19499, 0.19638, 0.053194, 0.2517, -0.34845, -0.10638, -0.34692, -0.19024, -0.2004, 0.12154, -0.29208, 0.023353, -0.11618, -0.35768, 0.062304, 0.35884, 0.02906, 0.0073005, 0.0049482, -0.15048, -0.12313, 0.19337, 0.12173, 0.44503, 0.25147, 0.10781, -0.17716, 0.038691, 0.08153, 0.14667, 0.063666, 0.061332, -0.075569, -0.37724, 0.01585, -0.30342, 0.28374, -0.042013, -0.040715, -0.15269, 0.07498, 0.15577, 0.10433, 0.31393, 0.19309, 0.19429, 0.15185, -0.10192, -0.018785, 0.20791, 0.13366, 0.19038, -0.25558, 0.304, -0.01896, 0.20147, -0.4211, -0.0075156, -0.27977, -0.19314, 0.046204, 0.19971, -0.30207, 0.25735, 0.68107, -0.19409, 0.23984, 0.22493, 0.65224, -0.13561, -0.17383, -0.048209, -0.1186, 0.0021588, -0.019525, 0.11948, 0.19346, -0.4082, -0.082966, 0.16626, -0.10601, 0.35861, 0.16922, 0.07259, -0.24803, -0.10024, -0.52491, -0.17745, -0.36647, 0.2618, -0.012077, 0.08319, -0.21528, 0.41045, 0.29136, 0.30869, 0.078864, 0.32207, -0.041023, -0.1097, -0.092041, -0.12339, -0.16416, 0.35382, -0.082774, 0.33171, -0.24738, -0.048928, 0.15746, 0.18988, -0.026642, 0.063315, -0.010673, 0.34089, 1.4106, 0.13417, 0.28191, -0.2594, 0.055267, -0.052425, -0.25789, 0.019127, -0.022084, 0.32113, 0.068818, 0.51207, 0.16478, -0.20194, 0.29232, 0.098575, 0.013145, -0.10652, 0.1351, -0.045332, 0.20697, -0.48425, -0.44706, 0.0033305, 0.0029264, -0.10975, -0.23325, 0.22442, -0.10503, 0.12339, 0.10978, 0.048994, -0.25157, 0.40319, 0.35318, 0.18651, -0.023622, -0.12734, 0.11475, 0.27359, -0.21866, 0.015794, 0.81754, -0.023792, -0.85469, -0.16203, 0.18076, 0.028014, -0.1434, 0.0013139, -0.091735, -0.089704, 0.11105, -0.16703, 0.068377, -0.087388, -0.039789, 0.014184, 0.21187, 0.28579, -0.28797, -0.058996, -0.032436, -0.0047009, -0.17052, -0.034741, -0.11489, 0.075093, 0.099526, 0.048183, -0.073775, -0.41817, 0.0041268, 0.44414, -0.16062, 0.14294, -2.2628, -0.027347, 0.81311, 0.77417, -0.25639, -0.11576, -0.11982, -0.21363, 0.028429, 0.27261, 0.031026, 0.096782, 0.0067769, 0.14082, -0.013064, -0.29686, -0.079913, 0.195, 0.031549, 0.28506, -0.087461, 0.0090611, -0.20989, 0.053913]\n",
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "quick:  [ 0.37594    0.063183   0.51835   -0.48652    0.34026   -0.17801\n",
      "  0.32615   -0.32989    0.25508   -1.026     -0.13841    0.27258\n",
      " -0.010244   0.35186    0.28341    0.3189    -0.18892   -0.292\n",
      " -0.071297   0.25631   -0.34286    0.16179    0.065725  -0.038052\n",
      "  0.1457     0.20289    0.14274   -0.15024    0.35412   -0.13715\n",
      " -0.2677    -0.011243  -0.1541     0.1765    -1.5424     0.37699\n",
      " -0.28239    0.19172   -0.46349   -0.26345   -0.39337    0.41276\n",
      "  0.42873   -0.1317     0.31096   -0.28715    0.47212   -0.41866\n",
      " -0.11871   -0.026621  -0.21089   -0.074757   0.26429   -0.032246\n",
      " -0.084233   0.48653   -0.38692   -0.3756    -0.27031    0.15208\n",
      "  0.2263    -0.28777    0.12475   -0.17029    0.019322   0.34175\n",
      "  0.69492   -0.42039    0.3161    -0.23871    0.30485   -0.078005\n",
      "  0.25937   -0.036324   0.27365   -0.1698    -0.10004    0.13614\n",
      "  0.0094279 -0.51261    0.3464     0.032112   0.25292    0.1947\n",
      "  0.16906   -0.044683  -0.039252   0.31651   -0.27185    0.10862\n",
      "  0.070371   0.31916   -0.55993   -0.5553     0.5493    -0.17244\n",
      " -0.70848    0.039063   0.33553   -0.11393   -0.28882   -0.53623\n",
      "  0.0021584  0.24971    0.31383   -0.2516    -0.28619    0.20113\n",
      " -0.29545   -0.3285     0.33289    0.19422    0.047601  -0.13157\n",
      "  0.4269     0.085041  -0.30294   -0.38344   -0.035083  -0.0463\n",
      "  0.035454  -0.052446   0.51216   -0.37809   -0.24834    0.28464\n",
      "  0.019408   0.61137    0.14859    0.30104    0.23773    0.37627\n",
      " -0.64467    0.19701   -0.19264   -0.013601   0.073281  -0.43031\n",
      "  0.38081   -0.42172   -0.16131    0.12108   -0.12078   -0.20818\n",
      " -0.4697     0.1279    -0.63088    0.16412    0.20474    0.16701\n",
      " -0.79632   -0.075741  -0.25251   -0.025189   0.081245  -0.081758\n",
      " -0.12925   -0.33034    0.039839  -0.30436    0.023003  -0.35589\n",
      "  0.40923   -0.10969   -0.084268   0.56261    0.37604    0.10676\n",
      " -0.1678     0.11219   -0.13141   -0.025916  -0.56102   -0.074779\n",
      " -0.14769    0.13028   -0.38351    0.055938   0.1996     0.0052525\n",
      "  0.11655   -0.58141    0.45407   -0.11067   -0.10262    0.31478\n",
      " -0.049739  -0.34926   -0.016468  -0.12476   -0.071381   0.34803\n",
      " -0.12247   -0.38406    0.095986   0.12451   -0.033609  -0.62353\n",
      "  0.25048   -0.1427     0.60613   -0.080829   0.25008    0.059055\n",
      " -0.17486    0.14913   -0.41488    0.27573   -0.11921   -0.02267\n",
      " -0.34188   -0.49563   -0.22119    0.49553   -0.035482  -0.11908\n",
      "  0.0096008 -0.44059   -0.35947   -0.19156    0.28505    0.35236\n",
      " -0.3384    -0.28643   -0.22068   -0.29761    0.10412   -0.067384\n",
      "  0.043089  -0.05794   -0.31212    0.24026    0.072559  -0.024896\n",
      " -0.19299    0.020044  -0.10826    0.2022     0.097076   0.43886\n",
      "  0.35085    0.38611   -0.1838    -0.047166  -0.5351    -0.17215\n",
      "  0.17407    0.17959   -0.35965   -0.23817    0.16348   -0.79488\n",
      "  0.18858    0.027404   0.23823    0.047581   0.2485     0.18332\n",
      " -0.22626    0.54554   -0.31324   -0.22699   -0.31341    0.68296\n",
      "  0.13422   -0.27644   -0.38901   -0.29207   -0.10058    0.12057\n",
      " -0.36691   -0.69507   -0.22242   -0.023121   0.72283    0.0051197\n",
      " -1.7769     0.40651    0.025966  -0.18157    0.23957    0.37943\n",
      "  0.67713    0.49789    0.3634    -0.60131    0.53868   -0.18682\n",
      " -0.14783    0.40581    0.1379     0.054337  -0.12388    0.064828\n",
      "  0.27453    0.5165    -0.1955    -0.55939   -0.2744    -0.12146  ]\n",
      "brown:  [ 0.2793     0.18372   -0.11257    0.21734   -0.21657   -0.50335\n",
      " -0.27194    0.32181    0.031892  -0.37998    0.15544   -0.32953\n",
      " -0.19827    0.20403    0.26768    0.292     -0.34187   -0.10766\n",
      " -0.43697   -0.14488    0.14634    0.21591    0.12576    0.14895\n",
      " -0.21763    0.030797   0.10949   -0.41689   -0.30296   -0.14592\n",
      " -0.56228    0.33282   -0.20436   -0.24403   -1.4732     0.68345\n",
      "  0.45336    0.43671   -0.15641    0.15075   -0.24265   -0.040059\n",
      "  0.22323    0.19523    0.37445   -0.18509   -0.10302   -0.055363\n",
      " -0.17274   -0.45401   -0.14729   -0.24133   -0.043826  -0.23243\n",
      "  0.42367    0.15906   -0.14039   -0.36185   -0.26695   -0.42724\n",
      " -0.08843   -0.099597   0.24257   -0.05424    0.10746   -1.1304\n",
      "  0.024651  -0.10212    0.046319  -0.68792    0.4214    -0.25844\n",
      "  0.17052    0.097878   0.026835   0.32044    0.0062988  0.24575\n",
      "  0.20126   -0.16771    0.19825    0.28939   -0.064994  -0.38766\n",
      "  0.52509    0.38195    0.32421    0.20683   -0.48472   -0.080334\n",
      " -0.15345    0.35459   -0.43765    0.071575  -0.39516   -0.22906\n",
      "  0.25686    0.26659    0.37626   -0.18556    0.16445   -0.33614\n",
      " -0.56262    0.067852  -0.61642    0.19546    0.45027    0.20238\n",
      "  0.33957    0.41372    0.11855    0.087619   0.18754    0.17901\n",
      "  0.022569  -0.10854   -0.47226    0.41039    0.32588   -0.58468\n",
      " -0.0057296 -0.29201   -0.12777   -0.15729   -0.40103   -0.039414\n",
      " -0.1192     0.40093    0.032862   0.39862   -0.63525    0.11594\n",
      " -0.39954    0.36919   -0.50021   -0.51169   -0.13955    0.18055\n",
      " -0.079918  -0.19474    0.53131    0.093723   0.2773    -0.40505\n",
      " -0.20568    0.11139    0.032661  -0.04852    0.44576    0.23667\n",
      "  0.54981    0.23585   -0.51539   -0.46424    0.021099  -0.3919\n",
      "  0.58338   -0.89908    0.094066   0.30159   -0.063199  -0.31635\n",
      "  0.50333   -0.068517  -0.38681    0.33      -0.49463    0.75491\n",
      " -0.088266  -0.19413    0.4238    -0.031727  -0.4464    -0.21028\n",
      " -0.11151   -0.07088   -0.027832  -0.63304    0.27336   -0.47925\n",
      " -0.03239    0.46069    0.16968   -0.38262   -0.31413   -0.29068\n",
      " -0.031801  -0.48974   -0.50999    0.1466     0.0027995  0.56333\n",
      " -0.044347  -0.085679   0.20559   -0.051593   0.75228   -0.013291\n",
      " -0.084694  -0.4305     1.1734    -0.083233   0.1561    -0.15758\n",
      "  0.19066   -0.2966     0.63704    0.45616   -0.34797   -0.12732\n",
      "  0.4901    -0.51217   -0.063474  -0.061496   0.28825    0.17711\n",
      "  0.46301   -0.12697   -0.044627  -1.0064     0.76394    0.20494\n",
      "  0.028766   0.27597    0.021726  -0.12054    0.23284    0.18999\n",
      "  0.30048   -0.056139   0.09546   -0.036514   0.0084885  0.016599\n",
      " -0.31428   -0.2707     0.099281   0.4445    -0.36      -0.55556\n",
      " -0.18551   -0.30644    0.056475  -0.19197   -0.48886    0.33044\n",
      "  0.19535   -0.53828    0.12385   -0.29372   -0.1036     0.0051129\n",
      "  0.11483   -0.10591    0.73337    0.26978   -0.06925    0.11565\n",
      "  0.27711    0.15109   -0.069137  -0.14481    0.32319    0.039345\n",
      " -0.44964    0.27103    0.045326  -0.064534  -0.37144    0.47615\n",
      " -0.61105   -0.11922   -0.068806   0.15401   -0.40812    0.32575\n",
      " -1.2888     0.0203    -0.12893   -0.22211   -0.16402    0.29018\n",
      "  0.36295   -0.081025  -0.50492    0.5046    -0.37485    0.52111\n",
      "  0.1757     0.069686   0.48937   -0.17747   -0.20577    0.70419\n",
      "  0.068633   0.47878   -0.21754   -0.016868  -0.91378    0.45643  ]\n",
      "fox:  [-1.1570e-01 -2.5048e-02 -1.1013e-01 -4.8060e-02 -8.5504e-02  3.7308e-01\n",
      " -9.4790e-01  5.9662e-01 -2.0006e-02 -3.0469e-01  2.4997e-01 -2.0005e-01\n",
      "  1.9284e-01  4.7197e-01  2.7099e-01  3.4190e-01 -2.9186e-01 -3.9718e-01\n",
      "  5.5653e-01  3.2774e-01  9.4860e-02 -4.6588e-01  6.2119e-01  2.0709e-01\n",
      "  8.2332e-02 -1.0175e-01  3.2067e-01 -3.1875e-01  1.7941e-01 -2.4127e-01\n",
      " -1.7961e-02 -8.7154e-02  3.3423e-01 -4.5026e-02 -1.2193e+00  1.9321e-01\n",
      "  2.1427e-01  2.3893e-01 -1.8084e-01 -1.5920e-01 -1.0387e-01  2.2060e-01\n",
      "  2.5423e-01  2.0815e-01 -3.3072e-01 -2.0672e-01 -3.1424e-01  2.6498e-02\n",
      "  2.1955e-01 -3.1157e-01 -7.8914e-02 -1.5089e-02 -4.0848e-02 -2.3807e-01\n",
      "  3.1983e-01 -7.4173e-02  2.2983e-01  5.5735e-02  5.6743e-03 -7.7368e-01\n",
      "  2.7412e-01 -1.8999e-01  5.3825e-01 -1.9784e-01 -1.0898e-01 -3.4022e-01\n",
      "  3.1582e-01  5.0317e-01  3.3955e-01 -2.7143e-01 -3.3026e-02  3.4279e-01\n",
      " -4.9815e-01  2.5056e-01 -1.2791e-01 -1.5912e-01 -8.7541e-03  1.0450e-01\n",
      "  2.1969e-01 -5.9077e-02 -3.7600e-01  6.0835e-02  1.8427e-01 -8.3592e-02\n",
      "  1.1632e-01  7.5145e-01  6.5186e-02 -3.3258e-01  1.4249e-01 -6.5348e-01\n",
      " -2.6661e-03  1.0233e-01 -3.8731e-01  6.0529e-01 -2.7326e-01 -3.5777e-01\n",
      " -5.3533e-01  4.9589e-02  2.0224e-01 -3.2805e-01  3.9305e-01 -2.7540e-01\n",
      " -1.3042e-01  9.5833e-01  3.8853e-01  5.9884e-01 -3.0503e-01 -1.0708e-01\n",
      "  4.5670e-01  7.2182e-01 -4.4221e-02 -6.8201e-02 -2.9903e-01  2.6483e-01\n",
      "  6.6810e-03  1.4919e-01 -5.3155e-01 -6.1697e-01 -5.4271e-01 -4.1103e-01\n",
      " -1.7534e-01  1.3229e-01  2.6958e-02 -1.5111e-01 -1.6844e-01 -3.0684e-01\n",
      " -5.1189e-02  9.2940e-01 -6.0882e-01 -1.1373e-01 -4.2237e-01 -4.0481e-01\n",
      " -3.4182e-01  3.8472e-01 -1.1566e-01  2.2679e-02  4.4793e-01  1.8849e-01\n",
      " -1.9598e-01 -6.8914e-02 -4.4520e-01  4.1643e-01  5.8319e-02 -3.9575e-01\n",
      " -4.0471e-01  4.2855e-01  1.6339e-01 -1.6965e-01 -5.7793e-02 -1.0358e-01\n",
      "  1.0223e+00  9.0564e-01 -5.5851e-01 -2.4856e-01  7.5582e-02 -2.7374e-01\n",
      "  3.8838e-01  5.1561e-01  6.0029e-02 -5.1990e-02  1.0113e+00  8.3607e-03\n",
      "  4.9764e-01  3.2839e-02  2.5892e-02  4.0404e-01 -3.7393e-01  1.3384e-01\n",
      " -2.9458e-01  3.4668e-01  5.2114e-02  1.0785e-01 -8.4700e-01  4.0936e-01\n",
      "  3.8801e-01  4.5079e-01 -4.3880e-01  1.9777e-01  1.9102e-01 -7.6580e-02\n",
      "  2.5788e-01  3.5642e-01  2.3305e-01 -1.6206e-01 -5.0847e-02  1.3708e-01\n",
      "  5.1338e-02 -3.8552e-01 -2.5497e-01  5.3901e-02 -6.2922e-01  3.2780e-01\n",
      " -2.0383e-01 -1.7265e-01 -9.9668e-02  2.4485e-01  3.2700e-02  3.2325e-01\n",
      "  2.3912e-01  7.7014e-03  1.2890e+00  6.8030e-02  2.0671e-02  2.6268e-01\n",
      "  1.9434e-01  5.6238e-01  7.1863e-02 -6.5502e-02 -2.2858e-01  9.2883e-02\n",
      "  3.9936e-01 -2.8038e-01  3.5633e-01  1.5203e-01 -2.7883e-01  3.8225e-02\n",
      " -3.9636e-01  3.2488e-01  2.4332e-01  2.2621e-01  2.3518e-01 -6.4747e-02\n",
      "  6.2858e-01 -1.1848e-01  1.2710e-01 -1.2697e-01  1.0015e+00 -2.5693e-01\n",
      "  2.0450e-01 -1.8789e-01 -1.2913e-01 -9.6456e-02  1.2301e-01 -1.3377e-01\n",
      " -1.2808e-01  4.1857e-02  8.6418e-01  3.6137e-01  9.0372e-02 -1.0687e-02\n",
      "  4.4736e-01  7.8975e-02 -3.2992e-01 -1.4470e-01 -6.1296e-01 -4.1543e-01\n",
      " -2.6104e-01 -8.4169e-01  4.7352e-01 -4.7640e-01 -3.4393e-01  8.2156e-02\n",
      " -2.4477e-01  7.9361e-01  2.6183e-01  4.6648e-01  1.9135e-01 -5.5033e-02\n",
      "  2.3492e-01  1.1178e-01 -3.7291e-01  2.6764e-01  1.1074e-01  6.8041e-01\n",
      " -1.0361e-03 -4.2537e-02  9.4006e-01 -2.8939e-01 -2.0841e-01  3.3001e-01\n",
      "  7.3318e-02 -5.1902e-01 -3.6412e-01 -4.1372e-01  2.0266e-01  4.3162e-01\n",
      " -1.0174e+00  2.5093e-01  3.8136e-01 -1.4332e-01 -2.0525e-01 -1.8724e-01\n",
      "  5.9925e-01  3.5604e-01 -9.1457e-01 -2.2401e-01 -4.2382e-02 -1.3299e-01\n",
      "  6.1671e-01  4.4886e-01 -1.1361e-01  2.0483e-01  2.5485e-01  2.7581e-01\n",
      " -8.7158e-01 -1.3156e-01  2.6117e-01  1.5815e-01 -3.2199e-01  7.1042e-01]\n",
      "jumps:  [-0.16814   -0.10948    0.2896    -0.21108   -0.29061    0.31201\n",
      "  0.04039   -0.10149   -0.18526   -0.55483   -0.36055   -0.093569\n",
      "  0.77334    0.027921   0.13389   -0.1014    -0.06482    0.24753\n",
      " -0.068026   0.26147   -0.14252    0.18657    0.030249  -0.07934\n",
      "  0.87696    0.61781    0.36835   -0.07306   -0.19303    0.3721\n",
      " -0.77087   -0.0062782 -0.19233   -0.43884   -0.79847    0.066636\n",
      " -0.21387   -0.65263   -0.073964   0.64115   -0.52014   -0.05981\n",
      " -0.1692    -0.413      0.060197   0.16327    0.43353    0.070021\n",
      "  0.063543   0.28527   -0.43474    0.15441    0.098037   0.098685\n",
      " -0.3965     0.38171   -0.084065  -0.4813    -0.59213    0.40444\n",
      " -0.2026     0.45569    0.039036  -0.41786   -0.20322   -0.11932\n",
      "  0.23747    0.26336    0.17139    0.12521   -0.55276    0.45515\n",
      " -0.63826    0.14054    0.35333   -0.28417    0.3889    -0.13004\n",
      " -0.027142  -0.23109    0.034327  -0.10685    0.85855    0.15145\n",
      "  0.16814    0.2281     0.22235   -0.1825    -0.019222   0.026105\n",
      "  0.47734    0.42115   -0.087767  -0.17439    0.22166   -0.36831\n",
      " -1.069      0.40489   -0.31038   -0.21588   -0.7282     0.29296\n",
      " -0.42949    0.23485    0.020585  -0.47795   -0.20216   -0.22146\n",
      " -0.45778    0.032547  -0.13727   -0.48945   -0.58148    0.051203\n",
      " -0.065926   0.46718   -0.080438  -0.25042   -0.4015     0.40254\n",
      "  0.12       0.5246     0.21582    0.1333     0.27662    0.2163\n",
      " -0.28177    0.67185   -0.025996  -0.40781   -0.23629    0.97455\n",
      " -0.51452    0.22697   -0.34857   -0.55928    0.019104  -0.016163\n",
      " -0.22626    0.094269   0.10808   -0.018804  -0.080299   0.0058964\n",
      " -0.61886    0.44842    0.2494    -0.25172    0.6705     0.23657\n",
      " -0.17631    0.28634   -0.39527   -0.22096    0.23069    0.0644\n",
      "  0.13151   -0.030479   0.38503   -0.084794   0.66178   -0.34578\n",
      "  0.3868     0.31506    0.20155   -0.026189   0.051188   0.16359\n",
      " -0.37096    0.21546   -0.19758   -0.083407   0.48437   -0.5487\n",
      "  0.35188   -0.43539    0.3976    -0.026514  -0.14485   -0.11205\n",
      "  0.59835    0.38055    0.1582    -0.24293    0.39837    0.44276\n",
      "  0.36113   -0.2358    -0.32568    0.54672   -0.36544   -0.33871\n",
      "  0.2154    -0.36877   -0.18857    0.42323   -0.56938   -0.20276\n",
      "  0.35452   -0.0167     1.157      0.22296   -0.35115    0.3662\n",
      " -0.20903    0.73497    0.018414   0.27861    0.044337   0.14504\n",
      " -0.23001   -0.54025   -0.26259   -0.94587    0.22239    0.44651\n",
      " -0.46075   -0.42224    0.0022315 -0.25522   -0.13135    0.65923\n",
      "  0.21267   -0.26498   -0.16777   -0.047013  -0.18551    0.12607\n",
      "  0.43205    0.153     -0.33748    0.010732  -0.18332   -0.28507\n",
      "  0.23396    0.3868    -0.34724    0.41692    0.56315    0.32366\n",
      "  0.55303    0.02256   -0.48281    0.54561   -0.51086   -0.55446\n",
      " -0.018992   0.62551    0.11436   -0.10939   -0.61894   -0.18419\n",
      "  0.27755   -0.29704   -0.21562   -0.011765   0.051859   0.04041\n",
      " -0.16798    0.65697   -0.36868    0.068085   0.34986    0.076252\n",
      "  0.12035    0.16001   -0.0088939  0.14892   -0.70374   -0.031891\n",
      "  0.2803    -0.26916   -0.61974    0.67423    0.080773  -0.32835\n",
      " -0.276     -0.60342   -0.60185    0.10982   -0.20312    0.42309\n",
      "  0.63701    0.51885   -0.31899    0.34256   -0.035933  -0.049542\n",
      " -0.89278   -0.03686    0.12862    0.41469   -0.37262   -0.20523\n",
      " -0.02476   -0.11959   -0.12129    0.099622   0.24538   -0.026313 ]\n",
      "over:  [-8.8137e-02 -2.1696e-02  2.9863e-01 -1.8325e-02 -2.3575e-01  1.1022e-01\n",
      " -1.7493e-01  9.9241e-03  2.3832e-01 -1.7643e+00  2.2489e-01  4.0552e-01\n",
      " -4.8176e-01 -6.6099e-02  1.3290e-01  4.7502e-01 -5.6438e-02  3.2902e-01\n",
      "  9.7628e-02  4.2467e-01  2.5285e-01 -1.7258e-01  7.6564e-02 -1.5678e-01\n",
      " -2.2694e-01 -2.1213e-01 -3.3460e-01  7.3842e-02 -4.4671e-01  3.3979e-01\n",
      " -2.3534e-01  1.5013e-01 -3.4718e-01  5.4379e-02 -8.8699e-01 -3.5534e-01\n",
      " -1.3166e-01 -1.8265e-02 -1.8708e-01  3.0193e-01  1.0008e-01 -1.1980e-01\n",
      " -7.4867e-01  1.5592e-01  4.2757e-01 -2.9996e-01 -1.2499e-01  1.2750e-01\n",
      " -2.1962e-01  4.5077e-01 -3.2268e-01 -2.8934e-01 -1.3745e-01 -1.7738e-01\n",
      "  2.4185e-01 -1.2240e-02  1.7264e-01  2.7287e-01 -8.2743e-02  2.0570e-01\n",
      "  2.2559e-02 -7.5793e-02  2.1027e-01 -3.1239e-01  1.3032e-01 -7.5693e-01\n",
      "  9.0872e-02  4.9975e-01  1.1746e-01 -5.3133e-01 -8.5348e-02  4.4042e-01\n",
      "  1.2231e-01 -1.8672e-01  1.1668e-01  1.0322e-02 -2.6779e-01  1.5042e-01\n",
      " -5.4751e-01 -3.8439e-01 -6.7117e-01 -3.4912e-01  1.4190e-01  1.2742e-01\n",
      " -1.2879e-01 -1.3582e-01  2.9059e-01  1.4339e-01  2.8467e-01  7.0642e-02\n",
      "  4.7118e-03  1.8321e-01 -4.0973e-01  2.1973e-01 -6.6693e-01 -1.0599e-01\n",
      " -3.4465e-01  2.3103e-01 -3.6278e-01 -1.2365e-02  1.1544e-01  3.6134e-02\n",
      "  7.4889e-02 -6.4933e-01 -3.3362e-02 -3.0498e-01  4.0964e-01  2.6898e-01\n",
      " -2.1150e-01 -1.1446e-01 -4.8326e-02 -5.9265e-01 -3.7301e-01 -5.4902e-02\n",
      " -1.9869e-01  2.3181e-01 -2.9017e-01  6.2177e-01  8.9856e-02 -6.4116e-01\n",
      "  3.9304e-02  9.7170e-02 -2.9954e-01  4.0426e-01  8.4010e-02  1.2902e-01\n",
      " -3.5864e-01  3.9975e-01  1.3183e-01 -1.4797e-01 -3.1449e-01  4.8654e-01\n",
      "  2.9974e-02  5.3637e-01  1.1619e-01  1.4643e-01 -2.2927e-01  7.9999e-02\n",
      "  1.3114e-01 -3.0946e-02  1.6410e-01 -3.5597e-01  2.0801e-01  3.3193e-01\n",
      " -8.4092e-01  3.3762e-01 -7.5518e-02 -4.3158e-01  1.9592e-01 -2.7815e-01\n",
      "  6.6744e-01  3.1189e-02 -1.1706e-02  5.3737e-01  2.9596e-01  3.7335e-01\n",
      " -2.2166e-01  2.3843e-02 -1.1905e-01 -1.8529e-01  1.1857e-02 -2.3540e-01\n",
      "  2.3800e-01 -3.1987e-02 -1.6400e-01 -7.9319e-02  9.9570e-02  2.6182e-02\n",
      " -6.0793e-01 -6.7515e-02  3.9870e-02  7.0721e-02 -6.0965e-01 -7.3549e-01\n",
      " -8.1241e-03 -1.3363e-01 -1.6572e-01  3.2373e-01  1.0115e-01  6.0659e-01\n",
      " -2.2547e-02  2.5960e-01  2.4251e-01 -7.8426e-02 -7.8699e-02 -4.1680e-01\n",
      " -1.4208e-01  2.5136e-02 -3.4187e-01  2.7631e-01  2.4892e-01  2.7898e-01\n",
      " -2.6522e-01 -3.3989e-01  1.7929e-01 -4.0393e-01  2.7421e-01  2.2639e-01\n",
      "  4.5234e-01 -2.4806e-01  9.4945e-01  1.4040e-01  2.3117e-02 -3.3321e-01\n",
      "  3.6958e-01 -1.7507e-01 -1.8646e-01 -2.4238e-01  2.0633e-02 -7.8795e-02\n",
      "  5.1631e-01  4.5651e-01  2.5398e-01  3.8711e-01  1.6812e-01 -5.8263e-01\n",
      " -9.9027e-02 -2.3445e-01  3.6928e-02  6.1330e-02  2.4186e-01 -2.0908e-01\n",
      " -3.8720e-02  1.4625e-01 -3.6779e-02  2.5993e-01  3.1897e-01 -5.5307e-02\n",
      " -7.7177e-02  3.0117e-01  2.3307e-01  4.1980e-01 -1.3339e-01 -2.0384e-02\n",
      "  7.1158e-01  8.4989e-02  3.2791e-01  4.8060e-01 -3.3761e-01  5.7392e-02\n",
      " -2.6536e-01  3.0228e-01  1.8540e-01 -8.3752e-02 -7.3776e-01 -1.4404e-03\n",
      "  3.4121e-01  2.9959e-02  4.2035e-01  7.3122e-02 -1.9430e-01 -2.2341e-01\n",
      "  5.8658e-01 -2.2756e-01  6.3029e-01 -6.8682e-02 -3.5365e-01  2.6851e-01\n",
      " -7.3124e-02  1.5702e-02  1.2022e-01 -2.4631e-01  2.8471e-01  3.8292e-01\n",
      " -3.8869e-01  2.0974e-01 -2.0512e-01 -1.8508e-01  3.2543e-01  3.7336e-03\n",
      " -1.9901e-02 -8.4242e-02  2.9326e-01 -5.8162e-02  3.6575e-01  4.6895e-02\n",
      " -2.1328e+00 -3.5109e-01  4.1716e-01  1.8393e-01 -3.0990e-01  2.7303e-01\n",
      "  1.6627e-02  4.4978e-03 -7.2902e-02  5.3814e-02 -5.7213e-02  1.5815e-01\n",
      "  1.4517e-01  1.6580e-01 -7.3557e-02  1.6125e-01 -9.9237e-02  2.6099e-01\n",
      "  2.7941e-02  3.2140e-01  1.6931e-01 -3.5469e-01 -4.2713e-01 -3.9323e-01]\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "lazy:  [ 4.2791e-01 -1.6070e-01  2.4912e-01  3.9763e-01 -3.2224e-01  1.9783e-04\n",
      "  8.8576e-02  4.0501e-01 -2.4655e-01  1.6410e-02 -2.3331e-01 -1.2307e-01\n",
      " -3.9679e-01  2.1877e-02  1.6211e-01  3.8852e-01  2.5025e-01 -4.3968e-02\n",
      "  1.0819e+00  6.6517e-01  2.1829e-01  7.9898e-01 -6.1695e-01 -1.0184e-01\n",
      "  2.2900e-01 -3.1982e-01  3.1205e-01 -1.0453e-01  4.4810e-01  3.6708e-02\n",
      " -1.1376e-02  7.0543e-01  2.7454e-01  2.5835e-01 -5.1821e-01  8.2996e-01\n",
      " -8.0672e-02 -4.5489e-01 -3.4843e-01  5.1421e-01 -5.7408e-01  6.4612e-01\n",
      " -2.1564e-01 -5.4848e-01  2.3656e-01  1.8453e-02  8.6403e-01 -1.0357e-01\n",
      " -5.6097e-02 -4.8564e-01  2.7927e-01 -6.1733e-01  5.2877e-01 -1.9221e-01\n",
      " -1.4684e-01  4.2272e-01  5.6312e-02 -3.3168e-01  8.1864e-02  2.2225e-01\n",
      "  2.7757e-01 -2.1934e-02  2.8499e-01  5.7453e-03 -4.0598e-01  6.1549e-02\n",
      "  1.1544e-01  7.2053e-02 -5.4945e-02 -3.9549e-02 -1.9371e-01  2.7872e-01\n",
      " -1.0139e-01 -7.1846e-02 -3.4043e-01 -2.2490e-02 -2.6097e-03  6.5403e-01\n",
      "  4.1425e-01 -1.1459e-01 -4.9802e-01 -3.4516e-02 -3.0815e-02 -6.1508e-01\n",
      "  4.8955e-01 -1.0641e-01  6.3485e-02  5.5039e-01 -1.6282e-01 -7.7526e-02\n",
      " -1.5945e-01  3.0791e-01 -3.0439e-02 -3.8283e-01  7.8436e-02  1.1488e-01\n",
      "  6.7573e-02  2.2181e-01 -1.4319e-02  1.4366e-02  2.8839e-01 -9.1358e-01\n",
      " -5.3084e-01  1.3097e-01 -2.0027e-01  2.1495e-01 -3.6158e-01  7.6012e-02\n",
      "  1.2718e-01  2.5851e-01  6.6530e-02  3.1628e-01 -6.3175e-01 -4.1942e-01\n",
      "  3.8640e-01  7.3017e-02 -9.2298e-02 -8.8510e-01 -2.1618e-01  2.9006e-01\n",
      "  3.6404e-01  2.3740e-01 -2.2910e-01  3.2436e-01  6.0954e-01  3.2458e-01\n",
      " -8.4691e-02  4.1471e-01  2.6053e-01  6.8716e-02 -4.4528e-01  2.9296e-02\n",
      " -4.7913e-02  4.5709e-01 -5.2956e-01 -3.0998e-01  3.2488e-01 -3.6054e-01\n",
      "  1.8449e-01 -3.8492e-01  3.9918e-02  4.0046e-01 -8.7039e-02 -4.9739e-01\n",
      " -7.1877e-01  1.4894e-01  1.5550e-01  3.2388e-01  6.7209e-01  4.0215e-01\n",
      " -7.3436e-01  7.0311e-02 -7.3754e-02  3.1062e-01  1.4725e-03  4.3386e-02\n",
      "  2.9529e-01 -1.0544e-01  7.5028e-01  2.2274e-01  4.3789e-02 -6.7316e-01\n",
      "  6.8448e-01 -5.5239e-01 -2.2204e-02 -1.8850e-01 -4.0747e-01 -5.7321e-02\n",
      "  1.8211e-01 -5.8300e-01 -2.0733e-01  2.8411e-01 -3.5269e-01 -1.2422e-03\n",
      " -1.7660e-01 -6.9418e-01 -2.7758e-01  3.6022e-01 -1.7472e-01 -2.9792e-01\n",
      "  4.5945e-01  1.3176e-01 -2.2119e-01 -1.0709e-01  1.6470e-02  1.5636e-01\n",
      "  4.0296e-01 -7.1640e-01  4.4003e-01 -2.0855e-01 -5.1675e-01  3.3292e-01\n",
      "  2.5018e-01 -5.7151e-01 -7.5581e-04 -8.8453e-02 -4.6790e-01 -6.3341e-01\n",
      "  1.7374e-01 -1.2076e-01  5.9592e-01 -2.6351e-02 -1.7858e-02  1.2978e-01\n",
      "  2.4669e-01  9.1323e-02 -1.4112e-01  4.6466e-02  1.8030e-01  1.7382e-01\n",
      " -7.7972e-02 -1.4600e-01 -5.1364e-01  5.1154e-01 -3.3271e-01  5.2346e-01\n",
      " -9.4829e-02 -2.0365e-01  5.6919e-01 -1.5709e-01 -5.4340e-01  2.4034e-01\n",
      " -4.5061e-02  1.5918e-01 -7.0530e-01 -7.9981e-03  5.1987e-01 -1.6802e-01\n",
      " -8.0854e-03  2.4719e-01  3.6062e-01 -2.2302e-01 -3.2196e-01 -7.6371e-01\n",
      "  1.9203e-02  2.0398e-01 -4.4568e-01  1.1332e-01 -1.3784e-01 -5.6301e-02\n",
      "  5.5306e-01  4.6183e-01 -8.0710e-01 -4.1624e-01 -5.1206e-01 -8.1953e-01\n",
      "  7.8391e-03  2.8204e-02  2.0151e-01  4.5986e-01  2.0020e-01 -9.1094e-02\n",
      "  4.3782e-01  3.4559e-01  3.6562e-01  3.4960e-01  1.7984e-01 -2.3978e-01\n",
      " -2.5039e-01  6.7002e-03  1.0974e-01  8.1626e-02 -2.4783e-01  2.6453e-01\n",
      " -3.6779e-02  3.1099e-01  6.7982e-01 -4.6699e-01 -2.8060e-01  8.2703e-01\n",
      "  2.3553e-01 -7.4127e-01  2.9891e-02 -1.3198e-01  2.2106e-01  1.7262e-01\n",
      " -4.2037e-01  5.6484e-01 -7.0211e-01 -1.5537e-01 -8.0067e-02 -6.9698e-02\n",
      "  8.0176e-01 -2.4841e-01 -7.1711e-01 -2.5340e-01  7.2812e-01  1.6527e-01\n",
      " -2.2780e-01 -3.2008e-01  2.8609e-01 -5.8733e-02 -5.4448e-01  2.6380e-01\n",
      "  3.3366e-01 -5.1100e-01 -1.0377e-01 -1.9413e-01 -3.9855e-01  2.2238e-01]\n",
      "dog:  [-1.1043e-01  8.1217e-01  7.3668e-02  1.9023e-01 -5.2888e-02  6.1468e-02\n",
      "  1.6076e-01  4.1302e-01 -3.0199e-01 -9.0827e-01  2.7504e-01 -3.1890e-02\n",
      " -2.8842e-01  2.3447e-01  4.7679e-01  5.0124e-01  2.9371e-01  2.7029e-01\n",
      "  5.4745e-02  9.8038e-02  5.7116e-01  3.6755e-01  4.0734e-02  3.4347e-01\n",
      " -1.8256e-01 -2.8935e-01  2.3826e-02 -1.9401e-01  2.4444e-01  1.3407e-01\n",
      " -1.6494e-01 -2.6983e-01 -2.6234e-01 -2.1779e-01 -8.7528e-01  7.3822e-01\n",
      " -8.7931e-02 -1.0876e-02 -2.6540e-01  3.4668e-01 -5.5814e-01  1.7591e-01\n",
      "  1.6926e-01 -1.5725e-01 -5.0430e-01 -2.0100e-01  6.6701e-01 -3.2518e-02\n",
      "  4.5012e-02  6.5675e-02 -1.6061e-01 -7.3363e-01  2.4642e-01  3.4325e-01\n",
      "  2.1899e-01  4.8646e-02 -5.9987e-01 -5.8153e-02 -5.1694e-02 -5.7846e-01\n",
      "  3.0000e-01  3.5078e-01  4.6646e-01 -7.5309e-03  1.0455e-01 -5.1016e-01\n",
      " -5.5987e-02 -1.0295e-01 -2.6476e-01 -4.1230e-02 -2.8371e-02  5.1979e-01\n",
      " -3.4849e-01 -4.7217e-01 -3.7229e-01 -3.2790e-02  1.3989e-01  3.5716e-01\n",
      "  1.9305e-01 -2.1986e-01  2.4136e-01  4.0976e-01  3.7516e-01  1.4255e-01\n",
      " -3.4143e-02 -7.2653e-01 -1.0832e-01  6.8616e-01 -2.6335e-01 -4.2345e-01\n",
      " -2.4253e-01  1.5778e-01  1.4258e-01 -3.2749e-01 -3.4699e-01  1.6148e-01\n",
      "  1.9603e-01  4.1639e-01 -2.3370e-01  7.5816e-02  1.5899e-01  1.6623e-03\n",
      " -4.8301e-02 -1.0611e-01 -1.9326e-01  1.4494e-01  1.5406e-02  1.0629e-01\n",
      " -3.6699e-02  6.3230e-01  1.2986e-01  4.9902e-01 -1.1323e+00 -1.2636e-01\n",
      "  6.4718e-02  1.2374e-01 -4.9712e-01 -1.4836e-02  1.0488e-01 -4.9818e-01\n",
      " -2.8856e-01  3.8949e-01 -3.1828e-02 -2.8625e-01 -9.8758e-02 -7.6990e-02\n",
      " -2.4234e-01  7.5793e-01  3.4835e-01 -7.1030e-01  4.5318e-01 -3.4418e-01\n",
      " -1.9459e-01  6.1478e-01 -2.9010e-02 -2.7864e-01  3.8556e-01  1.0072e-01\n",
      "  1.2895e-01  1.7992e-02  3.3670e-01  2.0698e-01 -3.8049e-01 -6.6661e-03\n",
      "  1.1540e-01 -8.5268e-02 -1.4608e-01  4.4514e-01 -9.3674e-02  2.3639e-01\n",
      " -1.1447e-01  1.0948e+00 -5.7823e-02 -1.6295e-01  5.5880e-01 -1.8988e-02\n",
      " -7.1374e-02  2.1319e-01  6.1277e-02  7.2759e-01  6.2747e-01 -1.9280e-01\n",
      "  1.3057e-01  1.7426e-01 -1.0229e-01  1.5232e-01  5.2500e-01 -2.1919e-01\n",
      " -2.7185e-01 -5.4186e-01  3.1752e-01  1.6375e-01 -2.9039e-01  1.7074e-01\n",
      " -3.1814e-01 -9.6421e-01 -1.1610e-01 -2.9951e-01  1.8686e-01 -4.5986e-01\n",
      "  4.1633e-01 -1.7583e-01 -3.4583e-01 -2.7244e-01 -5.0216e-01  1.2852e-02\n",
      "  5.9838e-01 -1.1237e-01  2.4697e-01 -4.9048e-01 -4.4188e-01 -1.6255e-01\n",
      " -7.3313e-01 -3.7677e-01 -6.8925e-01  6.1174e-02 -4.2101e-01 -1.3153e-01\n",
      " -8.3590e-03 -1.8360e-02  1.3686e+00  4.6169e-02  9.4622e-01 -1.5126e-02\n",
      " -1.2477e-01  4.8754e-01  2.2384e-01 -2.1820e-01 -2.3389e-01  1.5207e-01\n",
      " -2.8718e-01 -6.3908e-01 -2.2383e-01 -1.8014e-01 -3.3548e-01  5.3587e-01\n",
      " -2.9367e-01  1.0866e-01  6.3411e-02 -9.3424e-03 -1.5886e-01  2.2602e-01\n",
      "  1.1925e-01 -4.1442e-01 -7.8062e-02 -9.7857e-02  2.7938e-01 -1.8348e-01\n",
      " -3.4584e-01  1.8489e-01  1.7402e-01 -5.2198e-01 -4.3306e-01  1.6256e-01\n",
      "  1.4032e-01  3.5124e-01 -1.8280e-01 -3.5984e-01 -1.3009e-01  1.6304e-01\n",
      "  3.1734e-01  3.7716e-03 -4.5498e-02 -4.2066e-01 -4.4419e-01 -6.8985e-01\n",
      " -4.9359e-01  7.0281e-02 -1.4377e-01  6.2508e-01 -5.6311e-02  1.8850e-01\n",
      " -5.6785e-02  1.4052e-01  1.1973e+00  7.1894e-01  5.4332e-01 -1.2461e-01\n",
      " -1.1978e-01  3.0163e-01 -1.6273e-01 -4.6740e-02 -2.5249e-01 -3.0659e-02\n",
      " -3.2271e-01  3.2361e-01  3.3244e-01 -2.7819e-02 -3.3367e-01 -2.3444e-02\n",
      " -5.0394e-01 -2.0587e-01 -1.3013e-01 -3.5884e-01  4.5384e-02 -1.1863e-01\n",
      " -1.7257e+00  3.9441e-01 -5.3179e-01  5.8209e-01 -6.5771e-01  3.6849e-01\n",
      "  2.3518e-01  1.0802e-01 -8.3159e-01  6.1486e-01  2.5547e-01 -4.5289e-01\n",
      "  5.1446e-01 -1.7911e-01 -1.2389e-01  1.8688e-01 -4.1102e-01 -7.0877e-01\n",
      " -3.7501e-01 -6.6152e-01  6.7730e-01  3.3936e-01  5.7994e-01  6.8149e-02]\n",
      "\n",
      "Test sentence embeddings from vocabulary of 10000 words:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "quick:  [ 0.37594    0.063183   0.51835   -0.48652    0.34026   -0.17801\n",
      "  0.32615   -0.32989    0.25508   -1.026     -0.13841    0.27258\n",
      " -0.010244   0.35186    0.28341    0.3189    -0.18892   -0.292\n",
      " -0.071297   0.25631   -0.34286    0.16179    0.065725  -0.038052\n",
      "  0.1457     0.20289    0.14274   -0.15024    0.35412   -0.13715\n",
      " -0.2677    -0.011243  -0.1541     0.1765    -1.5424     0.37699\n",
      " -0.28239    0.19172   -0.46349   -0.26345   -0.39337    0.41276\n",
      "  0.42873   -0.1317     0.31096   -0.28715    0.47212   -0.41866\n",
      " -0.11871   -0.026621  -0.21089   -0.074757   0.26429   -0.032246\n",
      " -0.084233   0.48653   -0.38692   -0.3756    -0.27031    0.15208\n",
      "  0.2263    -0.28777    0.12475   -0.17029    0.019322   0.34175\n",
      "  0.69492   -0.42039    0.3161    -0.23871    0.30485   -0.078005\n",
      "  0.25937   -0.036324   0.27365   -0.1698    -0.10004    0.13614\n",
      "  0.0094279 -0.51261    0.3464     0.032112   0.25292    0.1947\n",
      "  0.16906   -0.044683  -0.039252   0.31651   -0.27185    0.10862\n",
      "  0.070371   0.31916   -0.55993   -0.5553     0.5493    -0.17244\n",
      " -0.70848    0.039063   0.33553   -0.11393   -0.28882   -0.53623\n",
      "  0.0021584  0.24971    0.31383   -0.2516    -0.28619    0.20113\n",
      " -0.29545   -0.3285     0.33289    0.19422    0.047601  -0.13157\n",
      "  0.4269     0.085041  -0.30294   -0.38344   -0.035083  -0.0463\n",
      "  0.035454  -0.052446   0.51216   -0.37809   -0.24834    0.28464\n",
      "  0.019408   0.61137    0.14859    0.30104    0.23773    0.37627\n",
      " -0.64467    0.19701   -0.19264   -0.013601   0.073281  -0.43031\n",
      "  0.38081   -0.42172   -0.16131    0.12108   -0.12078   -0.20818\n",
      " -0.4697     0.1279    -0.63088    0.16412    0.20474    0.16701\n",
      " -0.79632   -0.075741  -0.25251   -0.025189   0.081245  -0.081758\n",
      " -0.12925   -0.33034    0.039839  -0.30436    0.023003  -0.35589\n",
      "  0.40923   -0.10969   -0.084268   0.56261    0.37604    0.10676\n",
      " -0.1678     0.11219   -0.13141   -0.025916  -0.56102   -0.074779\n",
      " -0.14769    0.13028   -0.38351    0.055938   0.1996     0.0052525\n",
      "  0.11655   -0.58141    0.45407   -0.11067   -0.10262    0.31478\n",
      " -0.049739  -0.34926   -0.016468  -0.12476   -0.071381   0.34803\n",
      " -0.12247   -0.38406    0.095986   0.12451   -0.033609  -0.62353\n",
      "  0.25048   -0.1427     0.60613   -0.080829   0.25008    0.059055\n",
      " -0.17486    0.14913   -0.41488    0.27573   -0.11921   -0.02267\n",
      " -0.34188   -0.49563   -0.22119    0.49553   -0.035482  -0.11908\n",
      "  0.0096008 -0.44059   -0.35947   -0.19156    0.28505    0.35236\n",
      " -0.3384    -0.28643   -0.22068   -0.29761    0.10412   -0.067384\n",
      "  0.043089  -0.05794   -0.31212    0.24026    0.072559  -0.024896\n",
      " -0.19299    0.020044  -0.10826    0.2022     0.097076   0.43886\n",
      "  0.35085    0.38611   -0.1838    -0.047166  -0.5351    -0.17215\n",
      "  0.17407    0.17959   -0.35965   -0.23817    0.16348   -0.79488\n",
      "  0.18858    0.027404   0.23823    0.047581   0.2485     0.18332\n",
      " -0.22626    0.54554   -0.31324   -0.22699   -0.31341    0.68296\n",
      "  0.13422   -0.27644   -0.38901   -0.29207   -0.10058    0.12057\n",
      " -0.36691   -0.69507   -0.22242   -0.023121   0.72283    0.0051197\n",
      " -1.7769     0.40651    0.025966  -0.18157    0.23957    0.37943\n",
      "  0.67713    0.49789    0.3634    -0.60131    0.53868   -0.18682\n",
      " -0.14783    0.40581    0.1379     0.054337  -0.12388    0.064828\n",
      "  0.27453    0.5165    -0.1955    -0.55939   -0.2744    -0.12146  ]\n",
      "brown:  [ 0.2793     0.18372   -0.11257    0.21734   -0.21657   -0.50335\n",
      " -0.27194    0.32181    0.031892  -0.37998    0.15544   -0.32953\n",
      " -0.19827    0.20403    0.26768    0.292     -0.34187   -0.10766\n",
      " -0.43697   -0.14488    0.14634    0.21591    0.12576    0.14895\n",
      " -0.21763    0.030797   0.10949   -0.41689   -0.30296   -0.14592\n",
      " -0.56228    0.33282   -0.20436   -0.24403   -1.4732     0.68345\n",
      "  0.45336    0.43671   -0.15641    0.15075   -0.24265   -0.040059\n",
      "  0.22323    0.19523    0.37445   -0.18509   -0.10302   -0.055363\n",
      " -0.17274   -0.45401   -0.14729   -0.24133   -0.043826  -0.23243\n",
      "  0.42367    0.15906   -0.14039   -0.36185   -0.26695   -0.42724\n",
      " -0.08843   -0.099597   0.24257   -0.05424    0.10746   -1.1304\n",
      "  0.024651  -0.10212    0.046319  -0.68792    0.4214    -0.25844\n",
      "  0.17052    0.097878   0.026835   0.32044    0.0062988  0.24575\n",
      "  0.20126   -0.16771    0.19825    0.28939   -0.064994  -0.38766\n",
      "  0.52509    0.38195    0.32421    0.20683   -0.48472   -0.080334\n",
      " -0.15345    0.35459   -0.43765    0.071575  -0.39516   -0.22906\n",
      "  0.25686    0.26659    0.37626   -0.18556    0.16445   -0.33614\n",
      " -0.56262    0.067852  -0.61642    0.19546    0.45027    0.20238\n",
      "  0.33957    0.41372    0.11855    0.087619   0.18754    0.17901\n",
      "  0.022569  -0.10854   -0.47226    0.41039    0.32588   -0.58468\n",
      " -0.0057296 -0.29201   -0.12777   -0.15729   -0.40103   -0.039414\n",
      " -0.1192     0.40093    0.032862   0.39862   -0.63525    0.11594\n",
      " -0.39954    0.36919   -0.50021   -0.51169   -0.13955    0.18055\n",
      " -0.079918  -0.19474    0.53131    0.093723   0.2773    -0.40505\n",
      " -0.20568    0.11139    0.032661  -0.04852    0.44576    0.23667\n",
      "  0.54981    0.23585   -0.51539   -0.46424    0.021099  -0.3919\n",
      "  0.58338   -0.89908    0.094066   0.30159   -0.063199  -0.31635\n",
      "  0.50333   -0.068517  -0.38681    0.33      -0.49463    0.75491\n",
      " -0.088266  -0.19413    0.4238    -0.031727  -0.4464    -0.21028\n",
      " -0.11151   -0.07088   -0.027832  -0.63304    0.27336   -0.47925\n",
      " -0.03239    0.46069    0.16968   -0.38262   -0.31413   -0.29068\n",
      " -0.031801  -0.48974   -0.50999    0.1466     0.0027995  0.56333\n",
      " -0.044347  -0.085679   0.20559   -0.051593   0.75228   -0.013291\n",
      " -0.084694  -0.4305     1.1734    -0.083233   0.1561    -0.15758\n",
      "  0.19066   -0.2966     0.63704    0.45616   -0.34797   -0.12732\n",
      "  0.4901    -0.51217   -0.063474  -0.061496   0.28825    0.17711\n",
      "  0.46301   -0.12697   -0.044627  -1.0064     0.76394    0.20494\n",
      "  0.028766   0.27597    0.021726  -0.12054    0.23284    0.18999\n",
      "  0.30048   -0.056139   0.09546   -0.036514   0.0084885  0.016599\n",
      " -0.31428   -0.2707     0.099281   0.4445    -0.36      -0.55556\n",
      " -0.18551   -0.30644    0.056475  -0.19197   -0.48886    0.33044\n",
      "  0.19535   -0.53828    0.12385   -0.29372   -0.1036     0.0051129\n",
      "  0.11483   -0.10591    0.73337    0.26978   -0.06925    0.11565\n",
      "  0.27711    0.15109   -0.069137  -0.14481    0.32319    0.039345\n",
      " -0.44964    0.27103    0.045326  -0.064534  -0.37144    0.47615\n",
      " -0.61105   -0.11922   -0.068806   0.15401   -0.40812    0.32575\n",
      " -1.2888     0.0203    -0.12893   -0.22211   -0.16402    0.29018\n",
      "  0.36295   -0.081025  -0.50492    0.5046    -0.37485    0.52111\n",
      "  0.1757     0.069686   0.48937   -0.17747   -0.20577    0.70419\n",
      "  0.068633   0.47878   -0.21754   -0.016868  -0.91378    0.45643  ]\n",
      "fox:  [-1.1570e-01 -2.5048e-02 -1.1013e-01 -4.8060e-02 -8.5504e-02  3.7308e-01\n",
      " -9.4790e-01  5.9662e-01 -2.0006e-02 -3.0469e-01  2.4997e-01 -2.0005e-01\n",
      "  1.9284e-01  4.7197e-01  2.7099e-01  3.4190e-01 -2.9186e-01 -3.9718e-01\n",
      "  5.5653e-01  3.2774e-01  9.4860e-02 -4.6588e-01  6.2119e-01  2.0709e-01\n",
      "  8.2332e-02 -1.0175e-01  3.2067e-01 -3.1875e-01  1.7941e-01 -2.4127e-01\n",
      " -1.7961e-02 -8.7154e-02  3.3423e-01 -4.5026e-02 -1.2193e+00  1.9321e-01\n",
      "  2.1427e-01  2.3893e-01 -1.8084e-01 -1.5920e-01 -1.0387e-01  2.2060e-01\n",
      "  2.5423e-01  2.0815e-01 -3.3072e-01 -2.0672e-01 -3.1424e-01  2.6498e-02\n",
      "  2.1955e-01 -3.1157e-01 -7.8914e-02 -1.5089e-02 -4.0848e-02 -2.3807e-01\n",
      "  3.1983e-01 -7.4173e-02  2.2983e-01  5.5735e-02  5.6743e-03 -7.7368e-01\n",
      "  2.7412e-01 -1.8999e-01  5.3825e-01 -1.9784e-01 -1.0898e-01 -3.4022e-01\n",
      "  3.1582e-01  5.0317e-01  3.3955e-01 -2.7143e-01 -3.3026e-02  3.4279e-01\n",
      " -4.9815e-01  2.5056e-01 -1.2791e-01 -1.5912e-01 -8.7541e-03  1.0450e-01\n",
      "  2.1969e-01 -5.9077e-02 -3.7600e-01  6.0835e-02  1.8427e-01 -8.3592e-02\n",
      "  1.1632e-01  7.5145e-01  6.5186e-02 -3.3258e-01  1.4249e-01 -6.5348e-01\n",
      " -2.6661e-03  1.0233e-01 -3.8731e-01  6.0529e-01 -2.7326e-01 -3.5777e-01\n",
      " -5.3533e-01  4.9589e-02  2.0224e-01 -3.2805e-01  3.9305e-01 -2.7540e-01\n",
      " -1.3042e-01  9.5833e-01  3.8853e-01  5.9884e-01 -3.0503e-01 -1.0708e-01\n",
      "  4.5670e-01  7.2182e-01 -4.4221e-02 -6.8201e-02 -2.9903e-01  2.6483e-01\n",
      "  6.6810e-03  1.4919e-01 -5.3155e-01 -6.1697e-01 -5.4271e-01 -4.1103e-01\n",
      " -1.7534e-01  1.3229e-01  2.6958e-02 -1.5111e-01 -1.6844e-01 -3.0684e-01\n",
      " -5.1189e-02  9.2940e-01 -6.0882e-01 -1.1373e-01 -4.2237e-01 -4.0481e-01\n",
      " -3.4182e-01  3.8472e-01 -1.1566e-01  2.2679e-02  4.4793e-01  1.8849e-01\n",
      " -1.9598e-01 -6.8914e-02 -4.4520e-01  4.1643e-01  5.8319e-02 -3.9575e-01\n",
      " -4.0471e-01  4.2855e-01  1.6339e-01 -1.6965e-01 -5.7793e-02 -1.0358e-01\n",
      "  1.0223e+00  9.0564e-01 -5.5851e-01 -2.4856e-01  7.5582e-02 -2.7374e-01\n",
      "  3.8838e-01  5.1561e-01  6.0029e-02 -5.1990e-02  1.0113e+00  8.3607e-03\n",
      "  4.9764e-01  3.2839e-02  2.5892e-02  4.0404e-01 -3.7393e-01  1.3384e-01\n",
      " -2.9458e-01  3.4668e-01  5.2114e-02  1.0785e-01 -8.4700e-01  4.0936e-01\n",
      "  3.8801e-01  4.5079e-01 -4.3880e-01  1.9777e-01  1.9102e-01 -7.6580e-02\n",
      "  2.5788e-01  3.5642e-01  2.3305e-01 -1.6206e-01 -5.0847e-02  1.3708e-01\n",
      "  5.1338e-02 -3.8552e-01 -2.5497e-01  5.3901e-02 -6.2922e-01  3.2780e-01\n",
      " -2.0383e-01 -1.7265e-01 -9.9668e-02  2.4485e-01  3.2700e-02  3.2325e-01\n",
      "  2.3912e-01  7.7014e-03  1.2890e+00  6.8030e-02  2.0671e-02  2.6268e-01\n",
      "  1.9434e-01  5.6238e-01  7.1863e-02 -6.5502e-02 -2.2858e-01  9.2883e-02\n",
      "  3.9936e-01 -2.8038e-01  3.5633e-01  1.5203e-01 -2.7883e-01  3.8225e-02\n",
      " -3.9636e-01  3.2488e-01  2.4332e-01  2.2621e-01  2.3518e-01 -6.4747e-02\n",
      "  6.2858e-01 -1.1848e-01  1.2710e-01 -1.2697e-01  1.0015e+00 -2.5693e-01\n",
      "  2.0450e-01 -1.8789e-01 -1.2913e-01 -9.6456e-02  1.2301e-01 -1.3377e-01\n",
      " -1.2808e-01  4.1857e-02  8.6418e-01  3.6137e-01  9.0372e-02 -1.0687e-02\n",
      "  4.4736e-01  7.8975e-02 -3.2992e-01 -1.4470e-01 -6.1296e-01 -4.1543e-01\n",
      " -2.6104e-01 -8.4169e-01  4.7352e-01 -4.7640e-01 -3.4393e-01  8.2156e-02\n",
      " -2.4477e-01  7.9361e-01  2.6183e-01  4.6648e-01  1.9135e-01 -5.5033e-02\n",
      "  2.3492e-01  1.1178e-01 -3.7291e-01  2.6764e-01  1.1074e-01  6.8041e-01\n",
      " -1.0361e-03 -4.2537e-02  9.4006e-01 -2.8939e-01 -2.0841e-01  3.3001e-01\n",
      "  7.3318e-02 -5.1902e-01 -3.6412e-01 -4.1372e-01  2.0266e-01  4.3162e-01\n",
      " -1.0174e+00  2.5093e-01  3.8136e-01 -1.4332e-01 -2.0525e-01 -1.8724e-01\n",
      "  5.9925e-01  3.5604e-01 -9.1457e-01 -2.2401e-01 -4.2382e-02 -1.3299e-01\n",
      "  6.1671e-01  4.4886e-01 -1.1361e-01  2.0483e-01  2.5485e-01  2.7581e-01\n",
      " -8.7158e-01 -1.3156e-01  2.6117e-01  1.5815e-01 -3.2199e-01  7.1042e-01]\n",
      "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "over:  [-8.8137e-02 -2.1696e-02  2.9863e-01 -1.8325e-02 -2.3575e-01  1.1022e-01\n",
      " -1.7493e-01  9.9241e-03  2.3832e-01 -1.7643e+00  2.2489e-01  4.0552e-01\n",
      " -4.8176e-01 -6.6099e-02  1.3290e-01  4.7502e-01 -5.6438e-02  3.2902e-01\n",
      "  9.7628e-02  4.2467e-01  2.5285e-01 -1.7258e-01  7.6564e-02 -1.5678e-01\n",
      " -2.2694e-01 -2.1213e-01 -3.3460e-01  7.3842e-02 -4.4671e-01  3.3979e-01\n",
      " -2.3534e-01  1.5013e-01 -3.4718e-01  5.4379e-02 -8.8699e-01 -3.5534e-01\n",
      " -1.3166e-01 -1.8265e-02 -1.8708e-01  3.0193e-01  1.0008e-01 -1.1980e-01\n",
      " -7.4867e-01  1.5592e-01  4.2757e-01 -2.9996e-01 -1.2499e-01  1.2750e-01\n",
      " -2.1962e-01  4.5077e-01 -3.2268e-01 -2.8934e-01 -1.3745e-01 -1.7738e-01\n",
      "  2.4185e-01 -1.2240e-02  1.7264e-01  2.7287e-01 -8.2743e-02  2.0570e-01\n",
      "  2.2559e-02 -7.5793e-02  2.1027e-01 -3.1239e-01  1.3032e-01 -7.5693e-01\n",
      "  9.0872e-02  4.9975e-01  1.1746e-01 -5.3133e-01 -8.5348e-02  4.4042e-01\n",
      "  1.2231e-01 -1.8672e-01  1.1668e-01  1.0322e-02 -2.6779e-01  1.5042e-01\n",
      " -5.4751e-01 -3.8439e-01 -6.7117e-01 -3.4912e-01  1.4190e-01  1.2742e-01\n",
      " -1.2879e-01 -1.3582e-01  2.9059e-01  1.4339e-01  2.8467e-01  7.0642e-02\n",
      "  4.7118e-03  1.8321e-01 -4.0973e-01  2.1973e-01 -6.6693e-01 -1.0599e-01\n",
      " -3.4465e-01  2.3103e-01 -3.6278e-01 -1.2365e-02  1.1544e-01  3.6134e-02\n",
      "  7.4889e-02 -6.4933e-01 -3.3362e-02 -3.0498e-01  4.0964e-01  2.6898e-01\n",
      " -2.1150e-01 -1.1446e-01 -4.8326e-02 -5.9265e-01 -3.7301e-01 -5.4902e-02\n",
      " -1.9869e-01  2.3181e-01 -2.9017e-01  6.2177e-01  8.9856e-02 -6.4116e-01\n",
      "  3.9304e-02  9.7170e-02 -2.9954e-01  4.0426e-01  8.4010e-02  1.2902e-01\n",
      " -3.5864e-01  3.9975e-01  1.3183e-01 -1.4797e-01 -3.1449e-01  4.8654e-01\n",
      "  2.9974e-02  5.3637e-01  1.1619e-01  1.4643e-01 -2.2927e-01  7.9999e-02\n",
      "  1.3114e-01 -3.0946e-02  1.6410e-01 -3.5597e-01  2.0801e-01  3.3193e-01\n",
      " -8.4092e-01  3.3762e-01 -7.5518e-02 -4.3158e-01  1.9592e-01 -2.7815e-01\n",
      "  6.6744e-01  3.1189e-02 -1.1706e-02  5.3737e-01  2.9596e-01  3.7335e-01\n",
      " -2.2166e-01  2.3843e-02 -1.1905e-01 -1.8529e-01  1.1857e-02 -2.3540e-01\n",
      "  2.3800e-01 -3.1987e-02 -1.6400e-01 -7.9319e-02  9.9570e-02  2.6182e-02\n",
      " -6.0793e-01 -6.7515e-02  3.9870e-02  7.0721e-02 -6.0965e-01 -7.3549e-01\n",
      " -8.1241e-03 -1.3363e-01 -1.6572e-01  3.2373e-01  1.0115e-01  6.0659e-01\n",
      " -2.2547e-02  2.5960e-01  2.4251e-01 -7.8426e-02 -7.8699e-02 -4.1680e-01\n",
      " -1.4208e-01  2.5136e-02 -3.4187e-01  2.7631e-01  2.4892e-01  2.7898e-01\n",
      " -2.6522e-01 -3.3989e-01  1.7929e-01 -4.0393e-01  2.7421e-01  2.2639e-01\n",
      "  4.5234e-01 -2.4806e-01  9.4945e-01  1.4040e-01  2.3117e-02 -3.3321e-01\n",
      "  3.6958e-01 -1.7507e-01 -1.8646e-01 -2.4238e-01  2.0633e-02 -7.8795e-02\n",
      "  5.1631e-01  4.5651e-01  2.5398e-01  3.8711e-01  1.6812e-01 -5.8263e-01\n",
      " -9.9027e-02 -2.3445e-01  3.6928e-02  6.1330e-02  2.4186e-01 -2.0908e-01\n",
      " -3.8720e-02  1.4625e-01 -3.6779e-02  2.5993e-01  3.1897e-01 -5.5307e-02\n",
      " -7.7177e-02  3.0117e-01  2.3307e-01  4.1980e-01 -1.3339e-01 -2.0384e-02\n",
      "  7.1158e-01  8.4989e-02  3.2791e-01  4.8060e-01 -3.3761e-01  5.7392e-02\n",
      " -2.6536e-01  3.0228e-01  1.8540e-01 -8.3752e-02 -7.3776e-01 -1.4404e-03\n",
      "  3.4121e-01  2.9959e-02  4.2035e-01  7.3122e-02 -1.9430e-01 -2.2341e-01\n",
      "  5.8658e-01 -2.2756e-01  6.3029e-01 -6.8682e-02 -3.5365e-01  2.6851e-01\n",
      " -7.3124e-02  1.5702e-02  1.2022e-01 -2.4631e-01  2.8471e-01  3.8292e-01\n",
      " -3.8869e-01  2.0974e-01 -2.0512e-01 -1.8508e-01  3.2543e-01  3.7336e-03\n",
      " -1.9901e-02 -8.4242e-02  2.9326e-01 -5.8162e-02  3.6575e-01  4.6895e-02\n",
      " -2.1328e+00 -3.5109e-01  4.1716e-01  1.8393e-01 -3.0990e-01  2.7303e-01\n",
      "  1.6627e-02  4.4978e-03 -7.2902e-02  5.3814e-02 -5.7213e-02  1.5815e-01\n",
      "  1.4517e-01  1.6580e-01 -7.3557e-02  1.6125e-01 -9.9237e-02  2.6099e-01\n",
      "  2.7941e-02  3.2140e-01  1.6931e-01 -3.5469e-01 -4.2713e-01 -3.9323e-01]\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "dog:  [-1.1043e-01  8.1217e-01  7.3668e-02  1.9023e-01 -5.2888e-02  6.1468e-02\n",
      "  1.6076e-01  4.1302e-01 -3.0199e-01 -9.0827e-01  2.7504e-01 -3.1890e-02\n",
      " -2.8842e-01  2.3447e-01  4.7679e-01  5.0124e-01  2.9371e-01  2.7029e-01\n",
      "  5.4745e-02  9.8038e-02  5.7116e-01  3.6755e-01  4.0734e-02  3.4347e-01\n",
      " -1.8256e-01 -2.8935e-01  2.3826e-02 -1.9401e-01  2.4444e-01  1.3407e-01\n",
      " -1.6494e-01 -2.6983e-01 -2.6234e-01 -2.1779e-01 -8.7528e-01  7.3822e-01\n",
      " -8.7931e-02 -1.0876e-02 -2.6540e-01  3.4668e-01 -5.5814e-01  1.7591e-01\n",
      "  1.6926e-01 -1.5725e-01 -5.0430e-01 -2.0100e-01  6.6701e-01 -3.2518e-02\n",
      "  4.5012e-02  6.5675e-02 -1.6061e-01 -7.3363e-01  2.4642e-01  3.4325e-01\n",
      "  2.1899e-01  4.8646e-02 -5.9987e-01 -5.8153e-02 -5.1694e-02 -5.7846e-01\n",
      "  3.0000e-01  3.5078e-01  4.6646e-01 -7.5309e-03  1.0455e-01 -5.1016e-01\n",
      " -5.5987e-02 -1.0295e-01 -2.6476e-01 -4.1230e-02 -2.8371e-02  5.1979e-01\n",
      " -3.4849e-01 -4.7217e-01 -3.7229e-01 -3.2790e-02  1.3989e-01  3.5716e-01\n",
      "  1.9305e-01 -2.1986e-01  2.4136e-01  4.0976e-01  3.7516e-01  1.4255e-01\n",
      " -3.4143e-02 -7.2653e-01 -1.0832e-01  6.8616e-01 -2.6335e-01 -4.2345e-01\n",
      " -2.4253e-01  1.5778e-01  1.4258e-01 -3.2749e-01 -3.4699e-01  1.6148e-01\n",
      "  1.9603e-01  4.1639e-01 -2.3370e-01  7.5816e-02  1.5899e-01  1.6623e-03\n",
      " -4.8301e-02 -1.0611e-01 -1.9326e-01  1.4494e-01  1.5406e-02  1.0629e-01\n",
      " -3.6699e-02  6.3230e-01  1.2986e-01  4.9902e-01 -1.1323e+00 -1.2636e-01\n",
      "  6.4718e-02  1.2374e-01 -4.9712e-01 -1.4836e-02  1.0488e-01 -4.9818e-01\n",
      " -2.8856e-01  3.8949e-01 -3.1828e-02 -2.8625e-01 -9.8758e-02 -7.6990e-02\n",
      " -2.4234e-01  7.5793e-01  3.4835e-01 -7.1030e-01  4.5318e-01 -3.4418e-01\n",
      " -1.9459e-01  6.1478e-01 -2.9010e-02 -2.7864e-01  3.8556e-01  1.0072e-01\n",
      "  1.2895e-01  1.7992e-02  3.3670e-01  2.0698e-01 -3.8049e-01 -6.6661e-03\n",
      "  1.1540e-01 -8.5268e-02 -1.4608e-01  4.4514e-01 -9.3674e-02  2.3639e-01\n",
      " -1.1447e-01  1.0948e+00 -5.7823e-02 -1.6295e-01  5.5880e-01 -1.8988e-02\n",
      " -7.1374e-02  2.1319e-01  6.1277e-02  7.2759e-01  6.2747e-01 -1.9280e-01\n",
      "  1.3057e-01  1.7426e-01 -1.0229e-01  1.5232e-01  5.2500e-01 -2.1919e-01\n",
      " -2.7185e-01 -5.4186e-01  3.1752e-01  1.6375e-01 -2.9039e-01  1.7074e-01\n",
      " -3.1814e-01 -9.6421e-01 -1.1610e-01 -2.9951e-01  1.8686e-01 -4.5986e-01\n",
      "  4.1633e-01 -1.7583e-01 -3.4583e-01 -2.7244e-01 -5.0216e-01  1.2852e-02\n",
      "  5.9838e-01 -1.1237e-01  2.4697e-01 -4.9048e-01 -4.4188e-01 -1.6255e-01\n",
      " -7.3313e-01 -3.7677e-01 -6.8925e-01  6.1174e-02 -4.2101e-01 -1.3153e-01\n",
      " -8.3590e-03 -1.8360e-02  1.3686e+00  4.6169e-02  9.4622e-01 -1.5126e-02\n",
      " -1.2477e-01  4.8754e-01  2.2384e-01 -2.1820e-01 -2.3389e-01  1.5207e-01\n",
      " -2.8718e-01 -6.3908e-01 -2.2383e-01 -1.8014e-01 -3.3548e-01  5.3587e-01\n",
      " -2.9367e-01  1.0866e-01  6.3411e-02 -9.3424e-03 -1.5886e-01  2.2602e-01\n",
      "  1.1925e-01 -4.1442e-01 -7.8062e-02 -9.7857e-02  2.7938e-01 -1.8348e-01\n",
      " -3.4584e-01  1.8489e-01  1.7402e-01 -5.2198e-01 -4.3306e-01  1.6256e-01\n",
      "  1.4032e-01  3.5124e-01 -1.8280e-01 -3.5984e-01 -1.3009e-01  1.6304e-01\n",
      "  3.1734e-01  3.7716e-03 -4.5498e-02 -4.2066e-01 -4.4419e-01 -6.8985e-01\n",
      " -4.9359e-01  7.0281e-02 -1.4377e-01  6.2508e-01 -5.6311e-02  1.8850e-01\n",
      " -5.6785e-02  1.4052e-01  1.1973e+00  7.1894e-01  5.4332e-01 -1.2461e-01\n",
      " -1.1978e-01  3.0163e-01 -1.6273e-01 -4.6740e-02 -2.5249e-01 -3.0659e-02\n",
      " -3.2271e-01  3.2361e-01  3.3244e-01 -2.7819e-02 -3.3367e-01 -2.3444e-02\n",
      " -5.0394e-01 -2.0587e-01 -1.3013e-01 -3.5884e-01  4.5384e-02 -1.1863e-01\n",
      " -1.7257e+00  3.9441e-01 -5.3179e-01  5.8209e-01 -6.5771e-01  3.6849e-01\n",
      "  2.3518e-01  1.0802e-01 -8.3159e-01  6.1486e-01  2.5547e-01 -4.5289e-01\n",
      "  5.1446e-01 -1.7911e-01 -1.2389e-01  1.8688e-01 -4.1102e-01 -7.0877e-01\n",
      " -3.7501e-01 -6.6152e-01  6.7730e-01  3.3936e-01  5.7994e-01  6.8149e-02]\n",
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-positive\n",
      "max_review_length: 1052\n",
      "min_review_length: 22\n",
      "First word in first document: while\n",
      "Embedding for this word:\n",
      " [-1.9472e-01  1.8836e-01  1.1739e-01 -1.8991e-03 -1.8963e-01 -6.7638e-02\n",
      " -1.3963e-01 -3.6326e-02 -1.6799e-02 -1.4820e+00 -2.7629e-03  9.8703e-02\n",
      "  5.9868e-02  1.5742e-01  1.4054e-01  1.3313e-01 -1.3587e-01  7.9241e-02\n",
      " -5.7048e-02 -3.3121e-01 -1.0595e-01  3.1271e-01  4.3222e-01 -9.9310e-02\n",
      " -1.8090e-01 -2.3958e-01  7.0335e-02  5.7132e-02  4.9771e-02  1.6937e-01\n",
      " -1.4879e-01  3.5118e-01  3.2477e-01  6.3851e-02 -1.2043e+00 -3.8819e-01\n",
      "  1.9153e-01  1.9014e-03 -1.5201e-01  2.2219e-01  1.3716e-01 -4.2525e-02\n",
      " -3.3781e-01 -1.0667e-01 -1.4212e-01  2.5880e-02  3.8361e-01  3.6539e-01\n",
      " -4.2780e-02  7.7897e-02 -5.6149e-02 -2.6875e-01  2.4329e-01 -4.7278e-03\n",
      " -2.3142e-03  1.2450e-01 -4.9689e-03  3.3361e-02  1.3893e-01 -3.6135e-03\n",
      " -4.0820e-02  5.3111e-02  3.8276e-01  1.1413e-01 -1.1556e-01 -6.7151e-01\n",
      "  1.0093e-01  1.6463e-02  2.2443e-02  3.4206e-02 -1.1453e-01 -5.2428e-02\n",
      "  3.0135e-02 -5.9207e-02  2.4909e-02 -1.6277e-02  1.6996e-01  1.9266e-01\n",
      " -2.4422e-01  7.5231e-02  1.3790e-01 -2.5580e-01 -9.1647e-02  2.3055e-01\n",
      "  7.9488e-02 -5.5318e-03 -1.1481e-02  1.4759e-01  1.4456e-01  8.6181e-02\n",
      "  2.8940e-01  1.2176e-01 -9.0553e-02  2.2209e-01 -5.7667e-02 -3.2162e-01\n",
      " -3.2202e-01  3.0178e-02 -6.3200e-02 -5.1000e-01  2.5363e-02  3.1580e-02\n",
      " -2.1081e-01 -3.2171e-01 -1.2988e-01  4.6144e-01  2.0522e-01  3.8018e-01\n",
      " -4.6028e-01 -5.5502e-02 -5.9889e-02 -2.3684e-01 -1.7002e-02 -2.6099e-02\n",
      "  1.8669e-01  1.9199e-01 -1.4562e-01  9.6590e-02 -6.1454e-02 -5.6896e-02\n",
      " -5.1130e-01 -3.2049e-03 -1.0491e-01 -4.4255e-02 -3.7002e-01 -2.1858e-01\n",
      " -1.0680e-01 -2.2429e-01  2.7597e-01 -2.3302e-01 -4.2686e-01  5.4869e-04\n",
      "  2.8455e-01  2.1347e-02 -5.4664e-02  6.6111e-02  1.5381e-01 -1.2572e-01\n",
      "  2.7120e-01  3.3996e-01  2.5948e-01  1.8138e-01 -3.2012e-02  2.1576e-01\n",
      " -3.5938e-01  1.7972e-01  9.9849e-02 -2.8064e-01 -3.3303e-02 -8.4656e-02\n",
      "  2.8755e-01 -1.7199e-01 -9.4199e-02  1.3209e-01  4.4943e-01 -1.9775e-01\n",
      " -5.1901e-01 -4.9810e-02 -8.9386e-03  5.0636e-02  4.7966e-01 -6.0549e-01\n",
      "  1.7089e-01 -1.8651e-01  3.6259e-02  1.8433e-01 -1.2240e-01 -4.2995e-02\n",
      "  1.0503e-01  2.4983e-01  1.0062e-01 -2.0852e-01 -3.3951e-01  1.8636e-01\n",
      "  9.1880e-02 -2.8719e-02  7.2556e-02  2.0066e-01 -3.1819e-01  6.5279e-01\n",
      " -1.0925e-01  2.0030e-01  1.2181e-02 -1.4259e-01 -5.0102e-02 -1.7363e-02\n",
      " -6.9740e-03  1.6336e-01 -8.0266e-02 -1.7524e-01  1.3976e-02  1.5656e-01\n",
      " -3.2603e-01 -3.3087e-01 -2.7052e-01 -1.3885e-01 -1.6769e-01 -1.4385e-01\n",
      "  2.4883e-01 -1.0426e-01  7.9906e-01 -2.9098e-01 -1.7864e-01  4.1688e-01\n",
      " -7.4282e-02  7.7736e-02  6.3283e-02 -9.4733e-02  1.0527e-01 -1.2301e-01\n",
      " -2.7754e-02  1.3078e-01  4.2556e-02 -5.1533e-02  4.2722e-01  2.4317e-01\n",
      " -2.0954e-01  3.4540e-01 -1.1758e-01  1.7618e-01  3.0023e-01 -9.8725e-02\n",
      " -5.1650e-02  9.9928e-02 -3.6114e-01  8.4246e-02  1.9471e-01 -1.5502e-01\n",
      " -1.5988e-01  9.3713e-02 -5.5386e-02 -5.3721e-02  9.5336e-02  3.3402e-01\n",
      "  4.6005e-01 -2.6252e-01  1.1194e-01  3.0805e-01 -1.6688e-01  3.1467e-01\n",
      " -1.0996e-01  6.0206e-01  2.3838e-01  3.4277e-01 -3.7964e-01  2.0090e-03\n",
      "  5.7560e-01 -2.1853e-01 -1.5855e-01  3.4270e-01  3.3626e-02 -2.2984e-02\n",
      "  2.9521e-01  5.0213e-03  1.6369e-01 -1.9080e-01 -3.4195e-01 -1.9340e-01\n",
      " -1.5975e-01 -8.4150e-03  1.5673e-01 -1.2553e-01 -5.1382e-02  1.1615e-01\n",
      " -6.0482e-02  4.4293e-01  2.2198e-02 -2.9353e-01 -4.2204e-01  2.1842e-01\n",
      " -7.7585e-02 -1.0179e-01 -3.0344e-02  2.8122e-01 -1.0290e-01  1.5925e-01\n",
      " -1.8687e+00 -3.3726e-01  8.6103e-02  1.9028e-01 -4.1232e-01  4.2654e-02\n",
      " -7.0320e-02 -3.7245e-01  2.6863e-02  4.5806e-01 -3.7607e-01  5.3417e-01\n",
      "  1.1403e-01 -2.3730e-01  2.5681e-02  2.4612e-02 -3.5385e-02  2.2309e-02\n",
      "  2.6702e-01  4.1790e-01  6.4943e-03 -3.8294e-01 -2.5358e-01 -1.2843e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-1.9472e-01  1.8836e-01  1.1739e-01 -1.8991e-03 -1.8963e-01 -6.7638e-02\n",
      " -1.3963e-01 -3.6326e-02 -1.6799e-02 -1.4820e+00 -2.7629e-03  9.8703e-02\n",
      "  5.9868e-02  1.5742e-01  1.4054e-01  1.3313e-01 -1.3587e-01  7.9241e-02\n",
      " -5.7048e-02 -3.3121e-01 -1.0595e-01  3.1271e-01  4.3222e-01 -9.9310e-02\n",
      " -1.8090e-01 -2.3958e-01  7.0335e-02  5.7132e-02  4.9771e-02  1.6937e-01\n",
      " -1.4879e-01  3.5118e-01  3.2477e-01  6.3851e-02 -1.2043e+00 -3.8819e-01\n",
      "  1.9153e-01  1.9014e-03 -1.5201e-01  2.2219e-01  1.3716e-01 -4.2525e-02\n",
      " -3.3781e-01 -1.0667e-01 -1.4212e-01  2.5880e-02  3.8361e-01  3.6539e-01\n",
      " -4.2780e-02  7.7897e-02 -5.6149e-02 -2.6875e-01  2.4329e-01 -4.7278e-03\n",
      " -2.3142e-03  1.2450e-01 -4.9689e-03  3.3361e-02  1.3893e-01 -3.6135e-03\n",
      " -4.0820e-02  5.3111e-02  3.8276e-01  1.1413e-01 -1.1556e-01 -6.7151e-01\n",
      "  1.0093e-01  1.6463e-02  2.2443e-02  3.4206e-02 -1.1453e-01 -5.2428e-02\n",
      "  3.0135e-02 -5.9207e-02  2.4909e-02 -1.6277e-02  1.6996e-01  1.9266e-01\n",
      " -2.4422e-01  7.5231e-02  1.3790e-01 -2.5580e-01 -9.1647e-02  2.3055e-01\n",
      "  7.9488e-02 -5.5318e-03 -1.1481e-02  1.4759e-01  1.4456e-01  8.6181e-02\n",
      "  2.8940e-01  1.2176e-01 -9.0553e-02  2.2209e-01 -5.7667e-02 -3.2162e-01\n",
      " -3.2202e-01  3.0178e-02 -6.3200e-02 -5.1000e-01  2.5363e-02  3.1580e-02\n",
      " -2.1081e-01 -3.2171e-01 -1.2988e-01  4.6144e-01  2.0522e-01  3.8018e-01\n",
      " -4.6028e-01 -5.5502e-02 -5.9889e-02 -2.3684e-01 -1.7002e-02 -2.6099e-02\n",
      "  1.8669e-01  1.9199e-01 -1.4562e-01  9.6590e-02 -6.1454e-02 -5.6896e-02\n",
      " -5.1130e-01 -3.2049e-03 -1.0491e-01 -4.4255e-02 -3.7002e-01 -2.1858e-01\n",
      " -1.0680e-01 -2.2429e-01  2.7597e-01 -2.3302e-01 -4.2686e-01  5.4869e-04\n",
      "  2.8455e-01  2.1347e-02 -5.4664e-02  6.6111e-02  1.5381e-01 -1.2572e-01\n",
      "  2.7120e-01  3.3996e-01  2.5948e-01  1.8138e-01 -3.2012e-02  2.1576e-01\n",
      " -3.5938e-01  1.7972e-01  9.9849e-02 -2.8064e-01 -3.3303e-02 -8.4656e-02\n",
      "  2.8755e-01 -1.7199e-01 -9.4199e-02  1.3209e-01  4.4943e-01 -1.9775e-01\n",
      " -5.1901e-01 -4.9810e-02 -8.9386e-03  5.0636e-02  4.7966e-01 -6.0549e-01\n",
      "  1.7089e-01 -1.8651e-01  3.6259e-02  1.8433e-01 -1.2240e-01 -4.2995e-02\n",
      "  1.0503e-01  2.4983e-01  1.0062e-01 -2.0852e-01 -3.3951e-01  1.8636e-01\n",
      "  9.1880e-02 -2.8719e-02  7.2556e-02  2.0066e-01 -3.1819e-01  6.5279e-01\n",
      " -1.0925e-01  2.0030e-01  1.2181e-02 -1.4259e-01 -5.0102e-02 -1.7363e-02\n",
      " -6.9740e-03  1.6336e-01 -8.0266e-02 -1.7524e-01  1.3976e-02  1.5656e-01\n",
      " -3.2603e-01 -3.3087e-01 -2.7052e-01 -1.3885e-01 -1.6769e-01 -1.4385e-01\n",
      "  2.4883e-01 -1.0426e-01  7.9906e-01 -2.9098e-01 -1.7864e-01  4.1688e-01\n",
      " -7.4282e-02  7.7736e-02  6.3283e-02 -9.4733e-02  1.0527e-01 -1.2301e-01\n",
      " -2.7754e-02  1.3078e-01  4.2556e-02 -5.1533e-02  4.2722e-01  2.4317e-01\n",
      " -2.0954e-01  3.4540e-01 -1.1758e-01  1.7618e-01  3.0023e-01 -9.8725e-02\n",
      " -5.1650e-02  9.9928e-02 -3.6114e-01  8.4246e-02  1.9471e-01 -1.5502e-01\n",
      " -1.5988e-01  9.3713e-02 -5.5386e-02 -5.3721e-02  9.5336e-02  3.3402e-01\n",
      "  4.6005e-01 -2.6252e-01  1.1194e-01  3.0805e-01 -1.6688e-01  3.1467e-01\n",
      " -1.0996e-01  6.0206e-01  2.3838e-01  3.4277e-01 -3.7964e-01  2.0090e-03\n",
      "  5.7560e-01 -2.1853e-01 -1.5855e-01  3.4270e-01  3.3626e-02 -2.2984e-02\n",
      "  2.9521e-01  5.0213e-03  1.6369e-01 -1.9080e-01 -3.4195e-01 -1.9340e-01\n",
      " -1.5975e-01 -8.4150e-03  1.5673e-01 -1.2553e-01 -5.1382e-02  1.1615e-01\n",
      " -6.0482e-02  4.4293e-01  2.2198e-02 -2.9353e-01 -4.2204e-01  2.1842e-01\n",
      " -7.7585e-02 -1.0179e-01 -3.0344e-02  2.8122e-01 -1.0290e-01  1.5925e-01\n",
      " -1.8687e+00 -3.3726e-01  8.6103e-02  1.9028e-01 -4.1232e-01  4.2654e-02\n",
      " -7.0320e-02 -3.7245e-01  2.6863e-02  4.5806e-01 -3.7607e-01  5.3417e-01\n",
      "  1.1403e-01 -2.3730e-01  2.5681e-02  2.4612e-02 -3.5385e-02  2.2309e-02\n",
      "  2.6702e-01  4.1790e-01  6.4943e-03 -3.8294e-01 -2.5358e-01 -1.2843e-01]\n",
      "First word in first document: officially\n",
      "Embedding for this word:\n",
      " [ 2.3382e-01  1.4786e-01  1.1077e-01  2.3996e-01 -2.9585e-01 -1.6727e-01\n",
      " -2.8175e-02  1.6083e-01  1.1507e-01 -1.1158e+00  1.5384e-01  1.6024e-01\n",
      "  7.2660e-02 -1.6972e-01  6.8051e-01 -1.2424e-01  6.3706e-02 -5.2588e-02\n",
      "  2.7927e-01 -8.8411e-02  8.9556e-03  2.7327e-01 -1.1099e-01  2.3637e-01\n",
      " -2.0016e-01  1.5873e-01 -3.1033e-01 -3.6783e-01  3.4196e-01  2.6334e-01\n",
      "  7.8430e-01  9.5231e-02  3.5778e-01  2.6411e-01 -4.2257e-01  1.8524e-01\n",
      "  3.5195e-01  3.3158e-02 -8.7731e-04 -5.0436e-02 -3.2886e-01  2.8192e-01\n",
      " -2.6564e-01  6.4443e-01  1.9853e-01 -2.5915e-01  1.4498e-01 -2.7210e-01\n",
      " -1.0360e-01 -1.5602e-01 -1.5242e-01 -8.0593e-02 -2.5024e-01  4.0126e-01\n",
      " -8.9539e-02  1.4189e-01  5.0669e-01  3.3599e-01 -2.2001e-02 -2.2898e-01\n",
      " -1.0182e-01  5.8937e-01 -3.8595e-01 -1.4345e-01  9.8736e-02 -6.7044e-01\n",
      "  2.7835e-01  2.5707e-01 -5.0605e-01  5.5569e-02 -1.0464e-01 -9.3889e-02\n",
      " -1.1917e-01  5.0083e-01  3.2615e-02 -2.8911e-02  5.8029e-01  6.8503e-01\n",
      "  1.0228e-02  2.7785e-01 -2.0873e-01 -9.3943e-02  1.1824e-02 -9.9075e-02\n",
      "  4.6631e-01 -1.7372e-01 -4.2725e-01 -3.7469e-02  2.6391e-01  1.6405e-01\n",
      " -6.0724e-01 -8.4765e-01  2.9234e-01  1.6936e-01 -3.3004e-01  2.0924e-01\n",
      " -2.1591e-01  2.8508e-01 -2.6982e-01 -2.5857e-01  1.9987e-01 -7.0493e-01\n",
      "  1.5161e-01  4.0602e-01  1.5787e-01  2.8346e-01 -3.0581e-02 -2.3879e-01\n",
      " -4.2869e-02  5.4554e-01 -6.2593e-02 -8.4068e-02 -2.2914e-01  1.4995e-01\n",
      " -3.4521e-01 -3.2032e-01 -3.7594e-01 -4.5375e-01  6.3275e-01 -4.7160e-01\n",
      " -2.7696e-01 -4.7523e-02  3.4061e-02  7.5782e-02  4.3946e-02  1.2893e-01\n",
      " -1.0306e-02 -3.2766e-02 -4.7612e-02  2.1110e-02  7.2606e-03  2.4494e-02\n",
      "  4.1124e-01 -1.1190e-01 -3.7260e-01 -1.0450e-01  2.3772e-01  1.4764e-01\n",
      " -5.7967e-02 -3.2230e-01  1.1821e-01  6.2660e-01 -3.5705e-01 -5.7315e-02\n",
      "  9.8854e-02 -3.2027e-01  1.8756e-01 -6.0273e-02  1.4244e-01  5.1511e-01\n",
      "  6.2247e-01 -2.1388e-01 -4.0873e-01 -2.7409e-02  2.6528e-01  2.0100e-01\n",
      " -3.3757e-02 -3.7453e-01 -1.1762e-01  7.4104e-02  1.1627e-01  3.8526e-01\n",
      " -2.2062e-01  3.6973e-01 -9.6166e-02 -2.2351e-01 -2.5046e-01  6.0957e-01\n",
      "  3.0601e-01  1.0105e-01 -8.9082e-02 -8.0634e-02  1.4848e-01 -1.6454e-02\n",
      "  5.2633e-01  2.9695e-02  3.6702e-01 -2.4726e-01 -2.5372e-01  6.3151e-01\n",
      "  2.3604e-01  7.2727e-01  2.2325e-01  3.2010e-01  4.3032e-01  1.7370e-01\n",
      " -7.5582e-01  1.0571e-02 -1.2223e-01 -1.6370e-01  4.4239e-01 -4.5328e-02\n",
      " -1.1029e-01 -8.0053e-02 -8.0851e-02 -2.1034e-01 -1.5776e-01  3.2142e-01\n",
      "  2.0392e-01  2.5432e-01  6.6951e-01 -4.1141e-01 -3.1512e-01 -1.0077e-01\n",
      " -7.6977e-02  9.2796e-03  8.9229e-02  1.8865e-02  2.5894e-02  3.5324e-01\n",
      "  1.8868e-01 -3.4531e-01 -2.3498e-01 -1.0321e-01 -4.4609e-01  1.4354e-01\n",
      " -1.9525e-01  1.2388e-01 -1.6920e-01  3.8052e-01  3.7588e-01 -2.7014e-01\n",
      "  4.8001e-02 -4.0118e-01  5.4665e-01 -1.0004e-01  4.5669e-01 -1.8925e-01\n",
      " -8.1724e-02  4.0910e-01  2.9538e-01 -1.0452e+00 -2.8339e-01  1.4045e-02\n",
      " -9.1247e-02  1.1702e-02  2.1762e-02 -4.6720e-01  4.7209e-01  8.8291e-02\n",
      " -1.3753e-03 -5.7195e-01  3.7423e-01 -4.0193e-01  5.3075e-01  6.0023e-01\n",
      "  6.4926e-01 -1.9299e-01  3.3988e-01 -2.3158e-01  6.4617e-02  3.5429e-01\n",
      " -6.9228e-01  3.0218e-01 -3.4043e-01 -3.8970e-01 -7.5647e-02 -2.1047e-01\n",
      "  1.4411e-01 -1.2869e-01 -3.4671e-01  2.3031e-02 -9.8913e-02  7.2117e-02\n",
      " -3.4731e-01 -1.1492e-01  1.8138e-01 -1.0492e-01  9.3857e-02 -5.9654e-01\n",
      "  1.4362e-01 -6.6271e-02  6.1860e-02 -2.0465e-01  9.0121e-02 -4.8971e-03\n",
      " -1.6644e+00 -3.2758e-01  7.7074e-01  3.6992e-01  3.1160e-01 -4.1195e-01\n",
      "  9.4183e-02 -1.8495e-01 -3.9561e-01  1.1006e-01 -6.6274e-01  1.3011e-01\n",
      " -5.2989e-01  3.8145e-02 -1.6298e-01 -2.9403e-01 -2.6884e-01  1.3890e-01\n",
      " -5.5917e-01  5.4519e-01 -1.0583e-01 -3.4939e-01  4.7005e-01 -2.1085e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 2.3382e-01  1.4786e-01  1.1077e-01  2.3996e-01 -2.9585e-01 -1.6727e-01\n",
      " -2.8175e-02  1.6083e-01  1.1507e-01 -1.1158e+00  1.5384e-01  1.6024e-01\n",
      "  7.2660e-02 -1.6972e-01  6.8051e-01 -1.2424e-01  6.3706e-02 -5.2588e-02\n",
      "  2.7927e-01 -8.8411e-02  8.9556e-03  2.7327e-01 -1.1099e-01  2.3637e-01\n",
      " -2.0016e-01  1.5873e-01 -3.1033e-01 -3.6783e-01  3.4196e-01  2.6334e-01\n",
      "  7.8430e-01  9.5231e-02  3.5778e-01  2.6411e-01 -4.2257e-01  1.8524e-01\n",
      "  3.5195e-01  3.3158e-02 -8.7731e-04 -5.0436e-02 -3.2886e-01  2.8192e-01\n",
      " -2.6564e-01  6.4443e-01  1.9853e-01 -2.5915e-01  1.4498e-01 -2.7210e-01\n",
      " -1.0360e-01 -1.5602e-01 -1.5242e-01 -8.0593e-02 -2.5024e-01  4.0126e-01\n",
      " -8.9539e-02  1.4189e-01  5.0669e-01  3.3599e-01 -2.2001e-02 -2.2898e-01\n",
      " -1.0182e-01  5.8937e-01 -3.8595e-01 -1.4345e-01  9.8736e-02 -6.7044e-01\n",
      "  2.7835e-01  2.5707e-01 -5.0605e-01  5.5569e-02 -1.0464e-01 -9.3889e-02\n",
      " -1.1917e-01  5.0083e-01  3.2615e-02 -2.8911e-02  5.8029e-01  6.8503e-01\n",
      "  1.0228e-02  2.7785e-01 -2.0873e-01 -9.3943e-02  1.1824e-02 -9.9075e-02\n",
      "  4.6631e-01 -1.7372e-01 -4.2725e-01 -3.7469e-02  2.6391e-01  1.6405e-01\n",
      " -6.0724e-01 -8.4765e-01  2.9234e-01  1.6936e-01 -3.3004e-01  2.0924e-01\n",
      " -2.1591e-01  2.8508e-01 -2.6982e-01 -2.5857e-01  1.9987e-01 -7.0493e-01\n",
      "  1.5161e-01  4.0602e-01  1.5787e-01  2.8346e-01 -3.0581e-02 -2.3879e-01\n",
      " -4.2869e-02  5.4554e-01 -6.2593e-02 -8.4068e-02 -2.2914e-01  1.4995e-01\n",
      " -3.4521e-01 -3.2032e-01 -3.7594e-01 -4.5375e-01  6.3275e-01 -4.7160e-01\n",
      " -2.7696e-01 -4.7523e-02  3.4061e-02  7.5782e-02  4.3946e-02  1.2893e-01\n",
      " -1.0306e-02 -3.2766e-02 -4.7612e-02  2.1110e-02  7.2606e-03  2.4494e-02\n",
      "  4.1124e-01 -1.1190e-01 -3.7260e-01 -1.0450e-01  2.3772e-01  1.4764e-01\n",
      " -5.7967e-02 -3.2230e-01  1.1821e-01  6.2660e-01 -3.5705e-01 -5.7315e-02\n",
      "  9.8854e-02 -3.2027e-01  1.8756e-01 -6.0273e-02  1.4244e-01  5.1511e-01\n",
      "  6.2247e-01 -2.1388e-01 -4.0873e-01 -2.7409e-02  2.6528e-01  2.0100e-01\n",
      " -3.3757e-02 -3.7453e-01 -1.1762e-01  7.4104e-02  1.1627e-01  3.8526e-01\n",
      " -2.2062e-01  3.6973e-01 -9.6166e-02 -2.2351e-01 -2.5046e-01  6.0957e-01\n",
      "  3.0601e-01  1.0105e-01 -8.9082e-02 -8.0634e-02  1.4848e-01 -1.6454e-02\n",
      "  5.2633e-01  2.9695e-02  3.6702e-01 -2.4726e-01 -2.5372e-01  6.3151e-01\n",
      "  2.3604e-01  7.2727e-01  2.2325e-01  3.2010e-01  4.3032e-01  1.7370e-01\n",
      " -7.5582e-01  1.0571e-02 -1.2223e-01 -1.6370e-01  4.4239e-01 -4.5328e-02\n",
      " -1.1029e-01 -8.0053e-02 -8.0851e-02 -2.1034e-01 -1.5776e-01  3.2142e-01\n",
      "  2.0392e-01  2.5432e-01  6.6951e-01 -4.1141e-01 -3.1512e-01 -1.0077e-01\n",
      " -7.6977e-02  9.2796e-03  8.9229e-02  1.8865e-02  2.5894e-02  3.5324e-01\n",
      "  1.8868e-01 -3.4531e-01 -2.3498e-01 -1.0321e-01 -4.4609e-01  1.4354e-01\n",
      " -1.9525e-01  1.2388e-01 -1.6920e-01  3.8052e-01  3.7588e-01 -2.7014e-01\n",
      "  4.8001e-02 -4.0118e-01  5.4665e-01 -1.0004e-01  4.5669e-01 -1.8925e-01\n",
      " -8.1724e-02  4.0910e-01  2.9538e-01 -1.0452e+00 -2.8339e-01  1.4045e-02\n",
      " -9.1247e-02  1.1702e-02  2.1762e-02 -4.6720e-01  4.7209e-01  8.8291e-02\n",
      " -1.3753e-03 -5.7195e-01  3.7423e-01 -4.0193e-01  5.3075e-01  6.0023e-01\n",
      "  6.4926e-01 -1.9299e-01  3.3988e-01 -2.3158e-01  6.4617e-02  3.5429e-01\n",
      " -6.9228e-01  3.0218e-01 -3.4043e-01 -3.8970e-01 -7.5647e-02 -2.1047e-01\n",
      "  1.4411e-01 -1.2869e-01 -3.4671e-01  2.3031e-02 -9.8913e-02  7.2117e-02\n",
      " -3.4731e-01 -1.1492e-01  1.8138e-01 -1.0492e-01  9.3857e-02 -5.9654e-01\n",
      "  1.4362e-01 -6.6271e-02  6.1860e-02 -2.0465e-01  9.0121e-02 -4.8971e-03\n",
      " -1.6644e+00 -3.2758e-01  7.7074e-01  3.6992e-01  3.1160e-01 -4.1195e-01\n",
      "  9.4183e-02 -1.8495e-01 -3.9561e-01  1.1006e-01 -6.6274e-01  1.3011e-01\n",
      " -5.2989e-01  3.8145e-02 -1.6298e-01 -2.9403e-01 -2.6884e-01  1.3890e-01\n",
      " -5.5917e-01  5.4519e-01 -1.0583e-01 -3.4939e-01  4.7005e-01 -2.1085e-01]\n",
      "First word in first document: super\n",
      "Embedding for this word:\n",
      " [-1.3896e-01  1.2130e+00  2.4560e-01 -2.7516e-01  5.0725e-01  3.7306e-01\n",
      "  6.0455e-01  4.5422e-01 -3.2791e-01 -5.6665e-01  3.8336e-01 -6.8814e-01\n",
      " -1.3871e-01 -4.2354e-01  2.4258e-02 -2.1776e-01  2.0820e-01 -9.1329e-02\n",
      " -1.0846e-01 -4.5497e-01  4.2489e-01  2.1485e-01  2.5729e-01  1.0212e-01\n",
      "  3.3257e-01 -2.5691e-01  1.6149e-01 -2.1938e-01  4.1485e-01 -9.8947e-01\n",
      " -3.0017e-01 -3.3480e-01 -1.6191e-01  3.1174e-02 -1.5225e+00  1.4193e-01\n",
      " -6.4055e-02  2.9956e-01  2.9768e-01  7.6586e-02 -1.9656e-01 -3.4299e-02\n",
      " -5.3394e-01 -4.2036e-02 -3.0388e-01 -1.1457e-01  9.0635e-02 -6.1258e-01\n",
      "  1.6595e-01  1.1532e-01 -4.9001e-01  6.1127e-02 -3.5021e-01  2.3784e-01\n",
      " -6.5555e-01  4.4000e-01  3.4830e-01  3.9215e-02  3.0473e-01 -2.6100e-01\n",
      "  5.3471e-02  1.2667e-04 -8.3094e-01 -9.6958e-02 -2.4066e-01  1.4338e-01\n",
      "  2.8247e-02 -8.9358e-04  5.7781e-01  2.2228e-01  9.8966e-02  3.6975e-01\n",
      " -1.3305e-01  8.4037e-03 -5.3555e-01  1.5261e-01  1.7584e-01 -2.6886e-01\n",
      " -2.8254e-02  2.1199e-01  3.5677e-01  1.4297e-01 -1.4316e-01 -6.9715e-02\n",
      "  6.7739e-02 -7.6595e-02 -5.2456e-01  2.1021e-02  5.5970e-01 -2.1186e-01\n",
      "  2.1273e-01  6.7540e-01  4.3156e-01 -4.9846e-01 -7.2166e-01  3.8983e-01\n",
      " -1.6478e-01  2.3858e-01 -8.7138e-02 -5.1316e-01  5.2550e-01  7.1671e-02\n",
      "  1.8321e-01  1.4223e-01  2.0777e-01 -2.8119e-02 -3.0350e-02  5.0865e-02\n",
      " -1.7405e-01  1.9915e-01  4.4877e-01  9.3559e-03  4.4912e-02  3.0753e-01\n",
      " -5.7825e-02 -1.7355e-01  5.4990e-01  4.4797e-01 -2.1394e-01 -2.3778e-01\n",
      "  3.2441e-02  4.7505e-01  6.8611e-01 -4.4709e-01 -2.8654e-01 -3.3109e-01\n",
      " -4.9554e-01  5.2162e-01  6.2363e-03 -3.6394e-01 -3.2736e-01  1.7030e-01\n",
      "  3.3882e-01  8.2687e-01  3.0033e-01  2.3082e-01 -4.4573e-03  2.8606e-01\n",
      " -2.0765e-01  3.1867e-01 -3.4659e-02  4.2973e-01 -2.1132e-01  1.9403e-01\n",
      " -4.4046e-02  1.9378e-01 -1.2732e-01 -7.3637e-03 -6.2693e-02  7.7524e-01\n",
      "  2.7827e-01 -1.3073e-01 -3.2932e-01  5.5361e-01  6.5890e-01 -2.3727e-01\n",
      " -4.3464e-01 -3.1004e-01 -4.1203e-01 -5.3018e-01  5.5248e-02 -2.1693e-01\n",
      " -1.9772e-01  6.5287e-01 -4.1722e-02 -2.2901e-01 -2.1404e-01  2.9469e-01\n",
      "  3.3406e-02 -5.1285e-01  3.3919e-01 -6.0759e-01 -5.8998e-01  2.1878e-02\n",
      " -5.0021e-01 -2.5041e-02  7.9773e-02  4.4393e-01  6.4513e-01  1.3715e-01\n",
      "  1.8965e-01 -1.9270e-01  1.5054e-01 -2.4111e-01  4.6385e-01 -4.1356e-01\n",
      " -2.1579e-01  4.9487e-02  5.9819e-01 -1.7362e-01  6.7087e-01  8.1567e-01\n",
      " -3.7523e-01  8.7300e-01 -5.8350e-02 -6.6227e-01 -1.3735e-01  2.4340e-01\n",
      "  6.8609e-01  3.1613e-01  1.7172e+00  3.9614e-02 -1.9355e-01 -5.0587e-01\n",
      "  2.7908e-01 -4.8653e-01 -4.0767e-01 -3.4555e-03  3.4252e-01  3.0706e-02\n",
      " -1.5453e-01  3.6956e-01 -6.1618e-03  2.7957e-01 -3.8621e-01 -5.6737e-01\n",
      " -5.6869e-01 -1.0343e+00  6.0047e-01 -4.2010e-01  2.4884e-01 -5.4358e-01\n",
      "  1.0383e-01  7.5234e-02 -6.6842e-02 -4.0863e-01  1.0254e-01 -2.3354e-02\n",
      " -5.1596e-01  1.6922e-01  4.8039e-02 -2.1806e-01 -1.1743e-01 -7.3767e-01\n",
      "  1.0277e+00 -6.6193e-02  6.0333e-01 -2.6168e-01 -3.3119e-01  5.3970e-02\n",
      "  2.4385e-01 -2.6787e-01  1.9097e-01  5.4283e-01 -1.0041e+00 -1.2646e-01\n",
      " -6.1514e-01 -3.3854e-02 -4.1974e-01  1.1377e-01 -7.7943e-01 -6.6869e-03\n",
      " -5.4013e-01  6.9636e-01 -8.8223e-01  3.6032e-02 -9.6489e-03 -2.3583e-01\n",
      "  2.5422e-01  7.9554e-02 -2.0473e-01 -2.0923e-01 -2.4261e-01  8.8337e-01\n",
      "  4.4107e-01  5.9088e-01 -1.4094e-01 -2.7148e-01  2.8171e-01 -2.6856e-01\n",
      "  1.1057e-01 -1.1918e-01  2.7769e-01  2.6091e-01  1.8910e-01  3.5853e-02\n",
      " -9.3099e-01  8.8160e-01 -4.8039e-01 -2.3596e-01 -5.4017e-01  4.5207e-01\n",
      "  2.8975e-01  6.8806e-01 -6.7061e-01 -2.6671e-01 -4.5635e-01 -1.2258e-01\n",
      " -5.6972e-02 -9.9968e-02 -5.4236e-01  8.6518e-02 -7.2980e-01 -1.1631e-01\n",
      "  3.3359e-01  6.3978e-01 -1.8178e-01 -4.7838e-01 -5.5030e-01  3.9678e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-1.3896e-01  1.2130e+00  2.4560e-01 -2.7516e-01  5.0725e-01  3.7306e-01\n",
      "  6.0455e-01  4.5422e-01 -3.2791e-01 -5.6665e-01  3.8336e-01 -6.8814e-01\n",
      " -1.3871e-01 -4.2354e-01  2.4258e-02 -2.1776e-01  2.0820e-01 -9.1329e-02\n",
      " -1.0846e-01 -4.5497e-01  4.2489e-01  2.1485e-01  2.5729e-01  1.0212e-01\n",
      "  3.3257e-01 -2.5691e-01  1.6149e-01 -2.1938e-01  4.1485e-01 -9.8947e-01\n",
      " -3.0017e-01 -3.3480e-01 -1.6191e-01  3.1174e-02 -1.5225e+00  1.4193e-01\n",
      " -6.4055e-02  2.9956e-01  2.9768e-01  7.6586e-02 -1.9656e-01 -3.4299e-02\n",
      " -5.3394e-01 -4.2036e-02 -3.0388e-01 -1.1457e-01  9.0635e-02 -6.1258e-01\n",
      "  1.6595e-01  1.1532e-01 -4.9001e-01  6.1127e-02 -3.5021e-01  2.3784e-01\n",
      " -6.5555e-01  4.4000e-01  3.4830e-01  3.9215e-02  3.0473e-01 -2.6100e-01\n",
      "  5.3471e-02  1.2667e-04 -8.3094e-01 -9.6958e-02 -2.4066e-01  1.4338e-01\n",
      "  2.8247e-02 -8.9358e-04  5.7781e-01  2.2228e-01  9.8966e-02  3.6975e-01\n",
      " -1.3305e-01  8.4037e-03 -5.3555e-01  1.5261e-01  1.7584e-01 -2.6886e-01\n",
      " -2.8254e-02  2.1199e-01  3.5677e-01  1.4297e-01 -1.4316e-01 -6.9715e-02\n",
      "  6.7739e-02 -7.6595e-02 -5.2456e-01  2.1021e-02  5.5970e-01 -2.1186e-01\n",
      "  2.1273e-01  6.7540e-01  4.3156e-01 -4.9846e-01 -7.2166e-01  3.8983e-01\n",
      " -1.6478e-01  2.3858e-01 -8.7138e-02 -5.1316e-01  5.2550e-01  7.1671e-02\n",
      "  1.8321e-01  1.4223e-01  2.0777e-01 -2.8119e-02 -3.0350e-02  5.0865e-02\n",
      " -1.7405e-01  1.9915e-01  4.4877e-01  9.3559e-03  4.4912e-02  3.0753e-01\n",
      " -5.7825e-02 -1.7355e-01  5.4990e-01  4.4797e-01 -2.1394e-01 -2.3778e-01\n",
      "  3.2441e-02  4.7505e-01  6.8611e-01 -4.4709e-01 -2.8654e-01 -3.3109e-01\n",
      " -4.9554e-01  5.2162e-01  6.2363e-03 -3.6394e-01 -3.2736e-01  1.7030e-01\n",
      "  3.3882e-01  8.2687e-01  3.0033e-01  2.3082e-01 -4.4573e-03  2.8606e-01\n",
      " -2.0765e-01  3.1867e-01 -3.4659e-02  4.2973e-01 -2.1132e-01  1.9403e-01\n",
      " -4.4046e-02  1.9378e-01 -1.2732e-01 -7.3637e-03 -6.2693e-02  7.7524e-01\n",
      "  2.7827e-01 -1.3073e-01 -3.2932e-01  5.5361e-01  6.5890e-01 -2.3727e-01\n",
      " -4.3464e-01 -3.1004e-01 -4.1203e-01 -5.3018e-01  5.5248e-02 -2.1693e-01\n",
      " -1.9772e-01  6.5287e-01 -4.1722e-02 -2.2901e-01 -2.1404e-01  2.9469e-01\n",
      "  3.3406e-02 -5.1285e-01  3.3919e-01 -6.0759e-01 -5.8998e-01  2.1878e-02\n",
      " -5.0021e-01 -2.5041e-02  7.9773e-02  4.4393e-01  6.4513e-01  1.3715e-01\n",
      "  1.8965e-01 -1.9270e-01  1.5054e-01 -2.4111e-01  4.6385e-01 -4.1356e-01\n",
      " -2.1579e-01  4.9487e-02  5.9819e-01 -1.7362e-01  6.7087e-01  8.1567e-01\n",
      " -3.7523e-01  8.7300e-01 -5.8350e-02 -6.6227e-01 -1.3735e-01  2.4340e-01\n",
      "  6.8609e-01  3.1613e-01  1.7172e+00  3.9614e-02 -1.9355e-01 -5.0587e-01\n",
      "  2.7908e-01 -4.8653e-01 -4.0767e-01 -3.4555e-03  3.4252e-01  3.0706e-02\n",
      " -1.5453e-01  3.6956e-01 -6.1618e-03  2.7957e-01 -3.8621e-01 -5.6737e-01\n",
      " -5.6869e-01 -1.0343e+00  6.0047e-01 -4.2010e-01  2.4884e-01 -5.4358e-01\n",
      "  1.0383e-01  7.5234e-02 -6.6842e-02 -4.0863e-01  1.0254e-01 -2.3354e-02\n",
      " -5.1596e-01  1.6922e-01  4.8039e-02 -2.1806e-01 -1.1743e-01 -7.3767e-01\n",
      "  1.0277e+00 -6.6193e-02  6.0333e-01 -2.6168e-01 -3.3119e-01  5.3970e-02\n",
      "  2.4385e-01 -2.6787e-01  1.9097e-01  5.4283e-01 -1.0041e+00 -1.2646e-01\n",
      " -6.1514e-01 -3.3854e-02 -4.1974e-01  1.1377e-01 -7.7943e-01 -6.6869e-03\n",
      " -5.4013e-01  6.9636e-01 -8.8223e-01  3.6032e-02 -9.6489e-03 -2.3583e-01\n",
      "  2.5422e-01  7.9554e-02 -2.0473e-01 -2.0923e-01 -2.4261e-01  8.8337e-01\n",
      "  4.4107e-01  5.9088e-01 -1.4094e-01 -2.7148e-01  2.8171e-01 -2.6856e-01\n",
      "  1.1057e-01 -1.1918e-01  2.7769e-01  2.6091e-01  1.8910e-01  3.5853e-02\n",
      " -9.3099e-01  8.8160e-01 -4.8039e-01 -2.3596e-01 -5.4017e-01  4.5207e-01\n",
      "  2.8975e-01  6.8806e-01 -6.7061e-01 -2.6671e-01 -4.5635e-01 -1.2258e-01\n",
      " -5.6972e-02 -9.9968e-02 -5.4236e-01  8.6518e-02 -7.2980e-01 -1.1631e-01\n",
      "  3.3359e-01  6.3978e-01 -1.8178e-01 -4.7838e-01 -5.5030e-01  3.9678e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary \n",
    "\n",
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.300d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "# ------------------------------------------------------------- \n",
    "\n",
    "# Utility function for loading embeddings follows methods described in\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
    "# for the requested pre-trained word embeddings\n",
    "# \n",
    "# Note the use of defaultdict data structure from the Python Standard Library\n",
    "# collections_defaultdict.py lets the caller specify a default value up front\n",
    "# The default value will be retuned if the key is not a known dictionary key\n",
    "# That is, unknown words are represented by a vector of zeros\n",
    "# For word embeddings, this default value is a vector of zeros\n",
    "# Documentation for the Python standard library:\n",
    "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
    "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n",
    "\n",
    "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
    "\n",
    "# Additional background code from\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "\n",
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n",
    "\n",
    "# ------------------------------------------------------------- \n",
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding\n",
    "\n",
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# code for working with movie reviews data \n",
    "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
    "#    Upper Saddle River, N.J.: Pearson Education.\n",
    "#    ISBN-13: 978-0-13-388644-3\n",
    "# This original study used a simple bag-of-words approach\n",
    "# to sentiment analysis, along with pre-defined lists of\n",
    "# negative and positive words.        \n",
    "# Code available at:  https://github.com/mtpa/wnds       \n",
    "# ------------------------------------------------------------\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove\n",
    "\n",
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    \n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "\n",
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])\n",
    "\n",
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])\n",
    "\n",
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])        \n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "\n",
    "# --------------------------------------------------------------------------      \n",
    "# We use a very simple Recurrent Neural Network for this assignment\n",
    "# Géron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
    "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
    "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
    "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
    "#    Source code available at https://github.com/ageron/handson-ml\n",
    "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
    "#    See section on Training an sequence Classifier, # In [34]:\n",
    "#    which uses the MNIST case data...  we revise to accommodate\n",
    "#    the movie review data in this assignment    \n",
    "# --------------------------------------------------------------------------  \n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn4 (100k vocab size, glove.6B.300d.txt embedding)\n",
    "## train: 0.990, test: 0.640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B/glove.6B.300d.txt\n",
      "Embedding loaded from disks.\n",
      "Embedding is of shape: (400001, 300)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [0.04656, 0.21318, -0.0074364, -0.45854, -0.035639, 0.23643, -0.28836, 0.21521, -0.13486, -1.6413, -0.26091, 0.032434, 0.056621, -0.043296, -0.021672, 0.22476, -0.075129, -0.067018, -0.14247, 0.038825, -0.18951, 0.29977, 0.39305, 0.17887, -0.17343, -0.21178, 0.23617, -0.063681, -0.42318, -0.11661, 0.093754, 0.17296, -0.33073, 0.49112, -0.68995, -0.092462, 0.24742, -0.17991, 0.097908, 0.083118, 0.15299, -0.27276, -0.038934, 0.54453, 0.53737, 0.29105, -0.0073514, 0.04788, -0.4076, -0.026759, 0.17919, 0.010977, -0.10963, -0.26395, 0.07399, 0.26236, -0.1508, 0.34623, 0.25758, 0.11971, -0.037135, -0.071593, 0.43898, -0.040764, 0.016425, -0.4464, 0.17197, 0.046246, 0.058639, 0.041499, 0.53948, 0.52495, 0.11361, -0.048315, -0.36385, 0.18704, 0.092761, -0.11129, -0.42085, 0.13992, -0.39338, -0.067945, 0.12188, 0.16707, 0.075169, -0.015529, -0.19499, 0.19638, 0.053194, 0.2517, -0.34845, -0.10638, -0.34692, -0.19024, -0.2004, 0.12154, -0.29208, 0.023353, -0.11618, -0.35768, 0.062304, 0.35884, 0.02906, 0.0073005, 0.0049482, -0.15048, -0.12313, 0.19337, 0.12173, 0.44503, 0.25147, 0.10781, -0.17716, 0.038691, 0.08153, 0.14667, 0.063666, 0.061332, -0.075569, -0.37724, 0.01585, -0.30342, 0.28374, -0.042013, -0.040715, -0.15269, 0.07498, 0.15577, 0.10433, 0.31393, 0.19309, 0.19429, 0.15185, -0.10192, -0.018785, 0.20791, 0.13366, 0.19038, -0.25558, 0.304, -0.01896, 0.20147, -0.4211, -0.0075156, -0.27977, -0.19314, 0.046204, 0.19971, -0.30207, 0.25735, 0.68107, -0.19409, 0.23984, 0.22493, 0.65224, -0.13561, -0.17383, -0.048209, -0.1186, 0.0021588, -0.019525, 0.11948, 0.19346, -0.4082, -0.082966, 0.16626, -0.10601, 0.35861, 0.16922, 0.07259, -0.24803, -0.10024, -0.52491, -0.17745, -0.36647, 0.2618, -0.012077, 0.08319, -0.21528, 0.41045, 0.29136, 0.30869, 0.078864, 0.32207, -0.041023, -0.1097, -0.092041, -0.12339, -0.16416, 0.35382, -0.082774, 0.33171, -0.24738, -0.048928, 0.15746, 0.18988, -0.026642, 0.063315, -0.010673, 0.34089, 1.4106, 0.13417, 0.28191, -0.2594, 0.055267, -0.052425, -0.25789, 0.019127, -0.022084, 0.32113, 0.068818, 0.51207, 0.16478, -0.20194, 0.29232, 0.098575, 0.013145, -0.10652, 0.1351, -0.045332, 0.20697, -0.48425, -0.44706, 0.0033305, 0.0029264, -0.10975, -0.23325, 0.22442, -0.10503, 0.12339, 0.10978, 0.048994, -0.25157, 0.40319, 0.35318, 0.18651, -0.023622, -0.12734, 0.11475, 0.27359, -0.21866, 0.015794, 0.81754, -0.023792, -0.85469, -0.16203, 0.18076, 0.028014, -0.1434, 0.0013139, -0.091735, -0.089704, 0.11105, -0.16703, 0.068377, -0.087388, -0.039789, 0.014184, 0.21187, 0.28579, -0.28797, -0.058996, -0.032436, -0.0047009, -0.17052, -0.034741, -0.11489, 0.075093, 0.099526, 0.048183, -0.073775, -0.41817, 0.0041268, 0.44414, -0.16062, 0.14294, -2.2628, -0.027347, 0.81311, 0.77417, -0.25639, -0.11576, -0.11982, -0.21363, 0.028429, 0.27261, 0.031026, 0.096782, 0.0067769, 0.14082, -0.013064, -0.29686, -0.079913, 0.195, 0.031549, 0.28506, -0.087461, 0.0090611, -0.20989, 0.053913]\n",
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "quick:  [ 0.37594    0.063183   0.51835   -0.48652    0.34026   -0.17801\n",
      "  0.32615   -0.32989    0.25508   -1.026     -0.13841    0.27258\n",
      " -0.010244   0.35186    0.28341    0.3189    -0.18892   -0.292\n",
      " -0.071297   0.25631   -0.34286    0.16179    0.065725  -0.038052\n",
      "  0.1457     0.20289    0.14274   -0.15024    0.35412   -0.13715\n",
      " -0.2677    -0.011243  -0.1541     0.1765    -1.5424     0.37699\n",
      " -0.28239    0.19172   -0.46349   -0.26345   -0.39337    0.41276\n",
      "  0.42873   -0.1317     0.31096   -0.28715    0.47212   -0.41866\n",
      " -0.11871   -0.026621  -0.21089   -0.074757   0.26429   -0.032246\n",
      " -0.084233   0.48653   -0.38692   -0.3756    -0.27031    0.15208\n",
      "  0.2263    -0.28777    0.12475   -0.17029    0.019322   0.34175\n",
      "  0.69492   -0.42039    0.3161    -0.23871    0.30485   -0.078005\n",
      "  0.25937   -0.036324   0.27365   -0.1698    -0.10004    0.13614\n",
      "  0.0094279 -0.51261    0.3464     0.032112   0.25292    0.1947\n",
      "  0.16906   -0.044683  -0.039252   0.31651   -0.27185    0.10862\n",
      "  0.070371   0.31916   -0.55993   -0.5553     0.5493    -0.17244\n",
      " -0.70848    0.039063   0.33553   -0.11393   -0.28882   -0.53623\n",
      "  0.0021584  0.24971    0.31383   -0.2516    -0.28619    0.20113\n",
      " -0.29545   -0.3285     0.33289    0.19422    0.047601  -0.13157\n",
      "  0.4269     0.085041  -0.30294   -0.38344   -0.035083  -0.0463\n",
      "  0.035454  -0.052446   0.51216   -0.37809   -0.24834    0.28464\n",
      "  0.019408   0.61137    0.14859    0.30104    0.23773    0.37627\n",
      " -0.64467    0.19701   -0.19264   -0.013601   0.073281  -0.43031\n",
      "  0.38081   -0.42172   -0.16131    0.12108   -0.12078   -0.20818\n",
      " -0.4697     0.1279    -0.63088    0.16412    0.20474    0.16701\n",
      " -0.79632   -0.075741  -0.25251   -0.025189   0.081245  -0.081758\n",
      " -0.12925   -0.33034    0.039839  -0.30436    0.023003  -0.35589\n",
      "  0.40923   -0.10969   -0.084268   0.56261    0.37604    0.10676\n",
      " -0.1678     0.11219   -0.13141   -0.025916  -0.56102   -0.074779\n",
      " -0.14769    0.13028   -0.38351    0.055938   0.1996     0.0052525\n",
      "  0.11655   -0.58141    0.45407   -0.11067   -0.10262    0.31478\n",
      " -0.049739  -0.34926   -0.016468  -0.12476   -0.071381   0.34803\n",
      " -0.12247   -0.38406    0.095986   0.12451   -0.033609  -0.62353\n",
      "  0.25048   -0.1427     0.60613   -0.080829   0.25008    0.059055\n",
      " -0.17486    0.14913   -0.41488    0.27573   -0.11921   -0.02267\n",
      " -0.34188   -0.49563   -0.22119    0.49553   -0.035482  -0.11908\n",
      "  0.0096008 -0.44059   -0.35947   -0.19156    0.28505    0.35236\n",
      " -0.3384    -0.28643   -0.22068   -0.29761    0.10412   -0.067384\n",
      "  0.043089  -0.05794   -0.31212    0.24026    0.072559  -0.024896\n",
      " -0.19299    0.020044  -0.10826    0.2022     0.097076   0.43886\n",
      "  0.35085    0.38611   -0.1838    -0.047166  -0.5351    -0.17215\n",
      "  0.17407    0.17959   -0.35965   -0.23817    0.16348   -0.79488\n",
      "  0.18858    0.027404   0.23823    0.047581   0.2485     0.18332\n",
      " -0.22626    0.54554   -0.31324   -0.22699   -0.31341    0.68296\n",
      "  0.13422   -0.27644   -0.38901   -0.29207   -0.10058    0.12057\n",
      " -0.36691   -0.69507   -0.22242   -0.023121   0.72283    0.0051197\n",
      " -1.7769     0.40651    0.025966  -0.18157    0.23957    0.37943\n",
      "  0.67713    0.49789    0.3634    -0.60131    0.53868   -0.18682\n",
      " -0.14783    0.40581    0.1379     0.054337  -0.12388    0.064828\n",
      "  0.27453    0.5165    -0.1955    -0.55939   -0.2744    -0.12146  ]\n",
      "brown:  [ 0.2793     0.18372   -0.11257    0.21734   -0.21657   -0.50335\n",
      " -0.27194    0.32181    0.031892  -0.37998    0.15544   -0.32953\n",
      " -0.19827    0.20403    0.26768    0.292     -0.34187   -0.10766\n",
      " -0.43697   -0.14488    0.14634    0.21591    0.12576    0.14895\n",
      " -0.21763    0.030797   0.10949   -0.41689   -0.30296   -0.14592\n",
      " -0.56228    0.33282   -0.20436   -0.24403   -1.4732     0.68345\n",
      "  0.45336    0.43671   -0.15641    0.15075   -0.24265   -0.040059\n",
      "  0.22323    0.19523    0.37445   -0.18509   -0.10302   -0.055363\n",
      " -0.17274   -0.45401   -0.14729   -0.24133   -0.043826  -0.23243\n",
      "  0.42367    0.15906   -0.14039   -0.36185   -0.26695   -0.42724\n",
      " -0.08843   -0.099597   0.24257   -0.05424    0.10746   -1.1304\n",
      "  0.024651  -0.10212    0.046319  -0.68792    0.4214    -0.25844\n",
      "  0.17052    0.097878   0.026835   0.32044    0.0062988  0.24575\n",
      "  0.20126   -0.16771    0.19825    0.28939   -0.064994  -0.38766\n",
      "  0.52509    0.38195    0.32421    0.20683   -0.48472   -0.080334\n",
      " -0.15345    0.35459   -0.43765    0.071575  -0.39516   -0.22906\n",
      "  0.25686    0.26659    0.37626   -0.18556    0.16445   -0.33614\n",
      " -0.56262    0.067852  -0.61642    0.19546    0.45027    0.20238\n",
      "  0.33957    0.41372    0.11855    0.087619   0.18754    0.17901\n",
      "  0.022569  -0.10854   -0.47226    0.41039    0.32588   -0.58468\n",
      " -0.0057296 -0.29201   -0.12777   -0.15729   -0.40103   -0.039414\n",
      " -0.1192     0.40093    0.032862   0.39862   -0.63525    0.11594\n",
      " -0.39954    0.36919   -0.50021   -0.51169   -0.13955    0.18055\n",
      " -0.079918  -0.19474    0.53131    0.093723   0.2773    -0.40505\n",
      " -0.20568    0.11139    0.032661  -0.04852    0.44576    0.23667\n",
      "  0.54981    0.23585   -0.51539   -0.46424    0.021099  -0.3919\n",
      "  0.58338   -0.89908    0.094066   0.30159   -0.063199  -0.31635\n",
      "  0.50333   -0.068517  -0.38681    0.33      -0.49463    0.75491\n",
      " -0.088266  -0.19413    0.4238    -0.031727  -0.4464    -0.21028\n",
      " -0.11151   -0.07088   -0.027832  -0.63304    0.27336   -0.47925\n",
      " -0.03239    0.46069    0.16968   -0.38262   -0.31413   -0.29068\n",
      " -0.031801  -0.48974   -0.50999    0.1466     0.0027995  0.56333\n",
      " -0.044347  -0.085679   0.20559   -0.051593   0.75228   -0.013291\n",
      " -0.084694  -0.4305     1.1734    -0.083233   0.1561    -0.15758\n",
      "  0.19066   -0.2966     0.63704    0.45616   -0.34797   -0.12732\n",
      "  0.4901    -0.51217   -0.063474  -0.061496   0.28825    0.17711\n",
      "  0.46301   -0.12697   -0.044627  -1.0064     0.76394    0.20494\n",
      "  0.028766   0.27597    0.021726  -0.12054    0.23284    0.18999\n",
      "  0.30048   -0.056139   0.09546   -0.036514   0.0084885  0.016599\n",
      " -0.31428   -0.2707     0.099281   0.4445    -0.36      -0.55556\n",
      " -0.18551   -0.30644    0.056475  -0.19197   -0.48886    0.33044\n",
      "  0.19535   -0.53828    0.12385   -0.29372   -0.1036     0.0051129\n",
      "  0.11483   -0.10591    0.73337    0.26978   -0.06925    0.11565\n",
      "  0.27711    0.15109   -0.069137  -0.14481    0.32319    0.039345\n",
      " -0.44964    0.27103    0.045326  -0.064534  -0.37144    0.47615\n",
      " -0.61105   -0.11922   -0.068806   0.15401   -0.40812    0.32575\n",
      " -1.2888     0.0203    -0.12893   -0.22211   -0.16402    0.29018\n",
      "  0.36295   -0.081025  -0.50492    0.5046    -0.37485    0.52111\n",
      "  0.1757     0.069686   0.48937   -0.17747   -0.20577    0.70419\n",
      "  0.068633   0.47878   -0.21754   -0.016868  -0.91378    0.45643  ]\n",
      "fox:  [-1.1570e-01 -2.5048e-02 -1.1013e-01 -4.8060e-02 -8.5504e-02  3.7308e-01\n",
      " -9.4790e-01  5.9662e-01 -2.0006e-02 -3.0469e-01  2.4997e-01 -2.0005e-01\n",
      "  1.9284e-01  4.7197e-01  2.7099e-01  3.4190e-01 -2.9186e-01 -3.9718e-01\n",
      "  5.5653e-01  3.2774e-01  9.4860e-02 -4.6588e-01  6.2119e-01  2.0709e-01\n",
      "  8.2332e-02 -1.0175e-01  3.2067e-01 -3.1875e-01  1.7941e-01 -2.4127e-01\n",
      " -1.7961e-02 -8.7154e-02  3.3423e-01 -4.5026e-02 -1.2193e+00  1.9321e-01\n",
      "  2.1427e-01  2.3893e-01 -1.8084e-01 -1.5920e-01 -1.0387e-01  2.2060e-01\n",
      "  2.5423e-01  2.0815e-01 -3.3072e-01 -2.0672e-01 -3.1424e-01  2.6498e-02\n",
      "  2.1955e-01 -3.1157e-01 -7.8914e-02 -1.5089e-02 -4.0848e-02 -2.3807e-01\n",
      "  3.1983e-01 -7.4173e-02  2.2983e-01  5.5735e-02  5.6743e-03 -7.7368e-01\n",
      "  2.7412e-01 -1.8999e-01  5.3825e-01 -1.9784e-01 -1.0898e-01 -3.4022e-01\n",
      "  3.1582e-01  5.0317e-01  3.3955e-01 -2.7143e-01 -3.3026e-02  3.4279e-01\n",
      " -4.9815e-01  2.5056e-01 -1.2791e-01 -1.5912e-01 -8.7541e-03  1.0450e-01\n",
      "  2.1969e-01 -5.9077e-02 -3.7600e-01  6.0835e-02  1.8427e-01 -8.3592e-02\n",
      "  1.1632e-01  7.5145e-01  6.5186e-02 -3.3258e-01  1.4249e-01 -6.5348e-01\n",
      " -2.6661e-03  1.0233e-01 -3.8731e-01  6.0529e-01 -2.7326e-01 -3.5777e-01\n",
      " -5.3533e-01  4.9589e-02  2.0224e-01 -3.2805e-01  3.9305e-01 -2.7540e-01\n",
      " -1.3042e-01  9.5833e-01  3.8853e-01  5.9884e-01 -3.0503e-01 -1.0708e-01\n",
      "  4.5670e-01  7.2182e-01 -4.4221e-02 -6.8201e-02 -2.9903e-01  2.6483e-01\n",
      "  6.6810e-03  1.4919e-01 -5.3155e-01 -6.1697e-01 -5.4271e-01 -4.1103e-01\n",
      " -1.7534e-01  1.3229e-01  2.6958e-02 -1.5111e-01 -1.6844e-01 -3.0684e-01\n",
      " -5.1189e-02  9.2940e-01 -6.0882e-01 -1.1373e-01 -4.2237e-01 -4.0481e-01\n",
      " -3.4182e-01  3.8472e-01 -1.1566e-01  2.2679e-02  4.4793e-01  1.8849e-01\n",
      " -1.9598e-01 -6.8914e-02 -4.4520e-01  4.1643e-01  5.8319e-02 -3.9575e-01\n",
      " -4.0471e-01  4.2855e-01  1.6339e-01 -1.6965e-01 -5.7793e-02 -1.0358e-01\n",
      "  1.0223e+00  9.0564e-01 -5.5851e-01 -2.4856e-01  7.5582e-02 -2.7374e-01\n",
      "  3.8838e-01  5.1561e-01  6.0029e-02 -5.1990e-02  1.0113e+00  8.3607e-03\n",
      "  4.9764e-01  3.2839e-02  2.5892e-02  4.0404e-01 -3.7393e-01  1.3384e-01\n",
      " -2.9458e-01  3.4668e-01  5.2114e-02  1.0785e-01 -8.4700e-01  4.0936e-01\n",
      "  3.8801e-01  4.5079e-01 -4.3880e-01  1.9777e-01  1.9102e-01 -7.6580e-02\n",
      "  2.5788e-01  3.5642e-01  2.3305e-01 -1.6206e-01 -5.0847e-02  1.3708e-01\n",
      "  5.1338e-02 -3.8552e-01 -2.5497e-01  5.3901e-02 -6.2922e-01  3.2780e-01\n",
      " -2.0383e-01 -1.7265e-01 -9.9668e-02  2.4485e-01  3.2700e-02  3.2325e-01\n",
      "  2.3912e-01  7.7014e-03  1.2890e+00  6.8030e-02  2.0671e-02  2.6268e-01\n",
      "  1.9434e-01  5.6238e-01  7.1863e-02 -6.5502e-02 -2.2858e-01  9.2883e-02\n",
      "  3.9936e-01 -2.8038e-01  3.5633e-01  1.5203e-01 -2.7883e-01  3.8225e-02\n",
      " -3.9636e-01  3.2488e-01  2.4332e-01  2.2621e-01  2.3518e-01 -6.4747e-02\n",
      "  6.2858e-01 -1.1848e-01  1.2710e-01 -1.2697e-01  1.0015e+00 -2.5693e-01\n",
      "  2.0450e-01 -1.8789e-01 -1.2913e-01 -9.6456e-02  1.2301e-01 -1.3377e-01\n",
      " -1.2808e-01  4.1857e-02  8.6418e-01  3.6137e-01  9.0372e-02 -1.0687e-02\n",
      "  4.4736e-01  7.8975e-02 -3.2992e-01 -1.4470e-01 -6.1296e-01 -4.1543e-01\n",
      " -2.6104e-01 -8.4169e-01  4.7352e-01 -4.7640e-01 -3.4393e-01  8.2156e-02\n",
      " -2.4477e-01  7.9361e-01  2.6183e-01  4.6648e-01  1.9135e-01 -5.5033e-02\n",
      "  2.3492e-01  1.1178e-01 -3.7291e-01  2.6764e-01  1.1074e-01  6.8041e-01\n",
      " -1.0361e-03 -4.2537e-02  9.4006e-01 -2.8939e-01 -2.0841e-01  3.3001e-01\n",
      "  7.3318e-02 -5.1902e-01 -3.6412e-01 -4.1372e-01  2.0266e-01  4.3162e-01\n",
      " -1.0174e+00  2.5093e-01  3.8136e-01 -1.4332e-01 -2.0525e-01 -1.8724e-01\n",
      "  5.9925e-01  3.5604e-01 -9.1457e-01 -2.2401e-01 -4.2382e-02 -1.3299e-01\n",
      "  6.1671e-01  4.4886e-01 -1.1361e-01  2.0483e-01  2.5485e-01  2.7581e-01\n",
      " -8.7158e-01 -1.3156e-01  2.6117e-01  1.5815e-01 -3.2199e-01  7.1042e-01]\n",
      "jumps:  [-0.16814   -0.10948    0.2896    -0.21108   -0.29061    0.31201\n",
      "  0.04039   -0.10149   -0.18526   -0.55483   -0.36055   -0.093569\n",
      "  0.77334    0.027921   0.13389   -0.1014    -0.06482    0.24753\n",
      " -0.068026   0.26147   -0.14252    0.18657    0.030249  -0.07934\n",
      "  0.87696    0.61781    0.36835   -0.07306   -0.19303    0.3721\n",
      " -0.77087   -0.0062782 -0.19233   -0.43884   -0.79847    0.066636\n",
      " -0.21387   -0.65263   -0.073964   0.64115   -0.52014   -0.05981\n",
      " -0.1692    -0.413      0.060197   0.16327    0.43353    0.070021\n",
      "  0.063543   0.28527   -0.43474    0.15441    0.098037   0.098685\n",
      " -0.3965     0.38171   -0.084065  -0.4813    -0.59213    0.40444\n",
      " -0.2026     0.45569    0.039036  -0.41786   -0.20322   -0.11932\n",
      "  0.23747    0.26336    0.17139    0.12521   -0.55276    0.45515\n",
      " -0.63826    0.14054    0.35333   -0.28417    0.3889    -0.13004\n",
      " -0.027142  -0.23109    0.034327  -0.10685    0.85855    0.15145\n",
      "  0.16814    0.2281     0.22235   -0.1825    -0.019222   0.026105\n",
      "  0.47734    0.42115   -0.087767  -0.17439    0.22166   -0.36831\n",
      " -1.069      0.40489   -0.31038   -0.21588   -0.7282     0.29296\n",
      " -0.42949    0.23485    0.020585  -0.47795   -0.20216   -0.22146\n",
      " -0.45778    0.032547  -0.13727   -0.48945   -0.58148    0.051203\n",
      " -0.065926   0.46718   -0.080438  -0.25042   -0.4015     0.40254\n",
      "  0.12       0.5246     0.21582    0.1333     0.27662    0.2163\n",
      " -0.28177    0.67185   -0.025996  -0.40781   -0.23629    0.97455\n",
      " -0.51452    0.22697   -0.34857   -0.55928    0.019104  -0.016163\n",
      " -0.22626    0.094269   0.10808   -0.018804  -0.080299   0.0058964\n",
      " -0.61886    0.44842    0.2494    -0.25172    0.6705     0.23657\n",
      " -0.17631    0.28634   -0.39527   -0.22096    0.23069    0.0644\n",
      "  0.13151   -0.030479   0.38503   -0.084794   0.66178   -0.34578\n",
      "  0.3868     0.31506    0.20155   -0.026189   0.051188   0.16359\n",
      " -0.37096    0.21546   -0.19758   -0.083407   0.48437   -0.5487\n",
      "  0.35188   -0.43539    0.3976    -0.026514  -0.14485   -0.11205\n",
      "  0.59835    0.38055    0.1582    -0.24293    0.39837    0.44276\n",
      "  0.36113   -0.2358    -0.32568    0.54672   -0.36544   -0.33871\n",
      "  0.2154    -0.36877   -0.18857    0.42323   -0.56938   -0.20276\n",
      "  0.35452   -0.0167     1.157      0.22296   -0.35115    0.3662\n",
      " -0.20903    0.73497    0.018414   0.27861    0.044337   0.14504\n",
      " -0.23001   -0.54025   -0.26259   -0.94587    0.22239    0.44651\n",
      " -0.46075   -0.42224    0.0022315 -0.25522   -0.13135    0.65923\n",
      "  0.21267   -0.26498   -0.16777   -0.047013  -0.18551    0.12607\n",
      "  0.43205    0.153     -0.33748    0.010732  -0.18332   -0.28507\n",
      "  0.23396    0.3868    -0.34724    0.41692    0.56315    0.32366\n",
      "  0.55303    0.02256   -0.48281    0.54561   -0.51086   -0.55446\n",
      " -0.018992   0.62551    0.11436   -0.10939   -0.61894   -0.18419\n",
      "  0.27755   -0.29704   -0.21562   -0.011765   0.051859   0.04041\n",
      " -0.16798    0.65697   -0.36868    0.068085   0.34986    0.076252\n",
      "  0.12035    0.16001   -0.0088939  0.14892   -0.70374   -0.031891\n",
      "  0.2803    -0.26916   -0.61974    0.67423    0.080773  -0.32835\n",
      " -0.276     -0.60342   -0.60185    0.10982   -0.20312    0.42309\n",
      "  0.63701    0.51885   -0.31899    0.34256   -0.035933  -0.049542\n",
      " -0.89278   -0.03686    0.12862    0.41469   -0.37262   -0.20523\n",
      " -0.02476   -0.11959   -0.12129    0.099622   0.24538   -0.026313 ]\n",
      "over:  [-8.8137e-02 -2.1696e-02  2.9863e-01 -1.8325e-02 -2.3575e-01  1.1022e-01\n",
      " -1.7493e-01  9.9241e-03  2.3832e-01 -1.7643e+00  2.2489e-01  4.0552e-01\n",
      " -4.8176e-01 -6.6099e-02  1.3290e-01  4.7502e-01 -5.6438e-02  3.2902e-01\n",
      "  9.7628e-02  4.2467e-01  2.5285e-01 -1.7258e-01  7.6564e-02 -1.5678e-01\n",
      " -2.2694e-01 -2.1213e-01 -3.3460e-01  7.3842e-02 -4.4671e-01  3.3979e-01\n",
      " -2.3534e-01  1.5013e-01 -3.4718e-01  5.4379e-02 -8.8699e-01 -3.5534e-01\n",
      " -1.3166e-01 -1.8265e-02 -1.8708e-01  3.0193e-01  1.0008e-01 -1.1980e-01\n",
      " -7.4867e-01  1.5592e-01  4.2757e-01 -2.9996e-01 -1.2499e-01  1.2750e-01\n",
      " -2.1962e-01  4.5077e-01 -3.2268e-01 -2.8934e-01 -1.3745e-01 -1.7738e-01\n",
      "  2.4185e-01 -1.2240e-02  1.7264e-01  2.7287e-01 -8.2743e-02  2.0570e-01\n",
      "  2.2559e-02 -7.5793e-02  2.1027e-01 -3.1239e-01  1.3032e-01 -7.5693e-01\n",
      "  9.0872e-02  4.9975e-01  1.1746e-01 -5.3133e-01 -8.5348e-02  4.4042e-01\n",
      "  1.2231e-01 -1.8672e-01  1.1668e-01  1.0322e-02 -2.6779e-01  1.5042e-01\n",
      " -5.4751e-01 -3.8439e-01 -6.7117e-01 -3.4912e-01  1.4190e-01  1.2742e-01\n",
      " -1.2879e-01 -1.3582e-01  2.9059e-01  1.4339e-01  2.8467e-01  7.0642e-02\n",
      "  4.7118e-03  1.8321e-01 -4.0973e-01  2.1973e-01 -6.6693e-01 -1.0599e-01\n",
      " -3.4465e-01  2.3103e-01 -3.6278e-01 -1.2365e-02  1.1544e-01  3.6134e-02\n",
      "  7.4889e-02 -6.4933e-01 -3.3362e-02 -3.0498e-01  4.0964e-01  2.6898e-01\n",
      " -2.1150e-01 -1.1446e-01 -4.8326e-02 -5.9265e-01 -3.7301e-01 -5.4902e-02\n",
      " -1.9869e-01  2.3181e-01 -2.9017e-01  6.2177e-01  8.9856e-02 -6.4116e-01\n",
      "  3.9304e-02  9.7170e-02 -2.9954e-01  4.0426e-01  8.4010e-02  1.2902e-01\n",
      " -3.5864e-01  3.9975e-01  1.3183e-01 -1.4797e-01 -3.1449e-01  4.8654e-01\n",
      "  2.9974e-02  5.3637e-01  1.1619e-01  1.4643e-01 -2.2927e-01  7.9999e-02\n",
      "  1.3114e-01 -3.0946e-02  1.6410e-01 -3.5597e-01  2.0801e-01  3.3193e-01\n",
      " -8.4092e-01  3.3762e-01 -7.5518e-02 -4.3158e-01  1.9592e-01 -2.7815e-01\n",
      "  6.6744e-01  3.1189e-02 -1.1706e-02  5.3737e-01  2.9596e-01  3.7335e-01\n",
      " -2.2166e-01  2.3843e-02 -1.1905e-01 -1.8529e-01  1.1857e-02 -2.3540e-01\n",
      "  2.3800e-01 -3.1987e-02 -1.6400e-01 -7.9319e-02  9.9570e-02  2.6182e-02\n",
      " -6.0793e-01 -6.7515e-02  3.9870e-02  7.0721e-02 -6.0965e-01 -7.3549e-01\n",
      " -8.1241e-03 -1.3363e-01 -1.6572e-01  3.2373e-01  1.0115e-01  6.0659e-01\n",
      " -2.2547e-02  2.5960e-01  2.4251e-01 -7.8426e-02 -7.8699e-02 -4.1680e-01\n",
      " -1.4208e-01  2.5136e-02 -3.4187e-01  2.7631e-01  2.4892e-01  2.7898e-01\n",
      " -2.6522e-01 -3.3989e-01  1.7929e-01 -4.0393e-01  2.7421e-01  2.2639e-01\n",
      "  4.5234e-01 -2.4806e-01  9.4945e-01  1.4040e-01  2.3117e-02 -3.3321e-01\n",
      "  3.6958e-01 -1.7507e-01 -1.8646e-01 -2.4238e-01  2.0633e-02 -7.8795e-02\n",
      "  5.1631e-01  4.5651e-01  2.5398e-01  3.8711e-01  1.6812e-01 -5.8263e-01\n",
      " -9.9027e-02 -2.3445e-01  3.6928e-02  6.1330e-02  2.4186e-01 -2.0908e-01\n",
      " -3.8720e-02  1.4625e-01 -3.6779e-02  2.5993e-01  3.1897e-01 -5.5307e-02\n",
      " -7.7177e-02  3.0117e-01  2.3307e-01  4.1980e-01 -1.3339e-01 -2.0384e-02\n",
      "  7.1158e-01  8.4989e-02  3.2791e-01  4.8060e-01 -3.3761e-01  5.7392e-02\n",
      " -2.6536e-01  3.0228e-01  1.8540e-01 -8.3752e-02 -7.3776e-01 -1.4404e-03\n",
      "  3.4121e-01  2.9959e-02  4.2035e-01  7.3122e-02 -1.9430e-01 -2.2341e-01\n",
      "  5.8658e-01 -2.2756e-01  6.3029e-01 -6.8682e-02 -3.5365e-01  2.6851e-01\n",
      " -7.3124e-02  1.5702e-02  1.2022e-01 -2.4631e-01  2.8471e-01  3.8292e-01\n",
      " -3.8869e-01  2.0974e-01 -2.0512e-01 -1.8508e-01  3.2543e-01  3.7336e-03\n",
      " -1.9901e-02 -8.4242e-02  2.9326e-01 -5.8162e-02  3.6575e-01  4.6895e-02\n",
      " -2.1328e+00 -3.5109e-01  4.1716e-01  1.8393e-01 -3.0990e-01  2.7303e-01\n",
      "  1.6627e-02  4.4978e-03 -7.2902e-02  5.3814e-02 -5.7213e-02  1.5815e-01\n",
      "  1.4517e-01  1.6580e-01 -7.3557e-02  1.6125e-01 -9.9237e-02  2.6099e-01\n",
      "  2.7941e-02  3.2140e-01  1.6931e-01 -3.5469e-01 -4.2713e-01 -3.9323e-01]\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "lazy:  [ 4.2791e-01 -1.6070e-01  2.4912e-01  3.9763e-01 -3.2224e-01  1.9783e-04\n",
      "  8.8576e-02  4.0501e-01 -2.4655e-01  1.6410e-02 -2.3331e-01 -1.2307e-01\n",
      " -3.9679e-01  2.1877e-02  1.6211e-01  3.8852e-01  2.5025e-01 -4.3968e-02\n",
      "  1.0819e+00  6.6517e-01  2.1829e-01  7.9898e-01 -6.1695e-01 -1.0184e-01\n",
      "  2.2900e-01 -3.1982e-01  3.1205e-01 -1.0453e-01  4.4810e-01  3.6708e-02\n",
      " -1.1376e-02  7.0543e-01  2.7454e-01  2.5835e-01 -5.1821e-01  8.2996e-01\n",
      " -8.0672e-02 -4.5489e-01 -3.4843e-01  5.1421e-01 -5.7408e-01  6.4612e-01\n",
      " -2.1564e-01 -5.4848e-01  2.3656e-01  1.8453e-02  8.6403e-01 -1.0357e-01\n",
      " -5.6097e-02 -4.8564e-01  2.7927e-01 -6.1733e-01  5.2877e-01 -1.9221e-01\n",
      " -1.4684e-01  4.2272e-01  5.6312e-02 -3.3168e-01  8.1864e-02  2.2225e-01\n",
      "  2.7757e-01 -2.1934e-02  2.8499e-01  5.7453e-03 -4.0598e-01  6.1549e-02\n",
      "  1.1544e-01  7.2053e-02 -5.4945e-02 -3.9549e-02 -1.9371e-01  2.7872e-01\n",
      " -1.0139e-01 -7.1846e-02 -3.4043e-01 -2.2490e-02 -2.6097e-03  6.5403e-01\n",
      "  4.1425e-01 -1.1459e-01 -4.9802e-01 -3.4516e-02 -3.0815e-02 -6.1508e-01\n",
      "  4.8955e-01 -1.0641e-01  6.3485e-02  5.5039e-01 -1.6282e-01 -7.7526e-02\n",
      " -1.5945e-01  3.0791e-01 -3.0439e-02 -3.8283e-01  7.8436e-02  1.1488e-01\n",
      "  6.7573e-02  2.2181e-01 -1.4319e-02  1.4366e-02  2.8839e-01 -9.1358e-01\n",
      " -5.3084e-01  1.3097e-01 -2.0027e-01  2.1495e-01 -3.6158e-01  7.6012e-02\n",
      "  1.2718e-01  2.5851e-01  6.6530e-02  3.1628e-01 -6.3175e-01 -4.1942e-01\n",
      "  3.8640e-01  7.3017e-02 -9.2298e-02 -8.8510e-01 -2.1618e-01  2.9006e-01\n",
      "  3.6404e-01  2.3740e-01 -2.2910e-01  3.2436e-01  6.0954e-01  3.2458e-01\n",
      " -8.4691e-02  4.1471e-01  2.6053e-01  6.8716e-02 -4.4528e-01  2.9296e-02\n",
      " -4.7913e-02  4.5709e-01 -5.2956e-01 -3.0998e-01  3.2488e-01 -3.6054e-01\n",
      "  1.8449e-01 -3.8492e-01  3.9918e-02  4.0046e-01 -8.7039e-02 -4.9739e-01\n",
      " -7.1877e-01  1.4894e-01  1.5550e-01  3.2388e-01  6.7209e-01  4.0215e-01\n",
      " -7.3436e-01  7.0311e-02 -7.3754e-02  3.1062e-01  1.4725e-03  4.3386e-02\n",
      "  2.9529e-01 -1.0544e-01  7.5028e-01  2.2274e-01  4.3789e-02 -6.7316e-01\n",
      "  6.8448e-01 -5.5239e-01 -2.2204e-02 -1.8850e-01 -4.0747e-01 -5.7321e-02\n",
      "  1.8211e-01 -5.8300e-01 -2.0733e-01  2.8411e-01 -3.5269e-01 -1.2422e-03\n",
      " -1.7660e-01 -6.9418e-01 -2.7758e-01  3.6022e-01 -1.7472e-01 -2.9792e-01\n",
      "  4.5945e-01  1.3176e-01 -2.2119e-01 -1.0709e-01  1.6470e-02  1.5636e-01\n",
      "  4.0296e-01 -7.1640e-01  4.4003e-01 -2.0855e-01 -5.1675e-01  3.3292e-01\n",
      "  2.5018e-01 -5.7151e-01 -7.5581e-04 -8.8453e-02 -4.6790e-01 -6.3341e-01\n",
      "  1.7374e-01 -1.2076e-01  5.9592e-01 -2.6351e-02 -1.7858e-02  1.2978e-01\n",
      "  2.4669e-01  9.1323e-02 -1.4112e-01  4.6466e-02  1.8030e-01  1.7382e-01\n",
      " -7.7972e-02 -1.4600e-01 -5.1364e-01  5.1154e-01 -3.3271e-01  5.2346e-01\n",
      " -9.4829e-02 -2.0365e-01  5.6919e-01 -1.5709e-01 -5.4340e-01  2.4034e-01\n",
      " -4.5061e-02  1.5918e-01 -7.0530e-01 -7.9981e-03  5.1987e-01 -1.6802e-01\n",
      " -8.0854e-03  2.4719e-01  3.6062e-01 -2.2302e-01 -3.2196e-01 -7.6371e-01\n",
      "  1.9203e-02  2.0398e-01 -4.4568e-01  1.1332e-01 -1.3784e-01 -5.6301e-02\n",
      "  5.5306e-01  4.6183e-01 -8.0710e-01 -4.1624e-01 -5.1206e-01 -8.1953e-01\n",
      "  7.8391e-03  2.8204e-02  2.0151e-01  4.5986e-01  2.0020e-01 -9.1094e-02\n",
      "  4.3782e-01  3.4559e-01  3.6562e-01  3.4960e-01  1.7984e-01 -2.3978e-01\n",
      " -2.5039e-01  6.7002e-03  1.0974e-01  8.1626e-02 -2.4783e-01  2.6453e-01\n",
      " -3.6779e-02  3.1099e-01  6.7982e-01 -4.6699e-01 -2.8060e-01  8.2703e-01\n",
      "  2.3553e-01 -7.4127e-01  2.9891e-02 -1.3198e-01  2.2106e-01  1.7262e-01\n",
      " -4.2037e-01  5.6484e-01 -7.0211e-01 -1.5537e-01 -8.0067e-02 -6.9698e-02\n",
      "  8.0176e-01 -2.4841e-01 -7.1711e-01 -2.5340e-01  7.2812e-01  1.6527e-01\n",
      " -2.2780e-01 -3.2008e-01  2.8609e-01 -5.8733e-02 -5.4448e-01  2.6380e-01\n",
      "  3.3366e-01 -5.1100e-01 -1.0377e-01 -1.9413e-01 -3.9855e-01  2.2238e-01]\n",
      "dog:  [-1.1043e-01  8.1217e-01  7.3668e-02  1.9023e-01 -5.2888e-02  6.1468e-02\n",
      "  1.6076e-01  4.1302e-01 -3.0199e-01 -9.0827e-01  2.7504e-01 -3.1890e-02\n",
      " -2.8842e-01  2.3447e-01  4.7679e-01  5.0124e-01  2.9371e-01  2.7029e-01\n",
      "  5.4745e-02  9.8038e-02  5.7116e-01  3.6755e-01  4.0734e-02  3.4347e-01\n",
      " -1.8256e-01 -2.8935e-01  2.3826e-02 -1.9401e-01  2.4444e-01  1.3407e-01\n",
      " -1.6494e-01 -2.6983e-01 -2.6234e-01 -2.1779e-01 -8.7528e-01  7.3822e-01\n",
      " -8.7931e-02 -1.0876e-02 -2.6540e-01  3.4668e-01 -5.5814e-01  1.7591e-01\n",
      "  1.6926e-01 -1.5725e-01 -5.0430e-01 -2.0100e-01  6.6701e-01 -3.2518e-02\n",
      "  4.5012e-02  6.5675e-02 -1.6061e-01 -7.3363e-01  2.4642e-01  3.4325e-01\n",
      "  2.1899e-01  4.8646e-02 -5.9987e-01 -5.8153e-02 -5.1694e-02 -5.7846e-01\n",
      "  3.0000e-01  3.5078e-01  4.6646e-01 -7.5309e-03  1.0455e-01 -5.1016e-01\n",
      " -5.5987e-02 -1.0295e-01 -2.6476e-01 -4.1230e-02 -2.8371e-02  5.1979e-01\n",
      " -3.4849e-01 -4.7217e-01 -3.7229e-01 -3.2790e-02  1.3989e-01  3.5716e-01\n",
      "  1.9305e-01 -2.1986e-01  2.4136e-01  4.0976e-01  3.7516e-01  1.4255e-01\n",
      " -3.4143e-02 -7.2653e-01 -1.0832e-01  6.8616e-01 -2.6335e-01 -4.2345e-01\n",
      " -2.4253e-01  1.5778e-01  1.4258e-01 -3.2749e-01 -3.4699e-01  1.6148e-01\n",
      "  1.9603e-01  4.1639e-01 -2.3370e-01  7.5816e-02  1.5899e-01  1.6623e-03\n",
      " -4.8301e-02 -1.0611e-01 -1.9326e-01  1.4494e-01  1.5406e-02  1.0629e-01\n",
      " -3.6699e-02  6.3230e-01  1.2986e-01  4.9902e-01 -1.1323e+00 -1.2636e-01\n",
      "  6.4718e-02  1.2374e-01 -4.9712e-01 -1.4836e-02  1.0488e-01 -4.9818e-01\n",
      " -2.8856e-01  3.8949e-01 -3.1828e-02 -2.8625e-01 -9.8758e-02 -7.6990e-02\n",
      " -2.4234e-01  7.5793e-01  3.4835e-01 -7.1030e-01  4.5318e-01 -3.4418e-01\n",
      " -1.9459e-01  6.1478e-01 -2.9010e-02 -2.7864e-01  3.8556e-01  1.0072e-01\n",
      "  1.2895e-01  1.7992e-02  3.3670e-01  2.0698e-01 -3.8049e-01 -6.6661e-03\n",
      "  1.1540e-01 -8.5268e-02 -1.4608e-01  4.4514e-01 -9.3674e-02  2.3639e-01\n",
      " -1.1447e-01  1.0948e+00 -5.7823e-02 -1.6295e-01  5.5880e-01 -1.8988e-02\n",
      " -7.1374e-02  2.1319e-01  6.1277e-02  7.2759e-01  6.2747e-01 -1.9280e-01\n",
      "  1.3057e-01  1.7426e-01 -1.0229e-01  1.5232e-01  5.2500e-01 -2.1919e-01\n",
      " -2.7185e-01 -5.4186e-01  3.1752e-01  1.6375e-01 -2.9039e-01  1.7074e-01\n",
      " -3.1814e-01 -9.6421e-01 -1.1610e-01 -2.9951e-01  1.8686e-01 -4.5986e-01\n",
      "  4.1633e-01 -1.7583e-01 -3.4583e-01 -2.7244e-01 -5.0216e-01  1.2852e-02\n",
      "  5.9838e-01 -1.1237e-01  2.4697e-01 -4.9048e-01 -4.4188e-01 -1.6255e-01\n",
      " -7.3313e-01 -3.7677e-01 -6.8925e-01  6.1174e-02 -4.2101e-01 -1.3153e-01\n",
      " -8.3590e-03 -1.8360e-02  1.3686e+00  4.6169e-02  9.4622e-01 -1.5126e-02\n",
      " -1.2477e-01  4.8754e-01  2.2384e-01 -2.1820e-01 -2.3389e-01  1.5207e-01\n",
      " -2.8718e-01 -6.3908e-01 -2.2383e-01 -1.8014e-01 -3.3548e-01  5.3587e-01\n",
      " -2.9367e-01  1.0866e-01  6.3411e-02 -9.3424e-03 -1.5886e-01  2.2602e-01\n",
      "  1.1925e-01 -4.1442e-01 -7.8062e-02 -9.7857e-02  2.7938e-01 -1.8348e-01\n",
      " -3.4584e-01  1.8489e-01  1.7402e-01 -5.2198e-01 -4.3306e-01  1.6256e-01\n",
      "  1.4032e-01  3.5124e-01 -1.8280e-01 -3.5984e-01 -1.3009e-01  1.6304e-01\n",
      "  3.1734e-01  3.7716e-03 -4.5498e-02 -4.2066e-01 -4.4419e-01 -6.8985e-01\n",
      " -4.9359e-01  7.0281e-02 -1.4377e-01  6.2508e-01 -5.6311e-02  1.8850e-01\n",
      " -5.6785e-02  1.4052e-01  1.1973e+00  7.1894e-01  5.4332e-01 -1.2461e-01\n",
      " -1.1978e-01  3.0163e-01 -1.6273e-01 -4.6740e-02 -2.5249e-01 -3.0659e-02\n",
      " -3.2271e-01  3.2361e-01  3.3244e-01 -2.7819e-02 -3.3367e-01 -2.3444e-02\n",
      " -5.0394e-01 -2.0587e-01 -1.3013e-01 -3.5884e-01  4.5384e-02 -1.1863e-01\n",
      " -1.7257e+00  3.9441e-01 -5.3179e-01  5.8209e-01 -6.5771e-01  3.6849e-01\n",
      "  2.3518e-01  1.0802e-01 -8.3159e-01  6.1486e-01  2.5547e-01 -4.5289e-01\n",
      "  5.1446e-01 -1.7911e-01 -1.2389e-01  1.8688e-01 -4.1102e-01 -7.0877e-01\n",
      " -3.7501e-01 -6.6152e-01  6.7730e-01  3.3936e-01  5.7994e-01  6.8149e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 100000 words:\n",
      "\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "quick:  [ 0.37594    0.063183   0.51835   -0.48652    0.34026   -0.17801\n",
      "  0.32615   -0.32989    0.25508   -1.026     -0.13841    0.27258\n",
      " -0.010244   0.35186    0.28341    0.3189    -0.18892   -0.292\n",
      " -0.071297   0.25631   -0.34286    0.16179    0.065725  -0.038052\n",
      "  0.1457     0.20289    0.14274   -0.15024    0.35412   -0.13715\n",
      " -0.2677    -0.011243  -0.1541     0.1765    -1.5424     0.37699\n",
      " -0.28239    0.19172   -0.46349   -0.26345   -0.39337    0.41276\n",
      "  0.42873   -0.1317     0.31096   -0.28715    0.47212   -0.41866\n",
      " -0.11871   -0.026621  -0.21089   -0.074757   0.26429   -0.032246\n",
      " -0.084233   0.48653   -0.38692   -0.3756    -0.27031    0.15208\n",
      "  0.2263    -0.28777    0.12475   -0.17029    0.019322   0.34175\n",
      "  0.69492   -0.42039    0.3161    -0.23871    0.30485   -0.078005\n",
      "  0.25937   -0.036324   0.27365   -0.1698    -0.10004    0.13614\n",
      "  0.0094279 -0.51261    0.3464     0.032112   0.25292    0.1947\n",
      "  0.16906   -0.044683  -0.039252   0.31651   -0.27185    0.10862\n",
      "  0.070371   0.31916   -0.55993   -0.5553     0.5493    -0.17244\n",
      " -0.70848    0.039063   0.33553   -0.11393   -0.28882   -0.53623\n",
      "  0.0021584  0.24971    0.31383   -0.2516    -0.28619    0.20113\n",
      " -0.29545   -0.3285     0.33289    0.19422    0.047601  -0.13157\n",
      "  0.4269     0.085041  -0.30294   -0.38344   -0.035083  -0.0463\n",
      "  0.035454  -0.052446   0.51216   -0.37809   -0.24834    0.28464\n",
      "  0.019408   0.61137    0.14859    0.30104    0.23773    0.37627\n",
      " -0.64467    0.19701   -0.19264   -0.013601   0.073281  -0.43031\n",
      "  0.38081   -0.42172   -0.16131    0.12108   -0.12078   -0.20818\n",
      " -0.4697     0.1279    -0.63088    0.16412    0.20474    0.16701\n",
      " -0.79632   -0.075741  -0.25251   -0.025189   0.081245  -0.081758\n",
      " -0.12925   -0.33034    0.039839  -0.30436    0.023003  -0.35589\n",
      "  0.40923   -0.10969   -0.084268   0.56261    0.37604    0.10676\n",
      " -0.1678     0.11219   -0.13141   -0.025916  -0.56102   -0.074779\n",
      " -0.14769    0.13028   -0.38351    0.055938   0.1996     0.0052525\n",
      "  0.11655   -0.58141    0.45407   -0.11067   -0.10262    0.31478\n",
      " -0.049739  -0.34926   -0.016468  -0.12476   -0.071381   0.34803\n",
      " -0.12247   -0.38406    0.095986   0.12451   -0.033609  -0.62353\n",
      "  0.25048   -0.1427     0.60613   -0.080829   0.25008    0.059055\n",
      " -0.17486    0.14913   -0.41488    0.27573   -0.11921   -0.02267\n",
      " -0.34188   -0.49563   -0.22119    0.49553   -0.035482  -0.11908\n",
      "  0.0096008 -0.44059   -0.35947   -0.19156    0.28505    0.35236\n",
      " -0.3384    -0.28643   -0.22068   -0.29761    0.10412   -0.067384\n",
      "  0.043089  -0.05794   -0.31212    0.24026    0.072559  -0.024896\n",
      " -0.19299    0.020044  -0.10826    0.2022     0.097076   0.43886\n",
      "  0.35085    0.38611   -0.1838    -0.047166  -0.5351    -0.17215\n",
      "  0.17407    0.17959   -0.35965   -0.23817    0.16348   -0.79488\n",
      "  0.18858    0.027404   0.23823    0.047581   0.2485     0.18332\n",
      " -0.22626    0.54554   -0.31324   -0.22699   -0.31341    0.68296\n",
      "  0.13422   -0.27644   -0.38901   -0.29207   -0.10058    0.12057\n",
      " -0.36691   -0.69507   -0.22242   -0.023121   0.72283    0.0051197\n",
      " -1.7769     0.40651    0.025966  -0.18157    0.23957    0.37943\n",
      "  0.67713    0.49789    0.3634    -0.60131    0.53868   -0.18682\n",
      " -0.14783    0.40581    0.1379     0.054337  -0.12388    0.064828\n",
      "  0.27453    0.5165    -0.1955    -0.55939   -0.2744    -0.12146  ]\n",
      "brown:  [ 0.2793     0.18372   -0.11257    0.21734   -0.21657   -0.50335\n",
      " -0.27194    0.32181    0.031892  -0.37998    0.15544   -0.32953\n",
      " -0.19827    0.20403    0.26768    0.292     -0.34187   -0.10766\n",
      " -0.43697   -0.14488    0.14634    0.21591    0.12576    0.14895\n",
      " -0.21763    0.030797   0.10949   -0.41689   -0.30296   -0.14592\n",
      " -0.56228    0.33282   -0.20436   -0.24403   -1.4732     0.68345\n",
      "  0.45336    0.43671   -0.15641    0.15075   -0.24265   -0.040059\n",
      "  0.22323    0.19523    0.37445   -0.18509   -0.10302   -0.055363\n",
      " -0.17274   -0.45401   -0.14729   -0.24133   -0.043826  -0.23243\n",
      "  0.42367    0.15906   -0.14039   -0.36185   -0.26695   -0.42724\n",
      " -0.08843   -0.099597   0.24257   -0.05424    0.10746   -1.1304\n",
      "  0.024651  -0.10212    0.046319  -0.68792    0.4214    -0.25844\n",
      "  0.17052    0.097878   0.026835   0.32044    0.0062988  0.24575\n",
      "  0.20126   -0.16771    0.19825    0.28939   -0.064994  -0.38766\n",
      "  0.52509    0.38195    0.32421    0.20683   -0.48472   -0.080334\n",
      " -0.15345    0.35459   -0.43765    0.071575  -0.39516   -0.22906\n",
      "  0.25686    0.26659    0.37626   -0.18556    0.16445   -0.33614\n",
      " -0.56262    0.067852  -0.61642    0.19546    0.45027    0.20238\n",
      "  0.33957    0.41372    0.11855    0.087619   0.18754    0.17901\n",
      "  0.022569  -0.10854   -0.47226    0.41039    0.32588   -0.58468\n",
      " -0.0057296 -0.29201   -0.12777   -0.15729   -0.40103   -0.039414\n",
      " -0.1192     0.40093    0.032862   0.39862   -0.63525    0.11594\n",
      " -0.39954    0.36919   -0.50021   -0.51169   -0.13955    0.18055\n",
      " -0.079918  -0.19474    0.53131    0.093723   0.2773    -0.40505\n",
      " -0.20568    0.11139    0.032661  -0.04852    0.44576    0.23667\n",
      "  0.54981    0.23585   -0.51539   -0.46424    0.021099  -0.3919\n",
      "  0.58338   -0.89908    0.094066   0.30159   -0.063199  -0.31635\n",
      "  0.50333   -0.068517  -0.38681    0.33      -0.49463    0.75491\n",
      " -0.088266  -0.19413    0.4238    -0.031727  -0.4464    -0.21028\n",
      " -0.11151   -0.07088   -0.027832  -0.63304    0.27336   -0.47925\n",
      " -0.03239    0.46069    0.16968   -0.38262   -0.31413   -0.29068\n",
      " -0.031801  -0.48974   -0.50999    0.1466     0.0027995  0.56333\n",
      " -0.044347  -0.085679   0.20559   -0.051593   0.75228   -0.013291\n",
      " -0.084694  -0.4305     1.1734    -0.083233   0.1561    -0.15758\n",
      "  0.19066   -0.2966     0.63704    0.45616   -0.34797   -0.12732\n",
      "  0.4901    -0.51217   -0.063474  -0.061496   0.28825    0.17711\n",
      "  0.46301   -0.12697   -0.044627  -1.0064     0.76394    0.20494\n",
      "  0.028766   0.27597    0.021726  -0.12054    0.23284    0.18999\n",
      "  0.30048   -0.056139   0.09546   -0.036514   0.0084885  0.016599\n",
      " -0.31428   -0.2707     0.099281   0.4445    -0.36      -0.55556\n",
      " -0.18551   -0.30644    0.056475  -0.19197   -0.48886    0.33044\n",
      "  0.19535   -0.53828    0.12385   -0.29372   -0.1036     0.0051129\n",
      "  0.11483   -0.10591    0.73337    0.26978   -0.06925    0.11565\n",
      "  0.27711    0.15109   -0.069137  -0.14481    0.32319    0.039345\n",
      " -0.44964    0.27103    0.045326  -0.064534  -0.37144    0.47615\n",
      " -0.61105   -0.11922   -0.068806   0.15401   -0.40812    0.32575\n",
      " -1.2888     0.0203    -0.12893   -0.22211   -0.16402    0.29018\n",
      "  0.36295   -0.081025  -0.50492    0.5046    -0.37485    0.52111\n",
      "  0.1757     0.069686   0.48937   -0.17747   -0.20577    0.70419\n",
      "  0.068633   0.47878   -0.21754   -0.016868  -0.91378    0.45643  ]\n",
      "fox:  [-1.1570e-01 -2.5048e-02 -1.1013e-01 -4.8060e-02 -8.5504e-02  3.7308e-01\n",
      " -9.4790e-01  5.9662e-01 -2.0006e-02 -3.0469e-01  2.4997e-01 -2.0005e-01\n",
      "  1.9284e-01  4.7197e-01  2.7099e-01  3.4190e-01 -2.9186e-01 -3.9718e-01\n",
      "  5.5653e-01  3.2774e-01  9.4860e-02 -4.6588e-01  6.2119e-01  2.0709e-01\n",
      "  8.2332e-02 -1.0175e-01  3.2067e-01 -3.1875e-01  1.7941e-01 -2.4127e-01\n",
      " -1.7961e-02 -8.7154e-02  3.3423e-01 -4.5026e-02 -1.2193e+00  1.9321e-01\n",
      "  2.1427e-01  2.3893e-01 -1.8084e-01 -1.5920e-01 -1.0387e-01  2.2060e-01\n",
      "  2.5423e-01  2.0815e-01 -3.3072e-01 -2.0672e-01 -3.1424e-01  2.6498e-02\n",
      "  2.1955e-01 -3.1157e-01 -7.8914e-02 -1.5089e-02 -4.0848e-02 -2.3807e-01\n",
      "  3.1983e-01 -7.4173e-02  2.2983e-01  5.5735e-02  5.6743e-03 -7.7368e-01\n",
      "  2.7412e-01 -1.8999e-01  5.3825e-01 -1.9784e-01 -1.0898e-01 -3.4022e-01\n",
      "  3.1582e-01  5.0317e-01  3.3955e-01 -2.7143e-01 -3.3026e-02  3.4279e-01\n",
      " -4.9815e-01  2.5056e-01 -1.2791e-01 -1.5912e-01 -8.7541e-03  1.0450e-01\n",
      "  2.1969e-01 -5.9077e-02 -3.7600e-01  6.0835e-02  1.8427e-01 -8.3592e-02\n",
      "  1.1632e-01  7.5145e-01  6.5186e-02 -3.3258e-01  1.4249e-01 -6.5348e-01\n",
      " -2.6661e-03  1.0233e-01 -3.8731e-01  6.0529e-01 -2.7326e-01 -3.5777e-01\n",
      " -5.3533e-01  4.9589e-02  2.0224e-01 -3.2805e-01  3.9305e-01 -2.7540e-01\n",
      " -1.3042e-01  9.5833e-01  3.8853e-01  5.9884e-01 -3.0503e-01 -1.0708e-01\n",
      "  4.5670e-01  7.2182e-01 -4.4221e-02 -6.8201e-02 -2.9903e-01  2.6483e-01\n",
      "  6.6810e-03  1.4919e-01 -5.3155e-01 -6.1697e-01 -5.4271e-01 -4.1103e-01\n",
      " -1.7534e-01  1.3229e-01  2.6958e-02 -1.5111e-01 -1.6844e-01 -3.0684e-01\n",
      " -5.1189e-02  9.2940e-01 -6.0882e-01 -1.1373e-01 -4.2237e-01 -4.0481e-01\n",
      " -3.4182e-01  3.8472e-01 -1.1566e-01  2.2679e-02  4.4793e-01  1.8849e-01\n",
      " -1.9598e-01 -6.8914e-02 -4.4520e-01  4.1643e-01  5.8319e-02 -3.9575e-01\n",
      " -4.0471e-01  4.2855e-01  1.6339e-01 -1.6965e-01 -5.7793e-02 -1.0358e-01\n",
      "  1.0223e+00  9.0564e-01 -5.5851e-01 -2.4856e-01  7.5582e-02 -2.7374e-01\n",
      "  3.8838e-01  5.1561e-01  6.0029e-02 -5.1990e-02  1.0113e+00  8.3607e-03\n",
      "  4.9764e-01  3.2839e-02  2.5892e-02  4.0404e-01 -3.7393e-01  1.3384e-01\n",
      " -2.9458e-01  3.4668e-01  5.2114e-02  1.0785e-01 -8.4700e-01  4.0936e-01\n",
      "  3.8801e-01  4.5079e-01 -4.3880e-01  1.9777e-01  1.9102e-01 -7.6580e-02\n",
      "  2.5788e-01  3.5642e-01  2.3305e-01 -1.6206e-01 -5.0847e-02  1.3708e-01\n",
      "  5.1338e-02 -3.8552e-01 -2.5497e-01  5.3901e-02 -6.2922e-01  3.2780e-01\n",
      " -2.0383e-01 -1.7265e-01 -9.9668e-02  2.4485e-01  3.2700e-02  3.2325e-01\n",
      "  2.3912e-01  7.7014e-03  1.2890e+00  6.8030e-02  2.0671e-02  2.6268e-01\n",
      "  1.9434e-01  5.6238e-01  7.1863e-02 -6.5502e-02 -2.2858e-01  9.2883e-02\n",
      "  3.9936e-01 -2.8038e-01  3.5633e-01  1.5203e-01 -2.7883e-01  3.8225e-02\n",
      " -3.9636e-01  3.2488e-01  2.4332e-01  2.2621e-01  2.3518e-01 -6.4747e-02\n",
      "  6.2858e-01 -1.1848e-01  1.2710e-01 -1.2697e-01  1.0015e+00 -2.5693e-01\n",
      "  2.0450e-01 -1.8789e-01 -1.2913e-01 -9.6456e-02  1.2301e-01 -1.3377e-01\n",
      " -1.2808e-01  4.1857e-02  8.6418e-01  3.6137e-01  9.0372e-02 -1.0687e-02\n",
      "  4.4736e-01  7.8975e-02 -3.2992e-01 -1.4470e-01 -6.1296e-01 -4.1543e-01\n",
      " -2.6104e-01 -8.4169e-01  4.7352e-01 -4.7640e-01 -3.4393e-01  8.2156e-02\n",
      " -2.4477e-01  7.9361e-01  2.6183e-01  4.6648e-01  1.9135e-01 -5.5033e-02\n",
      "  2.3492e-01  1.1178e-01 -3.7291e-01  2.6764e-01  1.1074e-01  6.8041e-01\n",
      " -1.0361e-03 -4.2537e-02  9.4006e-01 -2.8939e-01 -2.0841e-01  3.3001e-01\n",
      "  7.3318e-02 -5.1902e-01 -3.6412e-01 -4.1372e-01  2.0266e-01  4.3162e-01\n",
      " -1.0174e+00  2.5093e-01  3.8136e-01 -1.4332e-01 -2.0525e-01 -1.8724e-01\n",
      "  5.9925e-01  3.5604e-01 -9.1457e-01 -2.2401e-01 -4.2382e-02 -1.3299e-01\n",
      "  6.1671e-01  4.4886e-01 -1.1361e-01  2.0483e-01  2.5485e-01  2.7581e-01\n",
      " -8.7158e-01 -1.3156e-01  2.6117e-01  1.5815e-01 -3.2199e-01  7.1042e-01]\n",
      "jumps:  [-0.16814   -0.10948    0.2896    -0.21108   -0.29061    0.31201\n",
      "  0.04039   -0.10149   -0.18526   -0.55483   -0.36055   -0.093569\n",
      "  0.77334    0.027921   0.13389   -0.1014    -0.06482    0.24753\n",
      " -0.068026   0.26147   -0.14252    0.18657    0.030249  -0.07934\n",
      "  0.87696    0.61781    0.36835   -0.07306   -0.19303    0.3721\n",
      " -0.77087   -0.0062782 -0.19233   -0.43884   -0.79847    0.066636\n",
      " -0.21387   -0.65263   -0.073964   0.64115   -0.52014   -0.05981\n",
      " -0.1692    -0.413      0.060197   0.16327    0.43353    0.070021\n",
      "  0.063543   0.28527   -0.43474    0.15441    0.098037   0.098685\n",
      " -0.3965     0.38171   -0.084065  -0.4813    -0.59213    0.40444\n",
      " -0.2026     0.45569    0.039036  -0.41786   -0.20322   -0.11932\n",
      "  0.23747    0.26336    0.17139    0.12521   -0.55276    0.45515\n",
      " -0.63826    0.14054    0.35333   -0.28417    0.3889    -0.13004\n",
      " -0.027142  -0.23109    0.034327  -0.10685    0.85855    0.15145\n",
      "  0.16814    0.2281     0.22235   -0.1825    -0.019222   0.026105\n",
      "  0.47734    0.42115   -0.087767  -0.17439    0.22166   -0.36831\n",
      " -1.069      0.40489   -0.31038   -0.21588   -0.7282     0.29296\n",
      " -0.42949    0.23485    0.020585  -0.47795   -0.20216   -0.22146\n",
      " -0.45778    0.032547  -0.13727   -0.48945   -0.58148    0.051203\n",
      " -0.065926   0.46718   -0.080438  -0.25042   -0.4015     0.40254\n",
      "  0.12       0.5246     0.21582    0.1333     0.27662    0.2163\n",
      " -0.28177    0.67185   -0.025996  -0.40781   -0.23629    0.97455\n",
      " -0.51452    0.22697   -0.34857   -0.55928    0.019104  -0.016163\n",
      " -0.22626    0.094269   0.10808   -0.018804  -0.080299   0.0058964\n",
      " -0.61886    0.44842    0.2494    -0.25172    0.6705     0.23657\n",
      " -0.17631    0.28634   -0.39527   -0.22096    0.23069    0.0644\n",
      "  0.13151   -0.030479   0.38503   -0.084794   0.66178   -0.34578\n",
      "  0.3868     0.31506    0.20155   -0.026189   0.051188   0.16359\n",
      " -0.37096    0.21546   -0.19758   -0.083407   0.48437   -0.5487\n",
      "  0.35188   -0.43539    0.3976    -0.026514  -0.14485   -0.11205\n",
      "  0.59835    0.38055    0.1582    -0.24293    0.39837    0.44276\n",
      "  0.36113   -0.2358    -0.32568    0.54672   -0.36544   -0.33871\n",
      "  0.2154    -0.36877   -0.18857    0.42323   -0.56938   -0.20276\n",
      "  0.35452   -0.0167     1.157      0.22296   -0.35115    0.3662\n",
      " -0.20903    0.73497    0.018414   0.27861    0.044337   0.14504\n",
      " -0.23001   -0.54025   -0.26259   -0.94587    0.22239    0.44651\n",
      " -0.46075   -0.42224    0.0022315 -0.25522   -0.13135    0.65923\n",
      "  0.21267   -0.26498   -0.16777   -0.047013  -0.18551    0.12607\n",
      "  0.43205    0.153     -0.33748    0.010732  -0.18332   -0.28507\n",
      "  0.23396    0.3868    -0.34724    0.41692    0.56315    0.32366\n",
      "  0.55303    0.02256   -0.48281    0.54561   -0.51086   -0.55446\n",
      " -0.018992   0.62551    0.11436   -0.10939   -0.61894   -0.18419\n",
      "  0.27755   -0.29704   -0.21562   -0.011765   0.051859   0.04041\n",
      " -0.16798    0.65697   -0.36868    0.068085   0.34986    0.076252\n",
      "  0.12035    0.16001   -0.0088939  0.14892   -0.70374   -0.031891\n",
      "  0.2803    -0.26916   -0.61974    0.67423    0.080773  -0.32835\n",
      " -0.276     -0.60342   -0.60185    0.10982   -0.20312    0.42309\n",
      "  0.63701    0.51885   -0.31899    0.34256   -0.035933  -0.049542\n",
      " -0.89278   -0.03686    0.12862    0.41469   -0.37262   -0.20523\n",
      " -0.02476   -0.11959   -0.12129    0.099622   0.24538   -0.026313 ]\n",
      "over:  [-8.8137e-02 -2.1696e-02  2.9863e-01 -1.8325e-02 -2.3575e-01  1.1022e-01\n",
      " -1.7493e-01  9.9241e-03  2.3832e-01 -1.7643e+00  2.2489e-01  4.0552e-01\n",
      " -4.8176e-01 -6.6099e-02  1.3290e-01  4.7502e-01 -5.6438e-02  3.2902e-01\n",
      "  9.7628e-02  4.2467e-01  2.5285e-01 -1.7258e-01  7.6564e-02 -1.5678e-01\n",
      " -2.2694e-01 -2.1213e-01 -3.3460e-01  7.3842e-02 -4.4671e-01  3.3979e-01\n",
      " -2.3534e-01  1.5013e-01 -3.4718e-01  5.4379e-02 -8.8699e-01 -3.5534e-01\n",
      " -1.3166e-01 -1.8265e-02 -1.8708e-01  3.0193e-01  1.0008e-01 -1.1980e-01\n",
      " -7.4867e-01  1.5592e-01  4.2757e-01 -2.9996e-01 -1.2499e-01  1.2750e-01\n",
      " -2.1962e-01  4.5077e-01 -3.2268e-01 -2.8934e-01 -1.3745e-01 -1.7738e-01\n",
      "  2.4185e-01 -1.2240e-02  1.7264e-01  2.7287e-01 -8.2743e-02  2.0570e-01\n",
      "  2.2559e-02 -7.5793e-02  2.1027e-01 -3.1239e-01  1.3032e-01 -7.5693e-01\n",
      "  9.0872e-02  4.9975e-01  1.1746e-01 -5.3133e-01 -8.5348e-02  4.4042e-01\n",
      "  1.2231e-01 -1.8672e-01  1.1668e-01  1.0322e-02 -2.6779e-01  1.5042e-01\n",
      " -5.4751e-01 -3.8439e-01 -6.7117e-01 -3.4912e-01  1.4190e-01  1.2742e-01\n",
      " -1.2879e-01 -1.3582e-01  2.9059e-01  1.4339e-01  2.8467e-01  7.0642e-02\n",
      "  4.7118e-03  1.8321e-01 -4.0973e-01  2.1973e-01 -6.6693e-01 -1.0599e-01\n",
      " -3.4465e-01  2.3103e-01 -3.6278e-01 -1.2365e-02  1.1544e-01  3.6134e-02\n",
      "  7.4889e-02 -6.4933e-01 -3.3362e-02 -3.0498e-01  4.0964e-01  2.6898e-01\n",
      " -2.1150e-01 -1.1446e-01 -4.8326e-02 -5.9265e-01 -3.7301e-01 -5.4902e-02\n",
      " -1.9869e-01  2.3181e-01 -2.9017e-01  6.2177e-01  8.9856e-02 -6.4116e-01\n",
      "  3.9304e-02  9.7170e-02 -2.9954e-01  4.0426e-01  8.4010e-02  1.2902e-01\n",
      " -3.5864e-01  3.9975e-01  1.3183e-01 -1.4797e-01 -3.1449e-01  4.8654e-01\n",
      "  2.9974e-02  5.3637e-01  1.1619e-01  1.4643e-01 -2.2927e-01  7.9999e-02\n",
      "  1.3114e-01 -3.0946e-02  1.6410e-01 -3.5597e-01  2.0801e-01  3.3193e-01\n",
      " -8.4092e-01  3.3762e-01 -7.5518e-02 -4.3158e-01  1.9592e-01 -2.7815e-01\n",
      "  6.6744e-01  3.1189e-02 -1.1706e-02  5.3737e-01  2.9596e-01  3.7335e-01\n",
      " -2.2166e-01  2.3843e-02 -1.1905e-01 -1.8529e-01  1.1857e-02 -2.3540e-01\n",
      "  2.3800e-01 -3.1987e-02 -1.6400e-01 -7.9319e-02  9.9570e-02  2.6182e-02\n",
      " -6.0793e-01 -6.7515e-02  3.9870e-02  7.0721e-02 -6.0965e-01 -7.3549e-01\n",
      " -8.1241e-03 -1.3363e-01 -1.6572e-01  3.2373e-01  1.0115e-01  6.0659e-01\n",
      " -2.2547e-02  2.5960e-01  2.4251e-01 -7.8426e-02 -7.8699e-02 -4.1680e-01\n",
      " -1.4208e-01  2.5136e-02 -3.4187e-01  2.7631e-01  2.4892e-01  2.7898e-01\n",
      " -2.6522e-01 -3.3989e-01  1.7929e-01 -4.0393e-01  2.7421e-01  2.2639e-01\n",
      "  4.5234e-01 -2.4806e-01  9.4945e-01  1.4040e-01  2.3117e-02 -3.3321e-01\n",
      "  3.6958e-01 -1.7507e-01 -1.8646e-01 -2.4238e-01  2.0633e-02 -7.8795e-02\n",
      "  5.1631e-01  4.5651e-01  2.5398e-01  3.8711e-01  1.6812e-01 -5.8263e-01\n",
      " -9.9027e-02 -2.3445e-01  3.6928e-02  6.1330e-02  2.4186e-01 -2.0908e-01\n",
      " -3.8720e-02  1.4625e-01 -3.6779e-02  2.5993e-01  3.1897e-01 -5.5307e-02\n",
      " -7.7177e-02  3.0117e-01  2.3307e-01  4.1980e-01 -1.3339e-01 -2.0384e-02\n",
      "  7.1158e-01  8.4989e-02  3.2791e-01  4.8060e-01 -3.3761e-01  5.7392e-02\n",
      " -2.6536e-01  3.0228e-01  1.8540e-01 -8.3752e-02 -7.3776e-01 -1.4404e-03\n",
      "  3.4121e-01  2.9959e-02  4.2035e-01  7.3122e-02 -1.9430e-01 -2.2341e-01\n",
      "  5.8658e-01 -2.2756e-01  6.3029e-01 -6.8682e-02 -3.5365e-01  2.6851e-01\n",
      " -7.3124e-02  1.5702e-02  1.2022e-01 -2.4631e-01  2.8471e-01  3.8292e-01\n",
      " -3.8869e-01  2.0974e-01 -2.0512e-01 -1.8508e-01  3.2543e-01  3.7336e-03\n",
      " -1.9901e-02 -8.4242e-02  2.9326e-01 -5.8162e-02  3.6575e-01  4.6895e-02\n",
      " -2.1328e+00 -3.5109e-01  4.1716e-01  1.8393e-01 -3.0990e-01  2.7303e-01\n",
      "  1.6627e-02  4.4978e-03 -7.2902e-02  5.3814e-02 -5.7213e-02  1.5815e-01\n",
      "  1.4517e-01  1.6580e-01 -7.3557e-02  1.6125e-01 -9.9237e-02  2.6099e-01\n",
      "  2.7941e-02  3.2140e-01  1.6931e-01 -3.5469e-01 -4.2713e-01 -3.9323e-01]\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "lazy:  [ 4.2791e-01 -1.6070e-01  2.4912e-01  3.9763e-01 -3.2224e-01  1.9783e-04\n",
      "  8.8576e-02  4.0501e-01 -2.4655e-01  1.6410e-02 -2.3331e-01 -1.2307e-01\n",
      " -3.9679e-01  2.1877e-02  1.6211e-01  3.8852e-01  2.5025e-01 -4.3968e-02\n",
      "  1.0819e+00  6.6517e-01  2.1829e-01  7.9898e-01 -6.1695e-01 -1.0184e-01\n",
      "  2.2900e-01 -3.1982e-01  3.1205e-01 -1.0453e-01  4.4810e-01  3.6708e-02\n",
      " -1.1376e-02  7.0543e-01  2.7454e-01  2.5835e-01 -5.1821e-01  8.2996e-01\n",
      " -8.0672e-02 -4.5489e-01 -3.4843e-01  5.1421e-01 -5.7408e-01  6.4612e-01\n",
      " -2.1564e-01 -5.4848e-01  2.3656e-01  1.8453e-02  8.6403e-01 -1.0357e-01\n",
      " -5.6097e-02 -4.8564e-01  2.7927e-01 -6.1733e-01  5.2877e-01 -1.9221e-01\n",
      " -1.4684e-01  4.2272e-01  5.6312e-02 -3.3168e-01  8.1864e-02  2.2225e-01\n",
      "  2.7757e-01 -2.1934e-02  2.8499e-01  5.7453e-03 -4.0598e-01  6.1549e-02\n",
      "  1.1544e-01  7.2053e-02 -5.4945e-02 -3.9549e-02 -1.9371e-01  2.7872e-01\n",
      " -1.0139e-01 -7.1846e-02 -3.4043e-01 -2.2490e-02 -2.6097e-03  6.5403e-01\n",
      "  4.1425e-01 -1.1459e-01 -4.9802e-01 -3.4516e-02 -3.0815e-02 -6.1508e-01\n",
      "  4.8955e-01 -1.0641e-01  6.3485e-02  5.5039e-01 -1.6282e-01 -7.7526e-02\n",
      " -1.5945e-01  3.0791e-01 -3.0439e-02 -3.8283e-01  7.8436e-02  1.1488e-01\n",
      "  6.7573e-02  2.2181e-01 -1.4319e-02  1.4366e-02  2.8839e-01 -9.1358e-01\n",
      " -5.3084e-01  1.3097e-01 -2.0027e-01  2.1495e-01 -3.6158e-01  7.6012e-02\n",
      "  1.2718e-01  2.5851e-01  6.6530e-02  3.1628e-01 -6.3175e-01 -4.1942e-01\n",
      "  3.8640e-01  7.3017e-02 -9.2298e-02 -8.8510e-01 -2.1618e-01  2.9006e-01\n",
      "  3.6404e-01  2.3740e-01 -2.2910e-01  3.2436e-01  6.0954e-01  3.2458e-01\n",
      " -8.4691e-02  4.1471e-01  2.6053e-01  6.8716e-02 -4.4528e-01  2.9296e-02\n",
      " -4.7913e-02  4.5709e-01 -5.2956e-01 -3.0998e-01  3.2488e-01 -3.6054e-01\n",
      "  1.8449e-01 -3.8492e-01  3.9918e-02  4.0046e-01 -8.7039e-02 -4.9739e-01\n",
      " -7.1877e-01  1.4894e-01  1.5550e-01  3.2388e-01  6.7209e-01  4.0215e-01\n",
      " -7.3436e-01  7.0311e-02 -7.3754e-02  3.1062e-01  1.4725e-03  4.3386e-02\n",
      "  2.9529e-01 -1.0544e-01  7.5028e-01  2.2274e-01  4.3789e-02 -6.7316e-01\n",
      "  6.8448e-01 -5.5239e-01 -2.2204e-02 -1.8850e-01 -4.0747e-01 -5.7321e-02\n",
      "  1.8211e-01 -5.8300e-01 -2.0733e-01  2.8411e-01 -3.5269e-01 -1.2422e-03\n",
      " -1.7660e-01 -6.9418e-01 -2.7758e-01  3.6022e-01 -1.7472e-01 -2.9792e-01\n",
      "  4.5945e-01  1.3176e-01 -2.2119e-01 -1.0709e-01  1.6470e-02  1.5636e-01\n",
      "  4.0296e-01 -7.1640e-01  4.4003e-01 -2.0855e-01 -5.1675e-01  3.3292e-01\n",
      "  2.5018e-01 -5.7151e-01 -7.5581e-04 -8.8453e-02 -4.6790e-01 -6.3341e-01\n",
      "  1.7374e-01 -1.2076e-01  5.9592e-01 -2.6351e-02 -1.7858e-02  1.2978e-01\n",
      "  2.4669e-01  9.1323e-02 -1.4112e-01  4.6466e-02  1.8030e-01  1.7382e-01\n",
      " -7.7972e-02 -1.4600e-01 -5.1364e-01  5.1154e-01 -3.3271e-01  5.2346e-01\n",
      " -9.4829e-02 -2.0365e-01  5.6919e-01 -1.5709e-01 -5.4340e-01  2.4034e-01\n",
      " -4.5061e-02  1.5918e-01 -7.0530e-01 -7.9981e-03  5.1987e-01 -1.6802e-01\n",
      " -8.0854e-03  2.4719e-01  3.6062e-01 -2.2302e-01 -3.2196e-01 -7.6371e-01\n",
      "  1.9203e-02  2.0398e-01 -4.4568e-01  1.1332e-01 -1.3784e-01 -5.6301e-02\n",
      "  5.5306e-01  4.6183e-01 -8.0710e-01 -4.1624e-01 -5.1206e-01 -8.1953e-01\n",
      "  7.8391e-03  2.8204e-02  2.0151e-01  4.5986e-01  2.0020e-01 -9.1094e-02\n",
      "  4.3782e-01  3.4559e-01  3.6562e-01  3.4960e-01  1.7984e-01 -2.3978e-01\n",
      " -2.5039e-01  6.7002e-03  1.0974e-01  8.1626e-02 -2.4783e-01  2.6453e-01\n",
      " -3.6779e-02  3.1099e-01  6.7982e-01 -4.6699e-01 -2.8060e-01  8.2703e-01\n",
      "  2.3553e-01 -7.4127e-01  2.9891e-02 -1.3198e-01  2.2106e-01  1.7262e-01\n",
      " -4.2037e-01  5.6484e-01 -7.0211e-01 -1.5537e-01 -8.0067e-02 -6.9698e-02\n",
      "  8.0176e-01 -2.4841e-01 -7.1711e-01 -2.5340e-01  7.2812e-01  1.6527e-01\n",
      " -2.2780e-01 -3.2008e-01  2.8609e-01 -5.8733e-02 -5.4448e-01  2.6380e-01\n",
      "  3.3366e-01 -5.1100e-01 -1.0377e-01 -1.9413e-01 -3.9855e-01  2.2238e-01]\n",
      "dog:  [-1.1043e-01  8.1217e-01  7.3668e-02  1.9023e-01 -5.2888e-02  6.1468e-02\n",
      "  1.6076e-01  4.1302e-01 -3.0199e-01 -9.0827e-01  2.7504e-01 -3.1890e-02\n",
      " -2.8842e-01  2.3447e-01  4.7679e-01  5.0124e-01  2.9371e-01  2.7029e-01\n",
      "  5.4745e-02  9.8038e-02  5.7116e-01  3.6755e-01  4.0734e-02  3.4347e-01\n",
      " -1.8256e-01 -2.8935e-01  2.3826e-02 -1.9401e-01  2.4444e-01  1.3407e-01\n",
      " -1.6494e-01 -2.6983e-01 -2.6234e-01 -2.1779e-01 -8.7528e-01  7.3822e-01\n",
      " -8.7931e-02 -1.0876e-02 -2.6540e-01  3.4668e-01 -5.5814e-01  1.7591e-01\n",
      "  1.6926e-01 -1.5725e-01 -5.0430e-01 -2.0100e-01  6.6701e-01 -3.2518e-02\n",
      "  4.5012e-02  6.5675e-02 -1.6061e-01 -7.3363e-01  2.4642e-01  3.4325e-01\n",
      "  2.1899e-01  4.8646e-02 -5.9987e-01 -5.8153e-02 -5.1694e-02 -5.7846e-01\n",
      "  3.0000e-01  3.5078e-01  4.6646e-01 -7.5309e-03  1.0455e-01 -5.1016e-01\n",
      " -5.5987e-02 -1.0295e-01 -2.6476e-01 -4.1230e-02 -2.8371e-02  5.1979e-01\n",
      " -3.4849e-01 -4.7217e-01 -3.7229e-01 -3.2790e-02  1.3989e-01  3.5716e-01\n",
      "  1.9305e-01 -2.1986e-01  2.4136e-01  4.0976e-01  3.7516e-01  1.4255e-01\n",
      " -3.4143e-02 -7.2653e-01 -1.0832e-01  6.8616e-01 -2.6335e-01 -4.2345e-01\n",
      " -2.4253e-01  1.5778e-01  1.4258e-01 -3.2749e-01 -3.4699e-01  1.6148e-01\n",
      "  1.9603e-01  4.1639e-01 -2.3370e-01  7.5816e-02  1.5899e-01  1.6623e-03\n",
      " -4.8301e-02 -1.0611e-01 -1.9326e-01  1.4494e-01  1.5406e-02  1.0629e-01\n",
      " -3.6699e-02  6.3230e-01  1.2986e-01  4.9902e-01 -1.1323e+00 -1.2636e-01\n",
      "  6.4718e-02  1.2374e-01 -4.9712e-01 -1.4836e-02  1.0488e-01 -4.9818e-01\n",
      " -2.8856e-01  3.8949e-01 -3.1828e-02 -2.8625e-01 -9.8758e-02 -7.6990e-02\n",
      " -2.4234e-01  7.5793e-01  3.4835e-01 -7.1030e-01  4.5318e-01 -3.4418e-01\n",
      " -1.9459e-01  6.1478e-01 -2.9010e-02 -2.7864e-01  3.8556e-01  1.0072e-01\n",
      "  1.2895e-01  1.7992e-02  3.3670e-01  2.0698e-01 -3.8049e-01 -6.6661e-03\n",
      "  1.1540e-01 -8.5268e-02 -1.4608e-01  4.4514e-01 -9.3674e-02  2.3639e-01\n",
      " -1.1447e-01  1.0948e+00 -5.7823e-02 -1.6295e-01  5.5880e-01 -1.8988e-02\n",
      " -7.1374e-02  2.1319e-01  6.1277e-02  7.2759e-01  6.2747e-01 -1.9280e-01\n",
      "  1.3057e-01  1.7426e-01 -1.0229e-01  1.5232e-01  5.2500e-01 -2.1919e-01\n",
      " -2.7185e-01 -5.4186e-01  3.1752e-01  1.6375e-01 -2.9039e-01  1.7074e-01\n",
      " -3.1814e-01 -9.6421e-01 -1.1610e-01 -2.9951e-01  1.8686e-01 -4.5986e-01\n",
      "  4.1633e-01 -1.7583e-01 -3.4583e-01 -2.7244e-01 -5.0216e-01  1.2852e-02\n",
      "  5.9838e-01 -1.1237e-01  2.4697e-01 -4.9048e-01 -4.4188e-01 -1.6255e-01\n",
      " -7.3313e-01 -3.7677e-01 -6.8925e-01  6.1174e-02 -4.2101e-01 -1.3153e-01\n",
      " -8.3590e-03 -1.8360e-02  1.3686e+00  4.6169e-02  9.4622e-01 -1.5126e-02\n",
      " -1.2477e-01  4.8754e-01  2.2384e-01 -2.1820e-01 -2.3389e-01  1.5207e-01\n",
      " -2.8718e-01 -6.3908e-01 -2.2383e-01 -1.8014e-01 -3.3548e-01  5.3587e-01\n",
      " -2.9367e-01  1.0866e-01  6.3411e-02 -9.3424e-03 -1.5886e-01  2.2602e-01\n",
      "  1.1925e-01 -4.1442e-01 -7.8062e-02 -9.7857e-02  2.7938e-01 -1.8348e-01\n",
      " -3.4584e-01  1.8489e-01  1.7402e-01 -5.2198e-01 -4.3306e-01  1.6256e-01\n",
      "  1.4032e-01  3.5124e-01 -1.8280e-01 -3.5984e-01 -1.3009e-01  1.6304e-01\n",
      "  3.1734e-01  3.7716e-03 -4.5498e-02 -4.2066e-01 -4.4419e-01 -6.8985e-01\n",
      " -4.9359e-01  7.0281e-02 -1.4377e-01  6.2508e-01 -5.6311e-02  1.8850e-01\n",
      " -5.6785e-02  1.4052e-01  1.1973e+00  7.1894e-01  5.4332e-01 -1.2461e-01\n",
      " -1.1978e-01  3.0163e-01 -1.6273e-01 -4.6740e-02 -2.5249e-01 -3.0659e-02\n",
      " -3.2271e-01  3.2361e-01  3.3244e-01 -2.7819e-02 -3.3367e-01 -2.3444e-02\n",
      " -5.0394e-01 -2.0587e-01 -1.3013e-01 -3.5884e-01  4.5384e-02 -1.1863e-01\n",
      " -1.7257e+00  3.9441e-01 -5.3179e-01  5.8209e-01 -6.5771e-01  3.6849e-01\n",
      "  2.3518e-01  1.0802e-01 -8.3159e-01  6.1486e-01  2.5547e-01 -4.5289e-01\n",
      "  5.1446e-01 -1.7911e-01 -1.2389e-01  1.8688e-01 -4.1102e-01 -7.0877e-01\n",
      " -3.7501e-01 -6.6152e-01  6.7730e-01  3.3936e-01  5.7994e-01  6.8149e-02]\n",
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-positive\n",
      "max_review_length: 1052\n",
      "min_review_length: 22\n",
      "First word in first document: while\n",
      "Embedding for this word:\n",
      " [-1.9472e-01  1.8836e-01  1.1739e-01 -1.8991e-03 -1.8963e-01 -6.7638e-02\n",
      " -1.3963e-01 -3.6326e-02 -1.6799e-02 -1.4820e+00 -2.7629e-03  9.8703e-02\n",
      "  5.9868e-02  1.5742e-01  1.4054e-01  1.3313e-01 -1.3587e-01  7.9241e-02\n",
      " -5.7048e-02 -3.3121e-01 -1.0595e-01  3.1271e-01  4.3222e-01 -9.9310e-02\n",
      " -1.8090e-01 -2.3958e-01  7.0335e-02  5.7132e-02  4.9771e-02  1.6937e-01\n",
      " -1.4879e-01  3.5118e-01  3.2477e-01  6.3851e-02 -1.2043e+00 -3.8819e-01\n",
      "  1.9153e-01  1.9014e-03 -1.5201e-01  2.2219e-01  1.3716e-01 -4.2525e-02\n",
      " -3.3781e-01 -1.0667e-01 -1.4212e-01  2.5880e-02  3.8361e-01  3.6539e-01\n",
      " -4.2780e-02  7.7897e-02 -5.6149e-02 -2.6875e-01  2.4329e-01 -4.7278e-03\n",
      " -2.3142e-03  1.2450e-01 -4.9689e-03  3.3361e-02  1.3893e-01 -3.6135e-03\n",
      " -4.0820e-02  5.3111e-02  3.8276e-01  1.1413e-01 -1.1556e-01 -6.7151e-01\n",
      "  1.0093e-01  1.6463e-02  2.2443e-02  3.4206e-02 -1.1453e-01 -5.2428e-02\n",
      "  3.0135e-02 -5.9207e-02  2.4909e-02 -1.6277e-02  1.6996e-01  1.9266e-01\n",
      " -2.4422e-01  7.5231e-02  1.3790e-01 -2.5580e-01 -9.1647e-02  2.3055e-01\n",
      "  7.9488e-02 -5.5318e-03 -1.1481e-02  1.4759e-01  1.4456e-01  8.6181e-02\n",
      "  2.8940e-01  1.2176e-01 -9.0553e-02  2.2209e-01 -5.7667e-02 -3.2162e-01\n",
      " -3.2202e-01  3.0178e-02 -6.3200e-02 -5.1000e-01  2.5363e-02  3.1580e-02\n",
      " -2.1081e-01 -3.2171e-01 -1.2988e-01  4.6144e-01  2.0522e-01  3.8018e-01\n",
      " -4.6028e-01 -5.5502e-02 -5.9889e-02 -2.3684e-01 -1.7002e-02 -2.6099e-02\n",
      "  1.8669e-01  1.9199e-01 -1.4562e-01  9.6590e-02 -6.1454e-02 -5.6896e-02\n",
      " -5.1130e-01 -3.2049e-03 -1.0491e-01 -4.4255e-02 -3.7002e-01 -2.1858e-01\n",
      " -1.0680e-01 -2.2429e-01  2.7597e-01 -2.3302e-01 -4.2686e-01  5.4869e-04\n",
      "  2.8455e-01  2.1347e-02 -5.4664e-02  6.6111e-02  1.5381e-01 -1.2572e-01\n",
      "  2.7120e-01  3.3996e-01  2.5948e-01  1.8138e-01 -3.2012e-02  2.1576e-01\n",
      " -3.5938e-01  1.7972e-01  9.9849e-02 -2.8064e-01 -3.3303e-02 -8.4656e-02\n",
      "  2.8755e-01 -1.7199e-01 -9.4199e-02  1.3209e-01  4.4943e-01 -1.9775e-01\n",
      " -5.1901e-01 -4.9810e-02 -8.9386e-03  5.0636e-02  4.7966e-01 -6.0549e-01\n",
      "  1.7089e-01 -1.8651e-01  3.6259e-02  1.8433e-01 -1.2240e-01 -4.2995e-02\n",
      "  1.0503e-01  2.4983e-01  1.0062e-01 -2.0852e-01 -3.3951e-01  1.8636e-01\n",
      "  9.1880e-02 -2.8719e-02  7.2556e-02  2.0066e-01 -3.1819e-01  6.5279e-01\n",
      " -1.0925e-01  2.0030e-01  1.2181e-02 -1.4259e-01 -5.0102e-02 -1.7363e-02\n",
      " -6.9740e-03  1.6336e-01 -8.0266e-02 -1.7524e-01  1.3976e-02  1.5656e-01\n",
      " -3.2603e-01 -3.3087e-01 -2.7052e-01 -1.3885e-01 -1.6769e-01 -1.4385e-01\n",
      "  2.4883e-01 -1.0426e-01  7.9906e-01 -2.9098e-01 -1.7864e-01  4.1688e-01\n",
      " -7.4282e-02  7.7736e-02  6.3283e-02 -9.4733e-02  1.0527e-01 -1.2301e-01\n",
      " -2.7754e-02  1.3078e-01  4.2556e-02 -5.1533e-02  4.2722e-01  2.4317e-01\n",
      " -2.0954e-01  3.4540e-01 -1.1758e-01  1.7618e-01  3.0023e-01 -9.8725e-02\n",
      " -5.1650e-02  9.9928e-02 -3.6114e-01  8.4246e-02  1.9471e-01 -1.5502e-01\n",
      " -1.5988e-01  9.3713e-02 -5.5386e-02 -5.3721e-02  9.5336e-02  3.3402e-01\n",
      "  4.6005e-01 -2.6252e-01  1.1194e-01  3.0805e-01 -1.6688e-01  3.1467e-01\n",
      " -1.0996e-01  6.0206e-01  2.3838e-01  3.4277e-01 -3.7964e-01  2.0090e-03\n",
      "  5.7560e-01 -2.1853e-01 -1.5855e-01  3.4270e-01  3.3626e-02 -2.2984e-02\n",
      "  2.9521e-01  5.0213e-03  1.6369e-01 -1.9080e-01 -3.4195e-01 -1.9340e-01\n",
      " -1.5975e-01 -8.4150e-03  1.5673e-01 -1.2553e-01 -5.1382e-02  1.1615e-01\n",
      " -6.0482e-02  4.4293e-01  2.2198e-02 -2.9353e-01 -4.2204e-01  2.1842e-01\n",
      " -7.7585e-02 -1.0179e-01 -3.0344e-02  2.8122e-01 -1.0290e-01  1.5925e-01\n",
      " -1.8687e+00 -3.3726e-01  8.6103e-02  1.9028e-01 -4.1232e-01  4.2654e-02\n",
      " -7.0320e-02 -3.7245e-01  2.6863e-02  4.5806e-01 -3.7607e-01  5.3417e-01\n",
      "  1.1403e-01 -2.3730e-01  2.5681e-02  2.4612e-02 -3.5385e-02  2.2309e-02\n",
      "  2.6702e-01  4.1790e-01  6.4943e-03 -3.8294e-01 -2.5358e-01 -1.2843e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-1.9472e-01  1.8836e-01  1.1739e-01 -1.8991e-03 -1.8963e-01 -6.7638e-02\n",
      " -1.3963e-01 -3.6326e-02 -1.6799e-02 -1.4820e+00 -2.7629e-03  9.8703e-02\n",
      "  5.9868e-02  1.5742e-01  1.4054e-01  1.3313e-01 -1.3587e-01  7.9241e-02\n",
      " -5.7048e-02 -3.3121e-01 -1.0595e-01  3.1271e-01  4.3222e-01 -9.9310e-02\n",
      " -1.8090e-01 -2.3958e-01  7.0335e-02  5.7132e-02  4.9771e-02  1.6937e-01\n",
      " -1.4879e-01  3.5118e-01  3.2477e-01  6.3851e-02 -1.2043e+00 -3.8819e-01\n",
      "  1.9153e-01  1.9014e-03 -1.5201e-01  2.2219e-01  1.3716e-01 -4.2525e-02\n",
      " -3.3781e-01 -1.0667e-01 -1.4212e-01  2.5880e-02  3.8361e-01  3.6539e-01\n",
      " -4.2780e-02  7.7897e-02 -5.6149e-02 -2.6875e-01  2.4329e-01 -4.7278e-03\n",
      " -2.3142e-03  1.2450e-01 -4.9689e-03  3.3361e-02  1.3893e-01 -3.6135e-03\n",
      " -4.0820e-02  5.3111e-02  3.8276e-01  1.1413e-01 -1.1556e-01 -6.7151e-01\n",
      "  1.0093e-01  1.6463e-02  2.2443e-02  3.4206e-02 -1.1453e-01 -5.2428e-02\n",
      "  3.0135e-02 -5.9207e-02  2.4909e-02 -1.6277e-02  1.6996e-01  1.9266e-01\n",
      " -2.4422e-01  7.5231e-02  1.3790e-01 -2.5580e-01 -9.1647e-02  2.3055e-01\n",
      "  7.9488e-02 -5.5318e-03 -1.1481e-02  1.4759e-01  1.4456e-01  8.6181e-02\n",
      "  2.8940e-01  1.2176e-01 -9.0553e-02  2.2209e-01 -5.7667e-02 -3.2162e-01\n",
      " -3.2202e-01  3.0178e-02 -6.3200e-02 -5.1000e-01  2.5363e-02  3.1580e-02\n",
      " -2.1081e-01 -3.2171e-01 -1.2988e-01  4.6144e-01  2.0522e-01  3.8018e-01\n",
      " -4.6028e-01 -5.5502e-02 -5.9889e-02 -2.3684e-01 -1.7002e-02 -2.6099e-02\n",
      "  1.8669e-01  1.9199e-01 -1.4562e-01  9.6590e-02 -6.1454e-02 -5.6896e-02\n",
      " -5.1130e-01 -3.2049e-03 -1.0491e-01 -4.4255e-02 -3.7002e-01 -2.1858e-01\n",
      " -1.0680e-01 -2.2429e-01  2.7597e-01 -2.3302e-01 -4.2686e-01  5.4869e-04\n",
      "  2.8455e-01  2.1347e-02 -5.4664e-02  6.6111e-02  1.5381e-01 -1.2572e-01\n",
      "  2.7120e-01  3.3996e-01  2.5948e-01  1.8138e-01 -3.2012e-02  2.1576e-01\n",
      " -3.5938e-01  1.7972e-01  9.9849e-02 -2.8064e-01 -3.3303e-02 -8.4656e-02\n",
      "  2.8755e-01 -1.7199e-01 -9.4199e-02  1.3209e-01  4.4943e-01 -1.9775e-01\n",
      " -5.1901e-01 -4.9810e-02 -8.9386e-03  5.0636e-02  4.7966e-01 -6.0549e-01\n",
      "  1.7089e-01 -1.8651e-01  3.6259e-02  1.8433e-01 -1.2240e-01 -4.2995e-02\n",
      "  1.0503e-01  2.4983e-01  1.0062e-01 -2.0852e-01 -3.3951e-01  1.8636e-01\n",
      "  9.1880e-02 -2.8719e-02  7.2556e-02  2.0066e-01 -3.1819e-01  6.5279e-01\n",
      " -1.0925e-01  2.0030e-01  1.2181e-02 -1.4259e-01 -5.0102e-02 -1.7363e-02\n",
      " -6.9740e-03  1.6336e-01 -8.0266e-02 -1.7524e-01  1.3976e-02  1.5656e-01\n",
      " -3.2603e-01 -3.3087e-01 -2.7052e-01 -1.3885e-01 -1.6769e-01 -1.4385e-01\n",
      "  2.4883e-01 -1.0426e-01  7.9906e-01 -2.9098e-01 -1.7864e-01  4.1688e-01\n",
      " -7.4282e-02  7.7736e-02  6.3283e-02 -9.4733e-02  1.0527e-01 -1.2301e-01\n",
      " -2.7754e-02  1.3078e-01  4.2556e-02 -5.1533e-02  4.2722e-01  2.4317e-01\n",
      " -2.0954e-01  3.4540e-01 -1.1758e-01  1.7618e-01  3.0023e-01 -9.8725e-02\n",
      " -5.1650e-02  9.9928e-02 -3.6114e-01  8.4246e-02  1.9471e-01 -1.5502e-01\n",
      " -1.5988e-01  9.3713e-02 -5.5386e-02 -5.3721e-02  9.5336e-02  3.3402e-01\n",
      "  4.6005e-01 -2.6252e-01  1.1194e-01  3.0805e-01 -1.6688e-01  3.1467e-01\n",
      " -1.0996e-01  6.0206e-01  2.3838e-01  3.4277e-01 -3.7964e-01  2.0090e-03\n",
      "  5.7560e-01 -2.1853e-01 -1.5855e-01  3.4270e-01  3.3626e-02 -2.2984e-02\n",
      "  2.9521e-01  5.0213e-03  1.6369e-01 -1.9080e-01 -3.4195e-01 -1.9340e-01\n",
      " -1.5975e-01 -8.4150e-03  1.5673e-01 -1.2553e-01 -5.1382e-02  1.1615e-01\n",
      " -6.0482e-02  4.4293e-01  2.2198e-02 -2.9353e-01 -4.2204e-01  2.1842e-01\n",
      " -7.7585e-02 -1.0179e-01 -3.0344e-02  2.8122e-01 -1.0290e-01  1.5925e-01\n",
      " -1.8687e+00 -3.3726e-01  8.6103e-02  1.9028e-01 -4.1232e-01  4.2654e-02\n",
      " -7.0320e-02 -3.7245e-01  2.6863e-02  4.5806e-01 -3.7607e-01  5.3417e-01\n",
      "  1.1403e-01 -2.3730e-01  2.5681e-02  2.4612e-02 -3.5385e-02  2.2309e-02\n",
      "  2.6702e-01  4.1790e-01  6.4943e-03 -3.8294e-01 -2.5358e-01 -1.2843e-01]\n",
      "First word in first document: officially\n",
      "Embedding for this word:\n",
      " [ 2.3382e-01  1.4786e-01  1.1077e-01  2.3996e-01 -2.9585e-01 -1.6727e-01\n",
      " -2.8175e-02  1.6083e-01  1.1507e-01 -1.1158e+00  1.5384e-01  1.6024e-01\n",
      "  7.2660e-02 -1.6972e-01  6.8051e-01 -1.2424e-01  6.3706e-02 -5.2588e-02\n",
      "  2.7927e-01 -8.8411e-02  8.9556e-03  2.7327e-01 -1.1099e-01  2.3637e-01\n",
      " -2.0016e-01  1.5873e-01 -3.1033e-01 -3.6783e-01  3.4196e-01  2.6334e-01\n",
      "  7.8430e-01  9.5231e-02  3.5778e-01  2.6411e-01 -4.2257e-01  1.8524e-01\n",
      "  3.5195e-01  3.3158e-02 -8.7731e-04 -5.0436e-02 -3.2886e-01  2.8192e-01\n",
      " -2.6564e-01  6.4443e-01  1.9853e-01 -2.5915e-01  1.4498e-01 -2.7210e-01\n",
      " -1.0360e-01 -1.5602e-01 -1.5242e-01 -8.0593e-02 -2.5024e-01  4.0126e-01\n",
      " -8.9539e-02  1.4189e-01  5.0669e-01  3.3599e-01 -2.2001e-02 -2.2898e-01\n",
      " -1.0182e-01  5.8937e-01 -3.8595e-01 -1.4345e-01  9.8736e-02 -6.7044e-01\n",
      "  2.7835e-01  2.5707e-01 -5.0605e-01  5.5569e-02 -1.0464e-01 -9.3889e-02\n",
      " -1.1917e-01  5.0083e-01  3.2615e-02 -2.8911e-02  5.8029e-01  6.8503e-01\n",
      "  1.0228e-02  2.7785e-01 -2.0873e-01 -9.3943e-02  1.1824e-02 -9.9075e-02\n",
      "  4.6631e-01 -1.7372e-01 -4.2725e-01 -3.7469e-02  2.6391e-01  1.6405e-01\n",
      " -6.0724e-01 -8.4765e-01  2.9234e-01  1.6936e-01 -3.3004e-01  2.0924e-01\n",
      " -2.1591e-01  2.8508e-01 -2.6982e-01 -2.5857e-01  1.9987e-01 -7.0493e-01\n",
      "  1.5161e-01  4.0602e-01  1.5787e-01  2.8346e-01 -3.0581e-02 -2.3879e-01\n",
      " -4.2869e-02  5.4554e-01 -6.2593e-02 -8.4068e-02 -2.2914e-01  1.4995e-01\n",
      " -3.4521e-01 -3.2032e-01 -3.7594e-01 -4.5375e-01  6.3275e-01 -4.7160e-01\n",
      " -2.7696e-01 -4.7523e-02  3.4061e-02  7.5782e-02  4.3946e-02  1.2893e-01\n",
      " -1.0306e-02 -3.2766e-02 -4.7612e-02  2.1110e-02  7.2606e-03  2.4494e-02\n",
      "  4.1124e-01 -1.1190e-01 -3.7260e-01 -1.0450e-01  2.3772e-01  1.4764e-01\n",
      " -5.7967e-02 -3.2230e-01  1.1821e-01  6.2660e-01 -3.5705e-01 -5.7315e-02\n",
      "  9.8854e-02 -3.2027e-01  1.8756e-01 -6.0273e-02  1.4244e-01  5.1511e-01\n",
      "  6.2247e-01 -2.1388e-01 -4.0873e-01 -2.7409e-02  2.6528e-01  2.0100e-01\n",
      " -3.3757e-02 -3.7453e-01 -1.1762e-01  7.4104e-02  1.1627e-01  3.8526e-01\n",
      " -2.2062e-01  3.6973e-01 -9.6166e-02 -2.2351e-01 -2.5046e-01  6.0957e-01\n",
      "  3.0601e-01  1.0105e-01 -8.9082e-02 -8.0634e-02  1.4848e-01 -1.6454e-02\n",
      "  5.2633e-01  2.9695e-02  3.6702e-01 -2.4726e-01 -2.5372e-01  6.3151e-01\n",
      "  2.3604e-01  7.2727e-01  2.2325e-01  3.2010e-01  4.3032e-01  1.7370e-01\n",
      " -7.5582e-01  1.0571e-02 -1.2223e-01 -1.6370e-01  4.4239e-01 -4.5328e-02\n",
      " -1.1029e-01 -8.0053e-02 -8.0851e-02 -2.1034e-01 -1.5776e-01  3.2142e-01\n",
      "  2.0392e-01  2.5432e-01  6.6951e-01 -4.1141e-01 -3.1512e-01 -1.0077e-01\n",
      " -7.6977e-02  9.2796e-03  8.9229e-02  1.8865e-02  2.5894e-02  3.5324e-01\n",
      "  1.8868e-01 -3.4531e-01 -2.3498e-01 -1.0321e-01 -4.4609e-01  1.4354e-01\n",
      " -1.9525e-01  1.2388e-01 -1.6920e-01  3.8052e-01  3.7588e-01 -2.7014e-01\n",
      "  4.8001e-02 -4.0118e-01  5.4665e-01 -1.0004e-01  4.5669e-01 -1.8925e-01\n",
      " -8.1724e-02  4.0910e-01  2.9538e-01 -1.0452e+00 -2.8339e-01  1.4045e-02\n",
      " -9.1247e-02  1.1702e-02  2.1762e-02 -4.6720e-01  4.7209e-01  8.8291e-02\n",
      " -1.3753e-03 -5.7195e-01  3.7423e-01 -4.0193e-01  5.3075e-01  6.0023e-01\n",
      "  6.4926e-01 -1.9299e-01  3.3988e-01 -2.3158e-01  6.4617e-02  3.5429e-01\n",
      " -6.9228e-01  3.0218e-01 -3.4043e-01 -3.8970e-01 -7.5647e-02 -2.1047e-01\n",
      "  1.4411e-01 -1.2869e-01 -3.4671e-01  2.3031e-02 -9.8913e-02  7.2117e-02\n",
      " -3.4731e-01 -1.1492e-01  1.8138e-01 -1.0492e-01  9.3857e-02 -5.9654e-01\n",
      "  1.4362e-01 -6.6271e-02  6.1860e-02 -2.0465e-01  9.0121e-02 -4.8971e-03\n",
      " -1.6644e+00 -3.2758e-01  7.7074e-01  3.6992e-01  3.1160e-01 -4.1195e-01\n",
      "  9.4183e-02 -1.8495e-01 -3.9561e-01  1.1006e-01 -6.6274e-01  1.3011e-01\n",
      " -5.2989e-01  3.8145e-02 -1.6298e-01 -2.9403e-01 -2.6884e-01  1.3890e-01\n",
      " -5.5917e-01  5.4519e-01 -1.0583e-01 -3.4939e-01  4.7005e-01 -2.1085e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 2.3382e-01  1.4786e-01  1.1077e-01  2.3996e-01 -2.9585e-01 -1.6727e-01\n",
      " -2.8175e-02  1.6083e-01  1.1507e-01 -1.1158e+00  1.5384e-01  1.6024e-01\n",
      "  7.2660e-02 -1.6972e-01  6.8051e-01 -1.2424e-01  6.3706e-02 -5.2588e-02\n",
      "  2.7927e-01 -8.8411e-02  8.9556e-03  2.7327e-01 -1.1099e-01  2.3637e-01\n",
      " -2.0016e-01  1.5873e-01 -3.1033e-01 -3.6783e-01  3.4196e-01  2.6334e-01\n",
      "  7.8430e-01  9.5231e-02  3.5778e-01  2.6411e-01 -4.2257e-01  1.8524e-01\n",
      "  3.5195e-01  3.3158e-02 -8.7731e-04 -5.0436e-02 -3.2886e-01  2.8192e-01\n",
      " -2.6564e-01  6.4443e-01  1.9853e-01 -2.5915e-01  1.4498e-01 -2.7210e-01\n",
      " -1.0360e-01 -1.5602e-01 -1.5242e-01 -8.0593e-02 -2.5024e-01  4.0126e-01\n",
      " -8.9539e-02  1.4189e-01  5.0669e-01  3.3599e-01 -2.2001e-02 -2.2898e-01\n",
      " -1.0182e-01  5.8937e-01 -3.8595e-01 -1.4345e-01  9.8736e-02 -6.7044e-01\n",
      "  2.7835e-01  2.5707e-01 -5.0605e-01  5.5569e-02 -1.0464e-01 -9.3889e-02\n",
      " -1.1917e-01  5.0083e-01  3.2615e-02 -2.8911e-02  5.8029e-01  6.8503e-01\n",
      "  1.0228e-02  2.7785e-01 -2.0873e-01 -9.3943e-02  1.1824e-02 -9.9075e-02\n",
      "  4.6631e-01 -1.7372e-01 -4.2725e-01 -3.7469e-02  2.6391e-01  1.6405e-01\n",
      " -6.0724e-01 -8.4765e-01  2.9234e-01  1.6936e-01 -3.3004e-01  2.0924e-01\n",
      " -2.1591e-01  2.8508e-01 -2.6982e-01 -2.5857e-01  1.9987e-01 -7.0493e-01\n",
      "  1.5161e-01  4.0602e-01  1.5787e-01  2.8346e-01 -3.0581e-02 -2.3879e-01\n",
      " -4.2869e-02  5.4554e-01 -6.2593e-02 -8.4068e-02 -2.2914e-01  1.4995e-01\n",
      " -3.4521e-01 -3.2032e-01 -3.7594e-01 -4.5375e-01  6.3275e-01 -4.7160e-01\n",
      " -2.7696e-01 -4.7523e-02  3.4061e-02  7.5782e-02  4.3946e-02  1.2893e-01\n",
      " -1.0306e-02 -3.2766e-02 -4.7612e-02  2.1110e-02  7.2606e-03  2.4494e-02\n",
      "  4.1124e-01 -1.1190e-01 -3.7260e-01 -1.0450e-01  2.3772e-01  1.4764e-01\n",
      " -5.7967e-02 -3.2230e-01  1.1821e-01  6.2660e-01 -3.5705e-01 -5.7315e-02\n",
      "  9.8854e-02 -3.2027e-01  1.8756e-01 -6.0273e-02  1.4244e-01  5.1511e-01\n",
      "  6.2247e-01 -2.1388e-01 -4.0873e-01 -2.7409e-02  2.6528e-01  2.0100e-01\n",
      " -3.3757e-02 -3.7453e-01 -1.1762e-01  7.4104e-02  1.1627e-01  3.8526e-01\n",
      " -2.2062e-01  3.6973e-01 -9.6166e-02 -2.2351e-01 -2.5046e-01  6.0957e-01\n",
      "  3.0601e-01  1.0105e-01 -8.9082e-02 -8.0634e-02  1.4848e-01 -1.6454e-02\n",
      "  5.2633e-01  2.9695e-02  3.6702e-01 -2.4726e-01 -2.5372e-01  6.3151e-01\n",
      "  2.3604e-01  7.2727e-01  2.2325e-01  3.2010e-01  4.3032e-01  1.7370e-01\n",
      " -7.5582e-01  1.0571e-02 -1.2223e-01 -1.6370e-01  4.4239e-01 -4.5328e-02\n",
      " -1.1029e-01 -8.0053e-02 -8.0851e-02 -2.1034e-01 -1.5776e-01  3.2142e-01\n",
      "  2.0392e-01  2.5432e-01  6.6951e-01 -4.1141e-01 -3.1512e-01 -1.0077e-01\n",
      " -7.6977e-02  9.2796e-03  8.9229e-02  1.8865e-02  2.5894e-02  3.5324e-01\n",
      "  1.8868e-01 -3.4531e-01 -2.3498e-01 -1.0321e-01 -4.4609e-01  1.4354e-01\n",
      " -1.9525e-01  1.2388e-01 -1.6920e-01  3.8052e-01  3.7588e-01 -2.7014e-01\n",
      "  4.8001e-02 -4.0118e-01  5.4665e-01 -1.0004e-01  4.5669e-01 -1.8925e-01\n",
      " -8.1724e-02  4.0910e-01  2.9538e-01 -1.0452e+00 -2.8339e-01  1.4045e-02\n",
      " -9.1247e-02  1.1702e-02  2.1762e-02 -4.6720e-01  4.7209e-01  8.8291e-02\n",
      " -1.3753e-03 -5.7195e-01  3.7423e-01 -4.0193e-01  5.3075e-01  6.0023e-01\n",
      "  6.4926e-01 -1.9299e-01  3.3988e-01 -2.3158e-01  6.4617e-02  3.5429e-01\n",
      " -6.9228e-01  3.0218e-01 -3.4043e-01 -3.8970e-01 -7.5647e-02 -2.1047e-01\n",
      "  1.4411e-01 -1.2869e-01 -3.4671e-01  2.3031e-02 -9.8913e-02  7.2117e-02\n",
      " -3.4731e-01 -1.1492e-01  1.8138e-01 -1.0492e-01  9.3857e-02 -5.9654e-01\n",
      "  1.4362e-01 -6.6271e-02  6.1860e-02 -2.0465e-01  9.0121e-02 -4.8971e-03\n",
      " -1.6644e+00 -3.2758e-01  7.7074e-01  3.6992e-01  3.1160e-01 -4.1195e-01\n",
      "  9.4183e-02 -1.8495e-01 -3.9561e-01  1.1006e-01 -6.6274e-01  1.3011e-01\n",
      " -5.2989e-01  3.8145e-02 -1.6298e-01 -2.9403e-01 -2.6884e-01  1.3890e-01\n",
      " -5.5917e-01  5.4519e-01 -1.0583e-01 -3.4939e-01  4.7005e-01 -2.1085e-01]\n",
      "First word in first document: super\n",
      "Embedding for this word:\n",
      " [-1.3896e-01  1.2130e+00  2.4560e-01 -2.7516e-01  5.0725e-01  3.7306e-01\n",
      "  6.0455e-01  4.5422e-01 -3.2791e-01 -5.6665e-01  3.8336e-01 -6.8814e-01\n",
      " -1.3871e-01 -4.2354e-01  2.4258e-02 -2.1776e-01  2.0820e-01 -9.1329e-02\n",
      " -1.0846e-01 -4.5497e-01  4.2489e-01  2.1485e-01  2.5729e-01  1.0212e-01\n",
      "  3.3257e-01 -2.5691e-01  1.6149e-01 -2.1938e-01  4.1485e-01 -9.8947e-01\n",
      " -3.0017e-01 -3.3480e-01 -1.6191e-01  3.1174e-02 -1.5225e+00  1.4193e-01\n",
      " -6.4055e-02  2.9956e-01  2.9768e-01  7.6586e-02 -1.9656e-01 -3.4299e-02\n",
      " -5.3394e-01 -4.2036e-02 -3.0388e-01 -1.1457e-01  9.0635e-02 -6.1258e-01\n",
      "  1.6595e-01  1.1532e-01 -4.9001e-01  6.1127e-02 -3.5021e-01  2.3784e-01\n",
      " -6.5555e-01  4.4000e-01  3.4830e-01  3.9215e-02  3.0473e-01 -2.6100e-01\n",
      "  5.3471e-02  1.2667e-04 -8.3094e-01 -9.6958e-02 -2.4066e-01  1.4338e-01\n",
      "  2.8247e-02 -8.9358e-04  5.7781e-01  2.2228e-01  9.8966e-02  3.6975e-01\n",
      " -1.3305e-01  8.4037e-03 -5.3555e-01  1.5261e-01  1.7584e-01 -2.6886e-01\n",
      " -2.8254e-02  2.1199e-01  3.5677e-01  1.4297e-01 -1.4316e-01 -6.9715e-02\n",
      "  6.7739e-02 -7.6595e-02 -5.2456e-01  2.1021e-02  5.5970e-01 -2.1186e-01\n",
      "  2.1273e-01  6.7540e-01  4.3156e-01 -4.9846e-01 -7.2166e-01  3.8983e-01\n",
      " -1.6478e-01  2.3858e-01 -8.7138e-02 -5.1316e-01  5.2550e-01  7.1671e-02\n",
      "  1.8321e-01  1.4223e-01  2.0777e-01 -2.8119e-02 -3.0350e-02  5.0865e-02\n",
      " -1.7405e-01  1.9915e-01  4.4877e-01  9.3559e-03  4.4912e-02  3.0753e-01\n",
      " -5.7825e-02 -1.7355e-01  5.4990e-01  4.4797e-01 -2.1394e-01 -2.3778e-01\n",
      "  3.2441e-02  4.7505e-01  6.8611e-01 -4.4709e-01 -2.8654e-01 -3.3109e-01\n",
      " -4.9554e-01  5.2162e-01  6.2363e-03 -3.6394e-01 -3.2736e-01  1.7030e-01\n",
      "  3.3882e-01  8.2687e-01  3.0033e-01  2.3082e-01 -4.4573e-03  2.8606e-01\n",
      " -2.0765e-01  3.1867e-01 -3.4659e-02  4.2973e-01 -2.1132e-01  1.9403e-01\n",
      " -4.4046e-02  1.9378e-01 -1.2732e-01 -7.3637e-03 -6.2693e-02  7.7524e-01\n",
      "  2.7827e-01 -1.3073e-01 -3.2932e-01  5.5361e-01  6.5890e-01 -2.3727e-01\n",
      " -4.3464e-01 -3.1004e-01 -4.1203e-01 -5.3018e-01  5.5248e-02 -2.1693e-01\n",
      " -1.9772e-01  6.5287e-01 -4.1722e-02 -2.2901e-01 -2.1404e-01  2.9469e-01\n",
      "  3.3406e-02 -5.1285e-01  3.3919e-01 -6.0759e-01 -5.8998e-01  2.1878e-02\n",
      " -5.0021e-01 -2.5041e-02  7.9773e-02  4.4393e-01  6.4513e-01  1.3715e-01\n",
      "  1.8965e-01 -1.9270e-01  1.5054e-01 -2.4111e-01  4.6385e-01 -4.1356e-01\n",
      " -2.1579e-01  4.9487e-02  5.9819e-01 -1.7362e-01  6.7087e-01  8.1567e-01\n",
      " -3.7523e-01  8.7300e-01 -5.8350e-02 -6.6227e-01 -1.3735e-01  2.4340e-01\n",
      "  6.8609e-01  3.1613e-01  1.7172e+00  3.9614e-02 -1.9355e-01 -5.0587e-01\n",
      "  2.7908e-01 -4.8653e-01 -4.0767e-01 -3.4555e-03  3.4252e-01  3.0706e-02\n",
      " -1.5453e-01  3.6956e-01 -6.1618e-03  2.7957e-01 -3.8621e-01 -5.6737e-01\n",
      " -5.6869e-01 -1.0343e+00  6.0047e-01 -4.2010e-01  2.4884e-01 -5.4358e-01\n",
      "  1.0383e-01  7.5234e-02 -6.6842e-02 -4.0863e-01  1.0254e-01 -2.3354e-02\n",
      " -5.1596e-01  1.6922e-01  4.8039e-02 -2.1806e-01 -1.1743e-01 -7.3767e-01\n",
      "  1.0277e+00 -6.6193e-02  6.0333e-01 -2.6168e-01 -3.3119e-01  5.3970e-02\n",
      "  2.4385e-01 -2.6787e-01  1.9097e-01  5.4283e-01 -1.0041e+00 -1.2646e-01\n",
      " -6.1514e-01 -3.3854e-02 -4.1974e-01  1.1377e-01 -7.7943e-01 -6.6869e-03\n",
      " -5.4013e-01  6.9636e-01 -8.8223e-01  3.6032e-02 -9.6489e-03 -2.3583e-01\n",
      "  2.5422e-01  7.9554e-02 -2.0473e-01 -2.0923e-01 -2.4261e-01  8.8337e-01\n",
      "  4.4107e-01  5.9088e-01 -1.4094e-01 -2.7148e-01  2.8171e-01 -2.6856e-01\n",
      "  1.1057e-01 -1.1918e-01  2.7769e-01  2.6091e-01  1.8910e-01  3.5853e-02\n",
      " -9.3099e-01  8.8160e-01 -4.8039e-01 -2.3596e-01 -5.4017e-01  4.5207e-01\n",
      "  2.8975e-01  6.8806e-01 -6.7061e-01 -2.6671e-01 -4.5635e-01 -1.2258e-01\n",
      " -5.6972e-02 -9.9968e-02 -5.4236e-01  8.6518e-02 -7.2980e-01 -1.1631e-01\n",
      "  3.3359e-01  6.3978e-01 -1.8178e-01 -4.7838e-01 -5.5030e-01  3.9678e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-1.3896e-01  1.2130e+00  2.4560e-01 -2.7516e-01  5.0725e-01  3.7306e-01\n",
      "  6.0455e-01  4.5422e-01 -3.2791e-01 -5.6665e-01  3.8336e-01 -6.8814e-01\n",
      " -1.3871e-01 -4.2354e-01  2.4258e-02 -2.1776e-01  2.0820e-01 -9.1329e-02\n",
      " -1.0846e-01 -4.5497e-01  4.2489e-01  2.1485e-01  2.5729e-01  1.0212e-01\n",
      "  3.3257e-01 -2.5691e-01  1.6149e-01 -2.1938e-01  4.1485e-01 -9.8947e-01\n",
      " -3.0017e-01 -3.3480e-01 -1.6191e-01  3.1174e-02 -1.5225e+00  1.4193e-01\n",
      " -6.4055e-02  2.9956e-01  2.9768e-01  7.6586e-02 -1.9656e-01 -3.4299e-02\n",
      " -5.3394e-01 -4.2036e-02 -3.0388e-01 -1.1457e-01  9.0635e-02 -6.1258e-01\n",
      "  1.6595e-01  1.1532e-01 -4.9001e-01  6.1127e-02 -3.5021e-01  2.3784e-01\n",
      " -6.5555e-01  4.4000e-01  3.4830e-01  3.9215e-02  3.0473e-01 -2.6100e-01\n",
      "  5.3471e-02  1.2667e-04 -8.3094e-01 -9.6958e-02 -2.4066e-01  1.4338e-01\n",
      "  2.8247e-02 -8.9358e-04  5.7781e-01  2.2228e-01  9.8966e-02  3.6975e-01\n",
      " -1.3305e-01  8.4037e-03 -5.3555e-01  1.5261e-01  1.7584e-01 -2.6886e-01\n",
      " -2.8254e-02  2.1199e-01  3.5677e-01  1.4297e-01 -1.4316e-01 -6.9715e-02\n",
      "  6.7739e-02 -7.6595e-02 -5.2456e-01  2.1021e-02  5.5970e-01 -2.1186e-01\n",
      "  2.1273e-01  6.7540e-01  4.3156e-01 -4.9846e-01 -7.2166e-01  3.8983e-01\n",
      " -1.6478e-01  2.3858e-01 -8.7138e-02 -5.1316e-01  5.2550e-01  7.1671e-02\n",
      "  1.8321e-01  1.4223e-01  2.0777e-01 -2.8119e-02 -3.0350e-02  5.0865e-02\n",
      " -1.7405e-01  1.9915e-01  4.4877e-01  9.3559e-03  4.4912e-02  3.0753e-01\n",
      " -5.7825e-02 -1.7355e-01  5.4990e-01  4.4797e-01 -2.1394e-01 -2.3778e-01\n",
      "  3.2441e-02  4.7505e-01  6.8611e-01 -4.4709e-01 -2.8654e-01 -3.3109e-01\n",
      " -4.9554e-01  5.2162e-01  6.2363e-03 -3.6394e-01 -3.2736e-01  1.7030e-01\n",
      "  3.3882e-01  8.2687e-01  3.0033e-01  2.3082e-01 -4.4573e-03  2.8606e-01\n",
      " -2.0765e-01  3.1867e-01 -3.4659e-02  4.2973e-01 -2.1132e-01  1.9403e-01\n",
      " -4.4046e-02  1.9378e-01 -1.2732e-01 -7.3637e-03 -6.2693e-02  7.7524e-01\n",
      "  2.7827e-01 -1.3073e-01 -3.2932e-01  5.5361e-01  6.5890e-01 -2.3727e-01\n",
      " -4.3464e-01 -3.1004e-01 -4.1203e-01 -5.3018e-01  5.5248e-02 -2.1693e-01\n",
      " -1.9772e-01  6.5287e-01 -4.1722e-02 -2.2901e-01 -2.1404e-01  2.9469e-01\n",
      "  3.3406e-02 -5.1285e-01  3.3919e-01 -6.0759e-01 -5.8998e-01  2.1878e-02\n",
      " -5.0021e-01 -2.5041e-02  7.9773e-02  4.4393e-01  6.4513e-01  1.3715e-01\n",
      "  1.8965e-01 -1.9270e-01  1.5054e-01 -2.4111e-01  4.6385e-01 -4.1356e-01\n",
      " -2.1579e-01  4.9487e-02  5.9819e-01 -1.7362e-01  6.7087e-01  8.1567e-01\n",
      " -3.7523e-01  8.7300e-01 -5.8350e-02 -6.6227e-01 -1.3735e-01  2.4340e-01\n",
      "  6.8609e-01  3.1613e-01  1.7172e+00  3.9614e-02 -1.9355e-01 -5.0587e-01\n",
      "  2.7908e-01 -4.8653e-01 -4.0767e-01 -3.4555e-03  3.4252e-01  3.0706e-02\n",
      " -1.5453e-01  3.6956e-01 -6.1618e-03  2.7957e-01 -3.8621e-01 -5.6737e-01\n",
      " -5.6869e-01 -1.0343e+00  6.0047e-01 -4.2010e-01  2.4884e-01 -5.4358e-01\n",
      "  1.0383e-01  7.5234e-02 -6.6842e-02 -4.0863e-01  1.0254e-01 -2.3354e-02\n",
      " -5.1596e-01  1.6922e-01  4.8039e-02 -2.1806e-01 -1.1743e-01 -7.3767e-01\n",
      "  1.0277e+00 -6.6193e-02  6.0333e-01 -2.6168e-01 -3.3119e-01  5.3970e-02\n",
      "  2.4385e-01 -2.6787e-01  1.9097e-01  5.4283e-01 -1.0041e+00 -1.2646e-01\n",
      " -6.1514e-01 -3.3854e-02 -4.1974e-01  1.1377e-01 -7.7943e-01 -6.6869e-03\n",
      " -5.4013e-01  6.9636e-01 -8.8223e-01  3.6032e-02 -9.6489e-03 -2.3583e-01\n",
      "  2.5422e-01  7.9554e-02 -2.0473e-01 -2.0923e-01 -2.4261e-01  8.8337e-01\n",
      "  4.4107e-01  5.9088e-01 -1.4094e-01 -2.7148e-01  2.8171e-01 -2.6856e-01\n",
      "  1.1057e-01 -1.1918e-01  2.7769e-01  2.6091e-01  1.8910e-01  3.5853e-02\n",
      " -9.3099e-01  8.8160e-01 -4.8039e-01 -2.3596e-01 -5.4017e-01  4.5207e-01\n",
      "  2.8975e-01  6.8806e-01 -6.7061e-01 -2.6671e-01 -4.5635e-01 -1.2258e-01\n",
      " -5.6972e-02 -9.9968e-02 -5.4236e-01  8.6518e-02 -7.2980e-01 -1.1631e-01\n",
      "  3.3359e-01  6.3978e-01 -1.8178e-01 -4.7838e-01 -5.5030e-01  3.9678e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.71 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 100000  # specify desired size of pre-defined embedding vocabulary \n",
    "\n",
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.300d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "# ------------------------------------------------------------- \n",
    "\n",
    "# Utility function for loading embeddings follows methods described in\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
    "# for the requested pre-trained word embeddings\n",
    "# \n",
    "# Note the use of defaultdict data structure from the Python Standard Library\n",
    "# collections_defaultdict.py lets the caller specify a default value up front\n",
    "# The default value will be retuned if the key is not a known dictionary key\n",
    "# That is, unknown words are represented by a vector of zeros\n",
    "# For word embeddings, this default value is a vector of zeros\n",
    "# Documentation for the Python standard library:\n",
    "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
    "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n",
    "\n",
    "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
    "\n",
    "# Additional background code from\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "\n",
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n",
    "\n",
    "# ------------------------------------------------------------- \n",
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding\n",
    "\n",
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# code for working with movie reviews data \n",
    "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
    "#    Upper Saddle River, N.J.: Pearson Education.\n",
    "#    ISBN-13: 978-0-13-388644-3\n",
    "# This original study used a simple bag-of-words approach\n",
    "# to sentiment analysis, along with pre-defined lists of\n",
    "# negative and positive words.        \n",
    "# Code available at:  https://github.com/mtpa/wnds       \n",
    "# ------------------------------------------------------------\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove\n",
    "\n",
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    \n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "\n",
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])\n",
    "\n",
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])\n",
    "\n",
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])        \n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "\n",
    "# --------------------------------------------------------------------------      \n",
    "# We use a very simple Recurrent Neural Network for this assignment\n",
    "# Géron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
    "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
    "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
    "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
    "#    Source code available at https://github.com/ageron/handson-ml\n",
    "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
    "#    See section on Training an sequence Classifier, # In [34]:\n",
    "#    which uses the MNIST case data...  we revise to accommodate\n",
    "#    the movie review data in this assignment    \n",
    "# --------------------------------------------------------------------------  \n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn5 (10k vocab size, glove.6B.50d.txt embeddings, increased batch size and epochs)\n",
    "## train: 1.0, test: 0.605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B/glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n",
      "Embedding is of shape: (400001, 50)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n",
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n",
      "\n",
      "Test sentence embeddings from vocabulary of 10000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n",
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-positive\n",
      "max_review_length: 1052\n",
      "min_review_length: 22\n",
      "First word in first document: while\n",
      "Embedding for this word:\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "First word in first document: officially\n",
      "Embedding for this word:\n",
      " [ 0.13682  -0.10324  -0.10126  -0.13996   0.080166 -0.18858  -0.96708\n",
      " -0.066722 -0.254    -0.61085   0.88298  -0.23186  -0.09482  -0.22099\n",
      "  0.85226   0.47223  -0.73086   0.054607 -0.22859   0.6526    0.05519\n",
      " -0.47021   0.35769   0.18049  -0.23699  -1.3029    0.14341   0.044548\n",
      " -0.70229   0.022042  2.3984   -0.46118  -0.88351  -0.5511   -0.25662\n",
      " -0.56969   1.1733   -0.077844 -0.96175  -0.30038  -0.58143  -0.8909\n",
      " -0.34433  -0.53421  -0.84671   0.03971  -1.0485   -0.12547  -0.072426\n",
      " -0.19364 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.13682  -0.10324  -0.10126  -0.13996   0.080166 -0.18858  -0.96708\n",
      " -0.066722 -0.254    -0.61085   0.88298  -0.23186  -0.09482  -0.22099\n",
      "  0.85226   0.47223  -0.73086   0.054607 -0.22859   0.6526    0.05519\n",
      " -0.47021   0.35769   0.18049  -0.23699  -1.3029    0.14341   0.044548\n",
      " -0.70229   0.022042  2.3984   -0.46118  -0.88351  -0.5511   -0.25662\n",
      " -0.56969   1.1733   -0.077844 -0.96175  -0.30038  -0.58143  -0.8909\n",
      " -0.34433  -0.53421  -0.84671   0.03971  -1.0485   -0.12547  -0.072426\n",
      " -0.19364 ]\n",
      "First word in first document: super\n",
      "Embedding for this word:\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n",
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.545 Test accuracy: 0.485\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.485 Test accuracy: 0.5\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.505 Test accuracy: 0.445\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.495 Test accuracy: 0.44\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.51 Test accuracy: 0.48\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.495 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.52 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.54 Test accuracy: 0.5\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.575 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.585 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.555\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.655 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.665 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.695 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.71 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.725 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.725 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.725 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.735 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.715 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.715 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.725 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.735 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  50  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  51  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  52  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  53  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  54  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  55  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.765 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  56  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  57  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.775 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  58  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.775 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  59  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  60  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.775 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  61  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  62  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  63  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.785 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  64  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.805 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  65  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.805 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  66  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.805 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  67  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  68  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  69  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  70  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  71  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  72  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  73  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  74  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  75  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  76  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  77  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  78  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  79  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.815 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  80  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  81  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  82  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  83  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.815 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  84  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  85  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  86  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  87  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  88  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  89  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  90  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  91  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  92  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  93  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  94  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  95  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  96  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.845 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  97  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  98  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.845 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  99  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  100  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  101  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.845 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  102  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.845 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  103  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  104  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.855 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  105  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.855 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  106  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.855 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  107  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.855 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  108  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  109  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.855 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  110  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.855 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  111  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.865 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  112  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.865 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  113  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  114  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  115  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.865 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  116  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  117  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.865 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  118  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.875 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  119  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.875 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  120  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.875 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  121  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.875 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  122  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  123  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  124  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  125  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  126  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  127  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  128  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  129  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.885 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  130  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.885 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  131  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.885 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  132  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  133  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.875 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  134  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  135  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  136  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.885 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  137  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  138  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  139  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.885 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  140  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.865 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  141  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.915 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  142  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.875 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  143  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  144  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  145  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.905 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  146  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.875 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  147  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  148  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.875 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  149  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.885 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  150  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  151  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  152  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  153  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  154  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.885 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  155  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.895 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  156  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  157  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.865 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  158  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.905 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  159  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.905 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  160  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  161  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  162  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.915 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  163  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  164  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  165  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.925 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  166  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.915 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  167  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  168  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.925 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  169  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.925 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  170  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.915 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  171  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  172  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  173  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  174  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.925 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  175  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  176  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  177  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  178  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  179  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  180  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  181  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  182  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  183  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  184  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.935 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  185  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.935 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  186  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  187  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  188  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  189  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.945 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  190  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.945 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  191  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.945 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  192  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.945 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  193  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.945 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  194  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.945 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  195  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.945 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  196  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  197  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  198  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  199  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  200  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  201  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  202  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  203  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  204  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  205  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  206  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  207  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  208  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  209  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  210  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  211  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  212  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  213  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  214  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  215  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  216  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  217  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  218  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  219  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  220  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  221  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  222  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  223  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  224  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  225  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  226  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  227  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.965 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  228  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  229  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  230  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  231  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  232  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  233  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  234  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  235  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  236  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  237  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  238  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  239  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  240  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  241  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  242  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  243  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  244  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  245  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  246  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.965 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  247  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  248  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  249  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  250  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.965 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  251  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  252  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.915 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  253  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  254  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.945 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  255  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  256  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  257  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  258  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.965 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  259  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  260  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  261  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  262  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  263  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  264  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  265  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  266  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  267  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  268  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  269  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  270  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  271  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  272  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  273  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  274  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  275  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  276  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  277  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  278  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  279  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  280  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  281  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  282  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  283  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  284  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  285  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  286  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  287  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  288  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  289  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  290  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  291  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  292  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  293  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  294  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  295  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  296  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  297  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  298  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  299  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  300  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  301  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  302  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  303  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  304  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  305  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  306  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  307  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  308  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  309  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  310  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  311  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  312  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  313  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  314  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  315  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  316  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  317  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  318  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  319  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  320  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  321  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  322  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  323  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  324  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  325  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  326  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  327  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  328  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  329  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  330  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  331  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  332  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  333  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  334  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  335  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  336  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  337  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  338  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  339  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  340  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  341  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  342  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  343  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  344  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  345  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  346  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  347  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  348  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  349  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  350  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  351  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  352  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  353  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  354  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  355  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  356  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  357  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  358  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  359  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  360  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  361  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  362  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  363  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  364  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  365  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  366  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  367  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  368  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  369  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  370  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  371  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  372  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  373  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  374  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  375  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  376  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  377  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  378  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  379  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  380  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  381  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  382  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  383  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  384  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  385  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  386  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  387  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  388  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  389  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  390  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  391  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  392  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  393  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.935 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  394  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  395  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  396  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  397  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  398  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  399  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.975 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  400  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  401  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  402  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  403  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  404  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  405  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  406  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  407  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  408  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.995 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  409  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  410  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  411  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  412  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  413  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  414  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  415  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  416  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  417  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  418  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  419  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  420  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  421  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  422  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  423  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  424  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  425  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  426  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  427  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  428  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  429  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  430  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  431  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  432  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  433  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  434  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  435  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  436  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  437  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  438  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  439  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  440  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  441  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  442  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  443  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  444  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  445  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  446  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  447  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  448  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  449  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  450  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  451  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  452  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  453  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  454  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  455  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  456  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  457  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  458  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  459  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  460  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  461  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  462  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  463  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  464  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  465  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  466  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  467  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  468  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  469  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  470  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  471  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  472  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  473  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  474  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  475  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  476  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  477  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  478  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  479  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  480  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  481  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  482  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  483  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  484  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  485  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  486  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  487  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  488  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  489  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  490  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  491  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  492  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  493  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  494  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  495  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  496  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  497  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  498  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  499  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.605\n"
     ]
    }
   ],
   "source": [
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary \n",
    "\n",
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "# ------------------------------------------------------------- \n",
    "\n",
    "# Utility function for loading embeddings follows methods described in\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
    "# for the requested pre-trained word embeddings\n",
    "# \n",
    "# Note the use of defaultdict data structure from the Python Standard Library\n",
    "# collections_defaultdict.py lets the caller specify a default value up front\n",
    "# The default value will be retuned if the key is not a known dictionary key\n",
    "# That is, unknown words are represented by a vector of zeros\n",
    "# For word embeddings, this default value is a vector of zeros\n",
    "# Documentation for the Python standard library:\n",
    "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
    "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n",
    "\n",
    "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
    "\n",
    "# Additional background code from\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "\n",
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n",
    "\n",
    "# ------------------------------------------------------------- \n",
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding\n",
    "\n",
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# code for working with movie reviews data \n",
    "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
    "#    Upper Saddle River, N.J.: Pearson Education.\n",
    "#    ISBN-13: 978-0-13-388644-3\n",
    "# This original study used a simple bag-of-words approach\n",
    "# to sentiment analysis, along with pre-defined lists of\n",
    "# negative and positive words.        \n",
    "# Code available at:  https://github.com/mtpa/wnds       \n",
    "# ------------------------------------------------------------\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove\n",
    "\n",
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    \n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "\n",
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])\n",
    "\n",
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])\n",
    "\n",
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])        \n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "\n",
    "# --------------------------------------------------------------------------      \n",
    "# We use a very simple Recurrent Neural Network for this assignment\n",
    "# Géron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
    "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
    "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
    "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
    "#    Source code available at https://github.com/ageron/handson-ml\n",
    "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
    "#    See section on Training an sequence Classifier, # In [34]:\n",
    "#    which uses the MNIST case data...  we revise to accommodate\n",
    "#    the movie review data in this assignment    \n",
    "# --------------------------------------------------------------------------  \n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 500\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outcome and results\n",
    "I ran 5 different model scenarios differentiating them by changing either the vocab size or word vector embedding file used. I used vocab sizes of either 10k or 100k to - I wanted to ensure I drastically differed them to understand the impact changing it has and used the word vectors from the glove folder of the 50 dimensions or 300 dimensions embeddings. What I found was that all in all the results were all pretty similar to each other. Changing these settings did not have huge impacts. The best overall result I got was the 10k vocab size and 50 dimension glove embeddings. This returned a test accuracy of 67.5% with a train accuracy of about 86.0%. So, slight overfitting but all-in-all the best result from the 5 models I ran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recommendation\n",
    "One of the biggest takeaways from this assignment is that these models are very prone to overfitting when trained on a large vocab and a high dimensional embedding vector. Also, in this exercise there is some definite parameter tuning that needs to be done to really optimize these models. If we use 50% as our baseline, any model should do substantially better than that in terms of accruacy to really be worth anything. \n",
    "\n",
    "Things that are definitely needed are broad and diverse vocabulary to ensure RNNs have the tools they need to learn the sentiment from reviews. To ensure language models are useful this needs to be done prior to running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
