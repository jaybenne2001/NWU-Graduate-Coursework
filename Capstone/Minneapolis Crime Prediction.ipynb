{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting Minneapolis Crimes\n",
    "\n",
    "Michael Haugan <br>\n",
    "Northwestern MSDS <br>\n",
    "Spring Capstone (2020) <br>\n",
    "\n",
    "I am using historical crime data from the city of Minneapolis (http://opendata.minneapolismn.gov/) as well as connecting other various external sources to see if I can predict crimes that will occur within the city.\n",
    "\n",
    "Weather: https://openweathermap.org/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import calendar\n",
    "import rtree\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from functools import reduce"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### police crime data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load in crime dataset for one year\n",
    "crime19 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2019.csv')\n",
    "crime182 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2018_PIMS.csv')\n",
    "crime181 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2018.csv')\n",
    "crime17 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2017.csv')\n",
    "crime16 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2016.csv')\n",
    "crime15 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2015.csv')\n",
    "crime14 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2014.csv')\n",
    "crime13 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2013.csv')\n",
    "crime12 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2012.csv')\n",
    "crime11 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2011.csv')\n",
    "crime10 = pd.read_csv('Mnpls_Data/Mnpls_Police_data/Police_Incidents_2010.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# view size\n",
    "print(crime19.shape)\n",
    "print(crime182.shape)\n",
    "print(crime181.shape)\n",
    "print(crime17.shape)\n",
    "print(crime16.shape)\n",
    "print(crime15.shape)\n",
    "print(crime14.shape)\n",
    "print(crime13.shape)\n",
    "print(crime12.shape)\n",
    "print(crime11.shape)\n",
    "print(crime10.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### clean column names"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# change columns names to all match\n",
    "crime19 = crime19.rename(columns={\"centerLat\": \"Lat\", \"centerLong\": \"Long\", \"description\": \"Description\", \n",
    "                                  \"offense\": \"Offense\", \"precinct\": \"Precinct\", \n",
    "                                  \"reportedDateTime\": \"ReportedDateTime\", \"neighborhood\": \"Neighborhood\"})\n",
    "crime182 = crime182.rename(columns={\"centerLat\": \"Lat\", \"centerLong\": \"Long\", \"description\": \"Description\", \n",
    "                                  \"offense\": \"Offense\", \"precinct\": \"Precinct\", \n",
    "                                    \"reportedDateTime\": \"ReportedDateTime\", \"neighborhood\": \"Neighborhood\"})\n",
    "crime181 = crime181.rename(columns={\"ReportedDate\": \"ReportedDateTime\"})\n",
    "crime17 = crime17.rename(columns={\"ReportedDate\": \"ReportedDateTime\"})\n",
    "crime16 = crime16.rename(columns={\"ReportedDate\": \"ReportedDateTime\"})\n",
    "crime15 = crime15.rename(columns={\"ReportedDate\": \"ReportedDateTime\"})\n",
    "crime14 = crime14.rename(columns={\"ReportedDate\": \"ReportedDateTime\"})\n",
    "crime13 = crime13.rename(columns={\"ReportedDate\": \"ReportedDateTime\"})\n",
    "crime12 = crime12.rename(columns={\"ReportedDate\": \"ReportedDateTime\"})\n",
    "crime11 = crime11.rename(columns={\"ReportedDate\": \"ReportedDateTime\"})\n",
    "crime10 = crime10.rename(columns={\"ReportedDate\": \"ReportedDateTime\"})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### subset dataframe to only needed columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# keep only necessary columns from df's\n",
    "c19 = crime19[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c182 = crime182[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c181 = crime181[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c17 = crime17[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c16 = crime16[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c15 = crime15[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c14 = crime14[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c13 = crime13[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c12 = crime12[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c11 = crime11[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]\n",
    "c10 = crime10[['ReportedDateTime', 'Lat', 'Long', 'Neighborhood', 'Precinct', 'Offense', 'Description']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### concatenate together, sort and clean"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# concatenate df's together and \n",
    "frames = [c19, c182, c181, c17, c16, c15, c14, c13, c12, c11, c10]\n",
    "df = pd.concat(frames)\n",
    "print(df.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sort by date\n",
    "df = df.sort_values(['ReportedDateTime'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# change timestamp to date type\n",
    "df['ReportedDateTime'] = pd.to_datetime(df['ReportedDateTime'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# strip date and time and hour from timestamp\n",
    "df['Time'] = df['ReportedDateTime'].apply( lambda d : d.time() )\n",
    "df['Date'] = df['ReportedDateTime'].apply( lambda d : d.date() )\n",
    "df['Year'] = df['ReportedDateTime'].apply( lambda d : d.year)\n",
    "df['Month'] = df['ReportedDateTime'].apply( lambda d : d.month)\n",
    "df['Month_Name'] = df['ReportedDateTime'].apply( lambda d : d.strftime(\"%B\"))\n",
    "df['DoW_Num'] = df['ReportedDateTime'].apply( lambda d : d.dayofweek)\n",
    "df['DoW'] = df['ReportedDateTime'].dt.day_name()\n",
    "df['Hour'] = df['ReportedDateTime'].apply( lambda d : d.hour)\n",
    "\n",
    "# make date a date char type\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# remove whitespace from Offense and Precinct\n",
    "df['Offense'] = df['Offense'].str.strip()\n",
    "\n",
    "# change AUTOTH to GTA to make more clear\n",
    "df['Offense']=df['Offense'].apply(lambda x:(x.replace('AUTOTH','GTA')))\n",
    "# combine other autothefts into one as they are all similar\n",
    "df['Offense']=df['Offense'].apply(lambda x:(x.replace('MVTHFT', 'TFMV').replace('TMVP', 'TFMV')))\n",
    "\n",
    "# remove punctuation from neighborhood column\n",
    "df['Neighborhood'] = df['Neighborhood'].str.replace(r\"[\\\"\\',]\", '')\n",
    "\n",
    "# remove na's (exist in precinct and neighborhood)\n",
    "df = df.dropna()\n",
    "\n",
    "# remove UI, 18 and 99 from precinct\n",
    "df = df[df.Precinct != 'UI    ']\n",
    "df = df[df.Precinct != 18]\n",
    "df = df[df.Precinct != 99]\n",
    "\n",
    "# convert precinct column to int\n",
    "df['Precinct'] = df['Precinct'].astype(int)\n",
    "\n",
    "# convert neighborhood to upper\n",
    "df['Neighborhood'] = df['Neighborhood'].str.upper()\n",
    "\n",
    "df.tail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### weather data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load in weather data\n",
    "weather = pd.read_csv('Mnpls_Data/Mnpls_weather_data/Mnpls_historical_weather.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# change timestamp to date type\n",
    "weather['dt_iso_new'] = pd.to_datetime(weather['dt_iso_new'])\n",
    "\n",
    "# strip date and time and hour from timestamp\n",
    "weather['Time'] = weather['dt_iso_new'].apply( lambda d : d.time() )\n",
    "weather['Date'] = weather['dt_iso_new'].apply( lambda d : d.date() )\n",
    "weather['Hour'] = weather['dt_iso_new'].apply( lambda d : d.hour)\n",
    "\n",
    "# make date a date char type\n",
    "weather['Date'] = pd.to_datetime(weather['Date'])\n",
    "weather.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# split weather data from 2010 through 2020\n",
    "split_date ='2010-01-01'\n",
    "weather_df = weather.loc[weather['Date'] >= split_date]\n",
    "\n",
    "# concatenate date and hour together to form join key weather data\n",
    "weather_df['Date'] = weather_df['Date'].astype(str)\n",
    "weather_df['Hour'] = weather_df['Hour'].astype(str)\n",
    "weather_df['date_join'] = weather_df[['Date', 'Hour']].apply(lambda x: ''.join(x), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### crime by day over time (10 yrs)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# group crimes by date and plot\n",
    "import seaborn as sns\n",
    "\n",
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7)})\n",
    "\n",
    "# plot\n",
    "df_date = df.groupby('Date').count()\n",
    "df_date.reset_index(inplace=True)\n",
    "s = sns.lineplot(x='Date', y='Offense', data=df_date)\n",
    "s.set_xlabel('Date', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Crime Count by Day', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is definitely some seasonality involved with crime. We will see this better when we look at crime by temperature, but an educated guess is that crime increases as the weather gets warmer out (this makes a huge difference in Minnesota!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### crime by day of week"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# group crimes by day of week and plot\n",
    "\n",
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7)})\n",
    "\n",
    "# plot\n",
    "s = sns.countplot(x=\"DoW\", order=['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday'], data=df)\n",
    "s.set_xlabel('Day of Week', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Crime Count by Day of Week', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Somewhat suprisingly (I guess?) Monday has the highest total crime, followed by Tuesday and then Friday."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### crime by hour of day"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# group crimes by time of day and plot\n",
    "import seaborn as sns\n",
    "\n",
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7), \"lines.linewidth\": 5})\n",
    "\n",
    "# plot\n",
    "df_time = df.groupby('Hour').count()\n",
    "df_time.reset_index(inplace=True)\n",
    "s = sns.lineplot(x='Hour', y='Offense', data=df_time)\n",
    "s.set_xlabel('Hour', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Crime Count by Hour of Day', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Crime spikes the highest between the hours of 7am through 3pm and then slowly tapers. It then spikes slightly again around 8pm and 10pm. Crime is lowest between the very early morning hours of 12am through 6am."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### crime by type"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# group crimes by type of crime and plot\n",
    "\n",
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7)})\n",
    "\n",
    "# plot\n",
    "s = sns.countplot(x='Offense', order=df['Offense'].value_counts().index, data=df)\n",
    "s.set_xlabel('Offense', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Crime by Type', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Theft (other), theft from motor vehicles, burglary and grand theft auto are far and away the most common crime types in Minneapolis (apparently a lot of kleptomaniacs)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# look into crime type by day\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# df.groupby(['DoW', 'Offense']).count()\n",
    "\n",
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7)})\n",
    "\n",
    "# plot\n",
    "s = sns.countplot(x='DoW', order=['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday'], data=df, hue='Offense')\n",
    "s.set_xlabel('DoW', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Crime Type by DoW and Offense', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### neighborhood breakdown"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# group crimes by neighborhood and plot to show highest crime neighborhoods\n",
    "\n",
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7)})\n",
    "\n",
    "# plot\n",
    "s = sns.countplot(x='Neighborhood', data=df, order=df.Neighborhood.value_counts().iloc[:15].index)\n",
    "s.set_xlabel('Neighborhood', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Top 15 Neighborhoods by Crime Count', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downtown west is far and away the least safe neighborhood followed by Whittier and Marcy Holmes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# group crimes by neighborhood and plot to show lowest crime neighborhoods\n",
    "\n",
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7)})\n",
    "\n",
    "# plot\n",
    "s = sns.countplot(x='Neighborhood', data=df, order=df.Neighborhood.value_counts().iloc[-15:].index)\n",
    "s.set_xlabel('Neighborhood', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Bottom 15 Neighborhoods by Crime Count', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are our safest neighborhood's by crime counts."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### precinct breakdown"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot precincts with precinct numbers\n",
    "precinct = gpd.read_file('Mnpls_Data/Mnpls_Geo_data/Minneapolis_Police_Precincts.shp')\n",
    "precinct[\"center\"] = precinct[\"geometry\"].centroid\n",
    "precinct_copy = precinct.copy()\n",
    "precinct_copy.set_geometry(\"center\", inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import adjustText as aT\n",
    "\n",
    "ax = precinct.plot(figsize = (8, 8))\n",
    "texts = []\n",
    "\n",
    "for x, y, label in zip(precinct_copy.geometry.x, precinct_copy.geometry.y, precinct_copy[\"PRECINCT\"]):\n",
    "    texts.append(plt.text(x, y, label, fontsize = 16))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# group crimes by precinct and plot\n",
    "\n",
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7)})\n",
    "\n",
    "# plot\n",
    "s = sns.countplot(x='Precinct', data=df)\n",
    "s.set_xlabel('Precinct', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Crime by Precinct', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7)})\n",
    "\n",
    "# plot\n",
    "s = sns.countplot(x='Offense', order=df['Offense'].value_counts().index, data=df, hue='Precinct')\n",
    "s.set_xlabel('Offense', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Crime Type by Precinct', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Despite not having the highest overall crime precinct 1 (downtown Minneapolis) far and away has the most thefts. While precinct 3 has the most thefts of a motor vehicle and burglaries. That is not suprising as precinct 3 is far more residential than precinct 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### weather effect"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# concatenate date and hour together to form join key for crime data\n",
    "df['Date'] = df['Date'].astype(str)\n",
    "df['Hour'] = df['Hour'].astype(str)\n",
    "df['date_join'] = df[['Date', 'Hour']].apply(lambda x: ''.join(x), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# look at correlation of weather "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# join crime and weather data\n",
    "df_new = df.merge(weather_df, on='date_join', how='left')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_new.head(n=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot crime count by temp\n",
    "# set plot specs\n",
    "sns.set(rc={'figure.figsize':(15, 7)})\n",
    "\n",
    "# plot\n",
    "df_temp = df_new.groupby('temp').count()\n",
    "df_temp.reset_index(inplace=True)\n",
    "s = sns.lineplot(x='temp', y='Offense', data=df_temp)\n",
    "s.set_xlabel('Temp', fontsize=20)\n",
    "s.set_ylabel('Count', fontsize=20)\n",
    "s.axes.set_title('Crime Count by Temp', fontsize=20)\n",
    "s.tick_params(labelsize=11)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected crime goes down when temperature is reading the extremes either very cold or very hot. I was expecting more of a normal bell shaped curve here but interestingly enough there seem to be two peaks. One around 35 degrees or so and another around 70 degrees."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### geo plotting by neighborhood"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot neighborhoods with geopandas\n",
    "import descartes\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hoods = gpd.read_file('Mnpls_Data/Mnpls_Geo_data/Minneapolis_Neighborhoods.shp')\n",
    "\n",
    "# change column header of BD_NAME to Neighborhood and make neighborhoods uppercase\n",
    "hoods = hoods.rename(columns={\"BDNAME\": \"Neighborhood\"})\n",
    "hoods['Neighborhood'] = hoods['Neighborhood'].str.upper()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hoods.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# turn normal df into geo df\n",
    "# crs = {'init': 'epsg:4326'}\n",
    "# geometry = [Point(xy) for xy in zip(df_new['Long'], df_new['Lat'])]\n",
    "# geo_df = gpd.GeoDataFrame(df_new, crs=crs, geometry=geometry)\n",
    "# geo_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# first group crime count by neighborhood\n",
    "crime_grouped = df_new.groupby('Neighborhood').count()\n",
    "\n",
    "# subset down to just 1 column needed\n",
    "df_sub = crime_grouped[['Offense']]\n",
    "\n",
    "# rename column\n",
    "df_sub = df_sub.rename(columns={\"Offense\": \"Crime_Count\"})\n",
    "\n",
    "# reset index\n",
    "df_sub = df_sub.reset_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# join the geodataframe with the cleaned up csv dataframe\n",
    "merged_geo = hoods.set_index('Neighborhood').join(df_sub.set_index('Neighborhood'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot\n",
    "\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = 15000, 30000\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "\n",
    "# create map\n",
    "merged_geo.plot('Crime_Count', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8')\n",
    "\n",
    "# remove the axis\n",
    "ax.axis('off')\n",
    "\n",
    "# add a title\n",
    "ax.set_title('Crime Count by Neighborhood in Minneapolis', fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "# Create colorbar as a legend\n",
    "sm = plt.cm.ScalarMappable(cmap='Blues', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "sm._A = []\n",
    "cbar = fig.colorbar(sm)\n",
    "\n",
    "# find centroids\n",
    "merged_geo[\"center\"] = merged_geo[\"geometry\"].centroid\n",
    "merged_geo_copy = merged_geo.copy()\n",
    "merged_geo_copy.set_geometry(\"center\", inplace = True)\n",
    "\n",
    "# this will save the figure as a high-res png. you can also save as svg\n",
    "fig.savefig('Mnpls_Crime_by_hood.png', dpi=150)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### plotting heatmaps with gmaps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gmaps\n",
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyC0vnk7385Yf14nC9STkbu_0bI6iIQDzQQ' #me!\n",
    "gmaps.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os \n",
    "# Get the value of \n",
    "# 'HOME' environment variable \n",
    "home = os.environ['GOOGLE_API_KEY'] \n",
    "  \n",
    "# Print the value of \n",
    "# 'HOME' environment variable \n",
    "print(\"GOOGLE_API_KEY:\", home)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define df\n",
    "theft = df_new[df_new['Offense'] == 'THEFT']\n",
    "\n",
    "# plot\n",
    "mnpls = (44.9778, -93.2650)\n",
    "fig = gmaps.figure(center=mnpls, zoom_level=14)\n",
    "heatmap_layer = gmaps.heatmap_layer(theft[['Lat', 'Long']], max_intensity=100, point_radius=25)\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# model build"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine crimes into violent crime or not\n",
    "\n",
    "df_new['violent_crime'] = np.where(df_new['Offense'].isin(['ARSON', 'ASLT1', 'ASLT2', 'ASLT3', 'ASLT4', 'CSCR', 'DASLT1'\n",
    "                                                                     'DASLT2', 'DASLT3', 'DASTR', 'MURDR', 'ROBPAG', \n",
    "                                                                      'ROBPER', 'ROBBIZ', 'TFPER', 'JHOMIC']), 1, 0)\n",
    "\n",
    "df_new['other_crime'] = np.where(~df_new['Offense'].isin(['ARSON', 'ASLT1', 'ASLT2', 'ASLT3', 'ASLT4', 'CSCR', 'DASLT1'\n",
    "                                                                     'DASLT2', 'DASLT3', 'DASTR', 'MURDR', 'ROBPAG', \n",
    "                                                                      'ROBPER', 'ROBBIZ', 'TFPER', 'JHOMIC']), 1, 0)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_new.head(n=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# subset df down to necessary columns\n",
    "model_df = df_new.drop(['Time_x', 'Date_x', 'Year', 'DoW', 'Month_Name', 'ReportedDateTime', 'Neighborhood', 'Precinct', 'Offense', 'Description', 'date_join', 'dt', 'dt_iso',\n",
    "            'dt_iso_new', 'timezone', 'city_name', 'lat', 'lon', 'feels_like', 'temp_min', 'temp_max',\n",
    "           'pressure', 'sea_level', 'grnd_level', 'wind_deg', 'rain_3h', 'rain_6h', 'rain_12h', 'rain_24h',\n",
    "           'rain_today', 'snow_3h', 'snow_6h', 'snow_12h', 'snow_24h', 'snow_today', 'clouds_all', 'weather_id',\n",
    "           'weather_main', 'weather_description', 'weather_icon', 'Time_y', 'Date_y', 'Hour_y'], axis=1)\n",
    "\n",
    "model_df['Hour_x'] = model_df['Hour_x'].astype(int)\n",
    "\n",
    "# model_df['Month_Name'] = calendar.month_name[model_df['Month']]\n",
    "# model_df['Month_Name'] = model_df['Month'].apply( lambda d : d.calendar.month_name() )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# deal with nas\n",
    "\n",
    "# count nas\n",
    "# model_df.isna().sum()\n",
    "\n",
    "# fill nas in cols\n",
    "model_df = model_df.fillna(0)\n",
    "\n",
    "model_df.tail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# look at correlations\n",
    "# correlation heat map setup for seaborn\n",
    "def corr_chart(df_corr):\n",
    "    corr=df_corr.corr()\n",
    "    #screen top half to get a triangle\n",
    "    top = np.zeros_like(corr, dtype=np.bool)\n",
    "    top[np.triu_indices_from(top)] = True\n",
    "    fig=plt.figure()\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    sns.heatmap(corr, mask=top, cmap='RdYlGn', \n",
    "        center = 0, square=True, \n",
    "        linewidths=.5, cbar_kws={'shrink':.5}, \n",
    "        annot = True, annot_kws={'size': 9}, fmt = '.3f')           \n",
    "    plt.xticks(rotation=45) # rotate variable labels on columns (x axis)\n",
    "    plt.yticks(rotation=0) # use horizontal variable labels on rows (y axis)\n",
    "    plt.title('Correlation Heat Map')   \n",
    "    plt.savefig('plot-corr-map.pdf', \n",
    "        bbox_inches = 'tight', dpi=None, facecolor='w', edgecolor='b', \n",
    "        orientation='portrait', papertype=None, format=None, \n",
    "        transparent=True, pad_inches=0.25, frameon=None)      \n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "corr_chart(df_corr = model_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# look at violent crime count\n",
    "model_df['violent_crime'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "38246/(38246+184698)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "about 17.2% of the crimes are considered violent crimes based on my categorization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cyclical feature engineering\n",
    "# model_df_cyclical = model_df.copy()\n",
    "# model_df_cyclical['hr_sin'] = np.sin(model_df_cyclical.Hour_x*(2.*np.pi/24))\n",
    "# model_df_cyclical['hr_cos'] = np.cos(model_df_cyclical.Hour_x*(2.*np.pi/24))\n",
    "# model_df_cyclical['day_sin'] = np.sin((model_df_cyclical.Day-1)*(2.*np.pi/12))\n",
    "# model_df_cyclical['day_cos'] = np.cos((model_df_cyclical.Day-1)*(2.*np.pi/12))\n",
    "# model_df_cyclical['mnth_sin'] = np.sin((model_df_cyclical.Month-1)*(2.*np.pi/12))\n",
    "# model_df_cyclical['mnth_cos'] = np.cos((model_df_cyclical.Month-1)*(2.*np.pi/12))\n",
    "model_df_cyclical = model_df.copy()\n",
    "model_df_cyclical['hr_sin'] = np.sin(model_df_cyclical.Hour_x*(2.*np.pi/24))\n",
    "model_df_cyclical['hr_cos'] = np.cos(model_df_cyclical.Hour_x*(2.*np.pi/24))\n",
    "model_df_cyclical['day_sin'] = np.sin((model_df_cyclical.DoW_Num-1)*(2.*np.pi/12))\n",
    "model_df_cyclical['day_cos'] = np.cos((model_df_cyclical.Dow_Num-1)*(2.*np.pi/12))\n",
    "model_df_cyclical['mnth_sin'] = np.sin((model_df_cyclical.Month-1)*(2.*np.pi/12))\n",
    "model_df_cyclical['mnth_cos'] = np.cos((model_df_cyclical.Month-1)*(2.*np.pi/12))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_df_cyclical.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# drop original cylical vars\n",
    "model_df_cyclical = model_df_cyclical.drop(['Month', 'Hour_x', 'Day'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train test split\n",
    "X = model_df_cyclical.drop(\"violent_crime\", axis = 1)\n",
    "y = model_df_cyclical[\"violent_crime\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run smote to balance dataset\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## grid search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set up gridsearch params\n",
    "rfc_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [20, 50],\n",
    "    'min_samples_leaf': [10, 20],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run gridsearch\n",
    "rfc = RandomForestClassifier(max_features='sqrt', bootstrap=True, random_state=23, n_jobs=-1)\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=rfc_grid, cv=5)\n",
    "CV_rfc.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# find best params\n",
    "CV_rfc.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## run random forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rfc = RandomForestClassifier(max_features='sqrt', bootstrap=True, max_depth=20, min_samples_leaf=10, \n",
    "                             min_samples_split=20, n_estimators=200, random_state=23, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# make predictions\n",
    "rfc_pred = rfc.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check accuracy scores\n",
    "print(\"Training set score: {:.4f}\".format(rfc.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(rfc.score(X_test, y_test)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "precision_score(y_test, rfc_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "recall_score(y_test, rfc_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f1_score(y_test, rfc_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# look at classification report\n",
    "print(classification_report(y_test, rfc_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extremely low recall for predicting crime which means we have a lot of false negatives happening (not classifying enough violent crime instances)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## feature importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# feature importance\n",
    "feat = pd.DataFrame({'importance':rfc.feature_importances_})\n",
    "feat['feature'] = X.columns\n",
    "feat.sort_values(by='importance', ascending=False, inplace=True)\n",
    "sns.barplot(data = feat, y = \"feature\", x = \"importance\", color = \"b\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# categorical encoding cyclical vars"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_df['Month'] = model_df['Month'].astype(str)\n",
    "model_df['Hour_x'] = model_df['Hour_x'].astype(str)\n",
    "model_df['Day'] = model_df['Day'].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_df = pd.get_dummies(model_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train test split\n",
    "X = model_df.drop(\"violent_crime\", axis = 1)\n",
    "y = model_df[\"violent_crime\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## run random forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run random forest using same params from first gridsearch\n",
    "rfc2 = RandomForestClassifier(max_features='sqrt', bootstrap=True, max_depth=20, min_samples_leaf=10, \n",
    "                             min_samples_split=20, n_estimators=200, random_state=23, n_jobs=-1)\n",
    "rfc2.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# make predictions\n",
    "rfc_pred2 = rfc2.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check accuracy scores\n",
    "print(\"Training set score: {:.4f}\".format(rfc2.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(rfc2.score(X_test, y_test)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# feature importance\n",
    "feat = pd.DataFrame({'importance':rfc2.feature_importances_})\n",
    "feat['feature'] = X.columns\n",
    "feat.sort_values(by='importance', ascending=False, inplace=True)\n",
    "sns.barplot(data = feat, y = \"feature\", x = \"importance\", color = \"b\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# view classification report\n",
    "print(classification_report(y_test, rfc_pred2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# view confusion matrix\n",
    "print(confusion_matrix(y_test, rfc_pred2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same issue exists here as we see practically the same results and very low recall for predicting violent crime."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## balance dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# change to string var\n",
    "model_df['Month'] = model_df['Month'].astype(str)\n",
    "model_df['Hour_x'] = model_df['Hour_x'].astype(str)\n",
    "model_df['Day'] = model_df['Day'].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create dummy vars\n",
    "model_df = pd.get_dummies(model_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train test split\n",
    "X = model_df.drop(\"violent_crime\", axis = 1)\n",
    "y = model_df[\"violent_crime\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# view class inbalance\n",
    "y_train.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run smote to balance dataset\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# view class inbalance after running smote\n",
    "y_train.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run random forest using same params from first gridsearch\n",
    "rfc3 = RandomForestClassifier(max_features='sqrt', bootstrap=True, max_depth=20, min_samples_leaf=10, \n",
    "                             min_samples_split=20, n_estimators=200, random_state=23, n_jobs=-1)\n",
    "rfc3.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# make predictions\n",
    "rfc_pred3 = rfc3.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check accuracy scores\n",
    "print(\"Training set score: {:.4f}\".format(rfc3.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(rfc3.score(X_test, y_test)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# feature importance\n",
    "feat = pd.DataFrame({'importance':rfc3.feature_importances_})\n",
    "feat['feature'] = X.columns\n",
    "feat.sort_values(by='importance', ascending=False, inplace=True)\n",
    "sns.barplot(data = feat, y = \"feature\", x = \"importance\", color = \"b\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# view classification report\n",
    "print(classification_report(y_test, rfc_pred3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# view confusion matrix\n",
    "print(confusion_matrix(y_test, rfc_pred3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## make into grid"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyproj import CRS\n",
    "crs = CRS.from_epsg(4326)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import numpy as np\n",
    "points = gpd.read_file('Mnpls_Data/Mnpls_Geo_data/Minneapolis_Boundary.shp',crs = crs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "points.crs = {'init': 'epsg:4326', 'no_defs': True}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get min/max coordinates\n",
    "xmin,ymin,xmax,ymax =  points.total_bounds\n",
    "\n",
    "# figure out the size of square. I wanted even squares, which is why I went with 20x20, but it can be anything.\n",
    "y_length = (ymax-ymin)/20 \n",
    "print(y_length)\n",
    "\n",
    "x_width = (xmax-xmin)/20\n",
    "print(x_width)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "crs = CRS.from_epsg(4326)\n",
    "\n",
    "height = y_length\n",
    "width = x_width\n",
    "\n",
    "rows = int(np.ceil((ymax-ymin) /  height))\n",
    "cols = int(np.ceil((xmax-xmin) / width))\n",
    "\n",
    "x_left = xmin\n",
    "x_right = xmin + width\n",
    "y_top = ymax\n",
    "y_bottom = ymax- height\n",
    "\n",
    "polygons = []\n",
    "for i in range(cols):\n",
    "    ytop = y_top\n",
    "    ybottom = y_bottom\n",
    "    for j in range(rows):\n",
    "        polygons.append(Polygon([(x_left, ytop), (x_right, ytop), (x_right, ybottom), (x_left, ybottom)])) \n",
    "        ytop = ytop - height\n",
    "        ybottom = ybottom - height\n",
    "    x_left = x_left + width\n",
    "    x_right = x_right + width\n",
    "\n",
    "\n",
    "grid = gpd.GeoDataFrame({'geometry':polygons},crs = crs)\n",
    "grid.to_file(\"grid_mn.shp\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "base = points.boundary.plot(color='red',figsize=(20, 15))\n",
    "grid.boundary.plot(ax=base,color = 'blue',figsize=(20, 15));"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# trim the grid to to match the city borders\n",
    "grid_trimmed = gpd.overlay(grid, points, how='intersection')\n",
    "grid_trimmed.boundary.plot(color='blue', figsize=(20, 15))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check number of groupings\n",
    "grid_trimmed.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create geo points for each lat/long coordinate\n",
    "crime_locations = [Point(xy) for xy in zip(df_new.Long, df_new.Lat)]\n",
    "df_new['geometry'] = crime_locations\n",
    "#data_loc_copy.drop(['lat','long'], axis = 1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert dataframe to a geo-dataframe\n",
    "data_gpd = gpd.GeoDataFrame(df_new, crs=crs, geometry=crime_locations)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# map each crime lat/long points to the index of the polygon that they fall within\n",
    "from geopandas.tools import sjoin\n",
    "data_final = sjoin(data_gpd, grid_trimmed, how='left',op=\"within\")\n",
    "data_final = data_final.rename(columns={'index_right': 'mapped_region'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check if mappings worked\n",
    "subset_gpd = data_final.loc[(data_final['mapped_region'] == 222),:]\n",
    "\n",
    "# plot all points that were tagged within index 1 polygon \n",
    "base = grid_trimmed.boundary.plot(color='blue',figsize=(20, 15))\n",
    "subset_gpd.geometry.plot(ax=base,color='red',figsize=(20, 15),markersize=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_final.head()\n",
    "data_final.sort_values(by=['date_join'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# subset df down to necessary columns\n",
    "grid_df = data_final.drop(['Lat', 'Long', 'Time_x', 'DoW', 'Month_Name', 'ReportedDateTime', 'Neighborhood', \n",
    "                           'Precinct', 'Offense', 'Description', 'dt', 'dt_iso','dt_iso_new', \n",
    "                           'timezone', 'city_name', 'lat', 'lon', 'temp_min', 'temp_max','pressure', 'sea_level', \n",
    "                           'grnd_level', 'wind_deg','rain_today', 'snow_today', 'clouds_all', 'weather_id',\n",
    "                           'weather_main', 'weather_description', 'weather_icon', 'Time_y', 'Date_y', 'Hour_y',\n",
    "                          'geometry', 'OBJECTID', 'NAME', 'Shape__Are', 'Shape__Len'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create part of day variable\n",
    "\n",
    "grid_df['Hour_x'] = grid_df['Hour_x'].astype('int64')\n",
    "\n",
    "conditions = [\n",
    "    (grid_df['Hour_x'] >= 0) & (grid_df['Hour_x'] <= 3),\n",
    "    (grid_df['Hour_x'] >= 4) & (grid_df['Hour_x'] <= 6),\n",
    "    (grid_df['Hour_x'] >= 7) & (grid_df['Hour_x'] <= 10),\n",
    "    (grid_df['Hour_x'] >= 11) & (grid_df['Hour_x'] <= 15),\n",
    "    (grid_df['Hour_x'] >= 16) & (grid_df['Hour_x'] <= 21),\n",
    "    (grid_df['Hour_x'] >= 22) & (grid_df['Hour_x'] <= 23)]\n",
    "choices = ['late_night', 'early_morning', 'morning', 'afternoon', 'evening', 'late_night']\n",
    "grid_df['part_of_day'] = np.select(conditions, choices)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# move date_join to front of df\n",
    "col_name = \"date_join\"\n",
    "first_col = grid_df.pop(col_name)\n",
    "grid_df.insert(0, col_name, first_col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# move mapped region to second column\n",
    "col_name = \"mapped_region\"\n",
    "first_col = grid_df.pop(col_name)\n",
    "grid_df.insert(1, col_name, first_col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# move part of day variable to third column\n",
    "col_name = \"part_of_day\"\n",
    "first_col = grid_df.pop(col_name)\n",
    "grid_df.insert(2, col_name, first_col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sort by time_loc\n",
    "grid_df = grid_df.sort_values(by=['date_join'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fill nas in cols\n",
    "grid_df = grid_df.fillna(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_df.head(n=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_df.sort_values(['mapped_region', 'date_join']).head(n=500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### daily weather api"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pip install wwo-hist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from wwo_hist import retrieve_hist_data\n",
    "import os\n",
    "os.chdir(\"/Users/michael/MSDS_AI_Capstone/Mnpls_Data/Mnpls_Weather_data\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frequency=24\n",
    "start_date = '01-Jan-2010'\n",
    "end_date = '01-MAR-2020'\n",
    "api_key = '159e095911d643c4b2414653202705'\n",
    "location_list = ['minneapolis','minnesota']\n",
    "\n",
    "hist_weather_data = retrieve_hist_data(api_key,\n",
    "                                location_list,\n",
    "                                start_date,\n",
    "                                end_date,\n",
    "                                frequency,\n",
    "                                location_label = False,\n",
    "                                export_csv = True,\n",
    "                                store_df = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pwd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read in csv\n",
    "weather_new = pd.read_csv('/Users/michael/MSDS_AI_Capstone/Mnpls_Data/Mnpls_Weather_data/minneapolis.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weather_new.head(n=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert date_time to date\n",
    "weather_new['date_time'] = pd.to_datetime(weather_new['date_time'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# join with crime dataset\n",
    "df_new_weather = pd.merge(df, weather_new[[\"date_time\", \"totalSnow_cm\", \"WindChillC\", \"precipMM\", \"tempC\"]], \n",
    "                                  left_on=\"Date\", right_on=\"date_time\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_new_weather.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine crimes into violent crime or not\n",
    "\n",
    "df_new_weather['violent_crime'] = np.where(df_new_weather['Offense'].isin(['ARSON', 'ASLT1', 'ASLT2', 'ASLT3', 'ASLT4', 'CSCR', 'DASLT1'\n",
    "                                                                     'DASLT2', 'DASLT3', 'DASTR', 'MURDR', 'ROBPAG', \n",
    "                                                                      'ROBPER', 'ROBBIZ', 'TFPER', 'JHOMIC']), 1, 0)\n",
    "\n",
    "df_new_weather['other_crime'] = np.where(~df_new_weather['Offense'].isin(['ARSON', 'ASLT1', 'ASLT2', 'ASLT3', 'ASLT4', 'CSCR', 'DASLT1'\n",
    "                                                                     'DASLT2', 'DASLT3', 'DASTR', 'MURDR', 'ROBPAG', \n",
    "                                                                      'ROBPER', 'ROBBIZ', 'TFPER', 'JHOMIC']), 1, 0)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# subset df down to necessary columns\n",
    "model_df_new = df_new_weather.drop(['ReportedDateTime', 'Neighborhood', 'Precinct', 'Offense', 'Description',\n",
    "                                   'Time', 'Year', 'Month', 'Month_Name', 'Hour', 'date_join',\n",
    "                                   'date_time'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_df_new.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create geo points for each lat/long coordinate\n",
    "crime_locations = [Point(xy) for xy in zip(model_df_new.Long, df_new.Lat)]\n",
    "model_df_new['geometry'] = crime_locations\n",
    "#data_loc_copy.drop(['lat','long'], axis = 1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert dataframe to a geo-dataframe\n",
    "from pyproj import CRS\n",
    "crs = CRS.from_epsg(4326)\n",
    "\n",
    "data_gpd = gpd.GeoDataFrame(model_df_new, crs=crs, geometry=crime_locations)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# map each crime lat/long points to the index of the polygon that they fall within\n",
    "from geopandas.tools import sjoin\n",
    "data_final = sjoin(data_gpd, grid_trimmed, how='left',op=\"within\")\n",
    "data_final = data_final.rename(columns={'index_right': 'mapped_region'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check if mappings worked\n",
    "subset_gpd = data_final.loc[(data_final['mapped_region'] == 222),:]\n",
    "\n",
    "# plot all points that were tagged within index 1 polygon \n",
    "base = grid_trimmed.boundary.plot(color='blue',figsize=(20, 15))\n",
    "subset_gpd.geometry.plot(ax=base,color='red',figsize=(20, 15),markersize=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_final.sort_values(['Date', 'mapped_region']).head(n=500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# subset df down to necessary columns\n",
    "data_final = data_final.drop(['Lat', 'Long', 'geometry', 'OBJECTID', 'NAME', 'Shape__Are', 'Shape__Len'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# move mapped_region to 2nd col of df\n",
    "col_name = \"mapped_region\"\n",
    "sec_col = data_final.pop(col_name)\n",
    "data_final.insert(1, col_name, sec_col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create df with only mapped region\n",
    "mapped_reg = data_final['mapped_region'].unique()\n",
    "mapped_reg_df = pd.DataFrame(mapped_reg)\n",
    "mapped_reg_df = mapped_reg_df.rename(columns={0: \"mapped_region\"})\n",
    "mapped_reg_df = mapped_reg_df.sort_values(['mapped_region'])"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# lets prepare data for just one day to see how it is going to look like\n",
    "calculated_date = pd.Timestamp('2010-02-01')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# breakpoints for different aggregations\n",
    "calculated_date_30 = calculated_date - pd.Timedelta(30, unit='d')\n",
    "calculated_date_7 = calculated_date - pd.Timedelta(7, unit='d')\n",
    "calculated_date_1 = calculated_date - pd.Timedelta(1, unit='d')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a slice of the data set for prior 30 days aggregation\n",
    "data_final_30 = data_final[(data_final[\"Date\"] < calculated_date) & (data_final[\"Date\"] >= calculated_date_30)]\n",
    "\n",
    "# create a slice of the data set for prior 7 days aggregation\n",
    "data_final_7 = data_final[(data_final[\"Date\"] < calculated_date) & (data_final[\"Date\"] >= calculated_date_7)]\n",
    "\n",
    "# create a slice of the data set for prior 1 day aggregation\n",
    "data_final_1 = data_final[(data_final[\"Date\"] < calculated_date) & (data_final[\"Date\"] >= calculated_date_1)]\n",
    "\n",
    "# create a slice of the data set for that day aggregation\n",
    "data_final_0 = data_final[(data_final[\"Date\"] == calculated_date)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_final_30"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create sum of violent, property and other crimes for each cluster center.\n",
    "# replace the non existent cluster centers' value with 0\n",
    "\n",
    "working_30_results = pd.merge(mapped_reg_df,\n",
    "         data_final_30.groupby(\"mapped_region\")[[\"violent_crime\",\"other_crime\"]].sum().reset_index(),\n",
    "         left_on=\"mapped_region\", right_on=\"mapped_region\", how=\"left\").fillna(0)\n",
    "\n",
    "working_7_results = pd.merge(mapped_reg_df,\n",
    "         data_final_7.groupby(\"mapped_region\")[[\"violent_crime\",\"other_crime\"]].sum().reset_index(),\n",
    "         left_on=\"mapped_region\", right_on=\"mapped_region\", how=\"left\").fillna(0)\n",
    "\n",
    "working_1_results = pd.merge(mapped_reg_df,\n",
    "         data_final_1.groupby(\"mapped_region\")[[\"violent_crime\",\"other_crime\"]].sum().reset_index(),\n",
    "         left_on=\"mapped_region\", right_on=\"mapped_region\", how=\"left\").fillna(0)\n",
    "\n",
    "working_0_results = pd.merge(mapped_reg_df,\n",
    "         data_final_0.groupby(\"mapped_region\")[[\"violent_crime\",\"other_crime\"]].sum().reset_index(),\n",
    "         left_on=\"mapped_region\", right_on=\"mapped_region\", how=\"left\").fillna(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "working_30_results.head(n=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# change col names\n",
    "working_30_results = working_30_results.rename(columns={\"violent_crime\": \"sum_violent_crime_30\", \n",
    "                                                        \"other_crime\": \"sum_other_crime_30\"})\n",
    "\n",
    "working_7_results = working_7_results.rename(columns={\"violent_crime\": \"sum_violent_crime_7\", \n",
    "                                                        \"other_crime\": \"sum_other_crime_7\"})\n",
    "\n",
    "working_1_results = working_1_results.rename(columns={\"violent_crime\": \"sum_violent_crime_1\", \n",
    "                                                        \"other_crime\": \"sum_other_crime_1\"})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "working_7_results.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "working_30_results[working_30_results[\"sum_violent_crime_30\"]>0].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "working_7_results[working_7_results[\"sum_violent_crime_7\"]>0].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "working_1_results[working_1_results[\"sum_violent_crime_1\"]>0].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "working_0_results[working_0_results[\"violent_crime\"]>0].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# update total number of crimes as 1 for the calculations of that day\n",
    "# meaning it happened that day. binary response variable. \n",
    "# we are going to try to predict these ones\n",
    "working_0_results.loc[working_0_results[\"violent_crime\"]!=0,\"violent_crime\"]=1\n",
    "\n",
    "working_0_results.loc[working_0_results[\"other_crime\"]!=0,\"other_crime\"]=1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a list of dataframes to merge. all those calculated sums are going to be \n",
    "# merged into a single dataframe\n",
    "df_list = [working_30_results, working_7_results, working_1_results, working_0_results]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['mapped_region'], how='inner'), df_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_merged[\"Date_new\"]=calculated_date"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_merged.head(n=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pick starting point for loop\n",
    "calculated_date = pd.Timestamp('2015-02-01')\n",
    "calculated_date"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_final_working=data_final[data_final[\"Date\"] > datetime.date(2010, 2, 1)]\n",
    "data_final_working.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initiliaze the list to hold dataframes for the whole year\n",
    "crime_stats_fordays_list=[0]*1829"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(1,1830):\n",
    "    # breakpoints for different aggregations\n",
    "    calculated_date_30 = calculated_date - pd.Timedelta(30, unit='d')\n",
    "    calculated_date_7 = calculated_date - pd.Timedelta(7, unit='d')\n",
    "    calculated_date_1 = calculated_date - pd.Timedelta(1, unit='d')\n",
    "    \n",
    "    \n",
    "    # create a slice of the data set for prior 30 days aggregation\n",
    "    data_final_30 = data_final[(data_final[\"Date\"] < calculated_date) & (data_final[\"Date\"] >= calculated_date_30)]\n",
    "\n",
    "    # create a slice of the data set for prior 7 days aggregation\n",
    "    data_final_7 = data_final[(data_final[\"Date\"] < calculated_date) & (data_final[\"Date\"] >= calculated_date_7)]\n",
    "\n",
    "    # create a slice of the data set for prior 1 day aggregation\n",
    "    data_final_1 = data_final[(data_final[\"Date\"] < calculated_date) & (data_final[\"Date\"] >= calculated_date_1)]\n",
    "\n",
    "    # create a slice of the data set for that day aggregation\n",
    "    data_final_0 = data_final[(data_final[\"Date\"] == calculated_date)]\n",
    "    \n",
    "    \n",
    "    # create sum of violent, property and other crimes for each cluster center.\n",
    "    # replace the non existent cluster centers' value with 0\n",
    "    working_30_results = pd.merge(mapped_reg_df,\n",
    "         data_final_30.groupby(\"mapped_region\")[[\"violent_crime\",\"other_crime\"]].sum().reset_index(),\n",
    "         left_on=\"mapped_region\", right_on=\"mapped_region\", how=\"left\").fillna(0)\n",
    "\n",
    "    working_7_results = pd.merge(mapped_reg_df,\n",
    "         data_final_7.groupby(\"mapped_region\")[[\"violent_crime\",\"other_crime\"]].sum().reset_index(),\n",
    "         left_on=\"mapped_region\", right_on=\"mapped_region\", how=\"left\").fillna(0)\n",
    "\n",
    "    working_1_results = pd.merge(mapped_reg_df,\n",
    "         data_final_1.groupby(\"mapped_region\")[[\"violent_crime\",\"other_crime\"]].sum().reset_index(),\n",
    "         left_on=\"mapped_region\", right_on=\"mapped_region\", how=\"left\").fillna(0)\n",
    "\n",
    "    working_0_results = pd.merge(mapped_reg_df,\n",
    "         data_final_0.groupby(\"mapped_region\")[[\"violent_crime\",\"other_crime\"]].sum().reset_index(),\n",
    "         left_on=\"mapped_region\", right_on=\"mapped_region\", how=\"left\").fillna(0)\n",
    "    \n",
    "    \n",
    "    # change col names\n",
    "    working_30_results = working_30_results.rename(columns={\"violent_crime\": \"sum_violent_crime_30\", \n",
    "                                                        \"other_crime\": \"sum_other_crime_30\"})\n",
    "\n",
    "    working_7_results = working_7_results.rename(columns={\"violent_crime\": \"sum_violent_crime_7\", \n",
    "                                                        \"other_crime\": \"sum_other_crime_7\"})\n",
    "\n",
    "    working_1_results = working_1_results.rename(columns={\"violent_crime\": \"sum_violent_crime_1\", \n",
    "                                                        \"other_crime\": \"sum_other_crime_1\"})\n",
    "    \n",
    "    \n",
    "    # update total number of crimes as 1 for the calculations of that day\n",
    "    working_0_results.loc[working_0_results[\"violent_crime\"]!=0,\"violent_crime\"]=1\n",
    "\n",
    "    working_0_results.loc[working_0_results[\"other_crime\"]!=0,\"other_crime\"]=1\n",
    "    \n",
    "    \n",
    "    # create a list of dataframes to merge. all those calculated sums are going to be \n",
    "    # merged into a single dataframe\n",
    "    df_list = [working_30_results, working_7_results, working_1_results, working_0_results]\n",
    "    \n",
    "    # merge\n",
    "    df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['mapped_region'], how='inner'), df_list)\n",
    "    \n",
    "        \n",
    "    # add the day\n",
    "    df_merged[\"Date_new\"]=calculated_date\n",
    "    \n",
    "    \n",
    "    # add the resulting dataframe to the list\n",
    "    crime_stats_fordays_list[i-1]=df_merged\n",
    "    \n",
    "    \n",
    "    print(calculated_date)\n",
    "    \n",
    "    \n",
    "    # increment the calculated date\n",
    "    calculated_date=calculated_date + pd.Timedelta(1, unit='d')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "crime_stats_fordays_list[100].head(n=50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(crime_stats_fordays_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# concat all the dataframes for each day\n",
    "crime_stats_fordays_df = pd.concat(crime_stats_fordays_list,ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# subset data_final table to pull in only necessary cols\n",
    "data_final_sub = data_final[[\"Date\", \"DoW\", 'totalSnow_cm', 'WindChillC', 'precipMM', 'tempC']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# join weather data in\n",
    "df_final_new = pd.merge(crime_stats_fordays_df, data_final_sub, left_on=\"Date_new\", right_on=\"Date\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "crime_stats_fordays_df['DoW'] = crime_stats_fordays_df['Date_new'].dt.day_name()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "crime_stats_fordays_df.head(n=500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_final_new = pd.merge(crime_stats_fordays_df,\n",
    "            weather_new[[\"date_time\", 'totalSnow_cm', 'WindChillC', 'precipMM', 'tempC']], \n",
    "                                  left_on=\"Date_new\", right_on=\"date_time\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# move date_join to front of df\n",
    "col_name = \"Date_new\"\n",
    "first_col = df_final_new.pop(col_name)\n",
    "df_final_new.insert(0, col_name, first_col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_final_new.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove extra date column\n",
    "df_final_new = df_final_new.drop(['date_time'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# make mapped region a categorical\n",
    "df_final_new['mapped_region'] = df_final_new['mapped_region'].astype(str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get_dummies on DOW var\n",
    "df_final_new = pd.get_dummies(df_final_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# scale using min-max\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "df_final_new.loc[:, ~df_final_new.columns.isin(['Date_new'])] = scaler.fit_transform(df_final_new.loc[:, ~df_final_new.columns.isin(['Date_new'])])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_final_new.head(n=500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train and test split to be about an 80/20 split\n",
    "\n",
    "# train\n",
    "mask = (df_final_new['Date_new'] >= '2015-02-01') & (df_final_new['Date_new'] <= '2019-02-04')\n",
    "train = df_final_new.loc[mask]\n",
    "\n",
    "# test\n",
    "mask_test = (df_final_new['Date_new'] > '2019-02-04')\n",
    "test = df_final_new.loc[mask_test]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set vars in x and y columns\n",
    "x_columns=[ 'mapped_region', \n",
    "        'sum_violent_crime_30', 'sum_other_crime_30', 'sum_violent_crime_7', \n",
    "        'sum_other_crime_7','sum_violent_crime_1', 'sum_other_crime_1', \n",
    "        'other_crime','totalSnow_cm', 'WindChillC', \n",
    "        'precipMM','tempC', 'DoW_Friday', \n",
    "        'DoW_Monday','DoW_Saturday', 'DoW_Sunday', 'DoW_Thursday', 'DoW_Tuesday', 'DoW_Wednesday']\n",
    "\n",
    "\n",
    "y_column=[\"violent_crime\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = train.drop(['Date_new', 'violent_crime'], axis = 1) \n",
    "y_train = train['violent_crime']\n",
    "X_test = test.drop(['Date_new', 'violent_crime'], axis = 1) \n",
    "y_test = test['violent_crime']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check balance of training set\n",
    "y_train.values.sum() / y_train.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train.values.sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pip install imblearn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# oversample using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check balance of trainig set after SMOTE\n",
    "y_train.values.sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# initialize and fit\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# make predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def intitial_eda_checks(df):\n",
    "    '''\n",
    "    Takes df\n",
    "    Checks nulls\n",
    "    '''\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        mask_total = df.isnull().sum().sort_values(ascending=False) \n",
    "        total = mask_total[mask_total > 0]\n",
    "\n",
    "        mask_percent = df.isnull().mean().sort_values(ascending=False) \n",
    "        percent = mask_percent[mask_percent > 0] \n",
    "\n",
    "        missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    \n",
    "        print(f'Total and Percentage of NaN:\\n {missing_data}')\n",
    "    else: \n",
    "        print('No NaN found.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "intitial_eda_checks(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def view_columns_w_many_nans(df, missing_percent):\n",
    "    '''\n",
    "    Checks which columns have over specified percentage of missing values\n",
    "    Takes df, missing percentage\n",
    "    Returns columns as a list\n",
    "    '''\n",
    "    mask_percent = df.isnull().mean()\n",
    "    series = mask_percent[mask_percent > missing_percent]\n",
    "    columns = series.index.to_list()\n",
    "    print(columns) \n",
    "    return columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "view_columns_w_many_nans(df, 0.0001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "1b7dac084ace4e2a148e6b8794ea73f213bd1a279d4d9ea03c42f57517526a2d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}